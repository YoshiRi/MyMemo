{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんかファイル開くのにもちょっと苦労している。\n",
    "- utf-8を指定\n",
    "https://qiita.com/Yuu94/items/9ffdfcb2c26d6b33792e\n",
    "- json decoderの設定？\n",
    "https://pod.hatenablog.com/entry/2017/08/31/035140\n",
    "- 1行ずつ読めば良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdic=[]\n",
    "with open(\"slam_hub_twint.json\", 'r',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        sdic.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cashtags': [],\n",
       "  'conversation_id': '1289395426723016708',\n",
       "  'created_at': 1596250862000,\n",
       "  'date': '2020-08-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1289395689697337344,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1289395689697337344',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:01:02',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.10985\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2007.10985'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1289395426723016708',\n",
       "  'created_at': 1596250800000,\n",
       "  'date': '2020-08-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1289395426723016708,\n",
       "  'likes_count': 23,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1289395426723016708',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EePiWZ2VAAY4Wok.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '点群データに対する教師なしの事前学習手法を提案．ScanNetから抽出した視点の異なる2つの点群に対し，点のマッチングを基に距離学習を行う．事前学習済みモデルを転移することで屋内外を含む6種の広範なデータセットで精度向上が得られた． https://arxiv.org/abs/2007.10985\\xa0 pic.twitter.com/gEDAQZD95r',\n",
       "  'urls': ['https://arxiv.org/abs/2007.10985'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1289191063937196036',\n",
       "  'created_at': 1596205830000,\n",
       "  'date': '2020-07-31',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1289206810314862593,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1289206810314862593',\n",
       "  'mentions': ['ossyaritoori'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '543648159', 'username': 'ossyaritoori'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '14:30:30',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '@ossyaritoori ご連絡ありがとうございます．とても面白い記事ですね．転載されているのは僅かでしたしご連絡頂いたので問題ございません．今後も更新していくので引き続きご覧頂けますと幸いです．',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1289033039637557249',\n",
       "  'created_at': 1596164451000,\n",
       "  'date': '2020-07-31',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1289033252682924034,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1289033252682924034',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:00:51',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Neural Topological SLAM for Visual Navigation, CVPR2020\\nProject:  https://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html\\xa0… pic.twitter.com/CVya3XyiuY',\n",
       "  'urls': ['https://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 1},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1289033039637557249',\n",
       "  'created_at': 1596164400000,\n",
       "  'date': '2020-07-31',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1289033039637557249,\n",
       "  'likes_count': 17,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1289033039637557249',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '目標画像位置を未知環境内で探すトポロジカルグラフSLAM＋探索手法を提案．各地点(ノード)での周囲の移動可能領域・目的地到達可能性を教師あり学習し，それをもとにグラフ上で探索行動を生成する．強化・教師なし学習ベース手法と比較し安定した学習・高精度な探索を実現． https://youtu.be/vubX97owdjQ\\xa0',\n",
       "  'urls': ['https://youtu.be/vubX97owdjQ'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1288666567933190144',\n",
       "  'created_at': 1596077027000,\n",
       "  'date': '2020-07-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1288666571095842818,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1288666571095842818',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '02:43:47',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Points2Surf：Learning Implicit Surfaces from Point Clouds (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.10453\\xa0\\nProject:  https://www.cg.tuwien.ac.at/research/publications/2020/erler-p2s/\\xa0…\\nCode:  https://github.com/ErlerPhilipp/points2surf\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2007.10453',\n",
       "   'https://www.cg.tuwien.ac.at/research/publications/2020/erler-p2s/',\n",
       "   'https://github.com/ErlerPhilipp/points2surf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1288666567933190144',\n",
       "  'created_at': 1596077026000,\n",
       "  'date': '2020-07-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1288666567933190144,\n",
       "  'likes_count': 32,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1288666567933190144',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EeJDOz5U8AE98jX.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '02:43:46',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ノイズや不均一な点群から物体のimplicit表面の推定手法を提案。クエリ点に対する点群のGlobalとLocal情報から、sign logitとabsolute distanceを別々に推定するネットワークを学習させ、signed distance field (SDF)を得て、TSDF へ変換、Marching Cubesで表面を生成する。\\n https://arxiv.org/abs/2007.10453\\xa0 pic.twitter.com/bWgcYMUvDy',\n",
       "  'urls': ['https://arxiv.org/abs/2007.10453'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1288308266947272704',\n",
       "  'created_at': 1595991618000,\n",
       "  'date': '2020-07-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1288308338145411073,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1288308338145411073',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:00:18',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'DeepCap: Monocular Human Performance Capture Using Weak Supervision (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.08325\\xa0\\nProject:  https://people.mpi-inf.mpg.de/~mhaberma/projects/2020-cvpr-deepcap/\\xa0…\\nVideo: https://www.youtube.com/watch?v=C4eDrvJ9aBs\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2003.08325',\n",
       "   'https://people.mpi-inf.mpg.de/~mhaberma/projects/2020-cvpr-deepcap/',\n",
       "   'https://www.youtube.com/watch?v=C4eDrvJ9aBs'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1288308266947272704',\n",
       "  'created_at': 1595991601000,\n",
       "  'date': '2020-07-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1288308266947272704,\n",
       "  'likes_count': 26,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1288308266947272704',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EeD8ogfUMAAU5TB.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '単一の画像からの衣服を含む人体形状復元．PoseNetによる関節位置の推定とDefNetによる表面形状の変形推定を組み合わせたロス関数を設計し学習．単一画像のみから，多視点画像を利用する従来手法と競合する復元精度を実現している．\\n https://arxiv.org/abs/2003.08325\\xa0 pic.twitter.com/hH7pC6l1xs',\n",
       "  'urls': ['https://arxiv.org/abs/2003.08325'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287945878561361920',\n",
       "  'created_at': 1595905240000,\n",
       "  'date': '2020-07-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287946042286026752,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287946042286026752',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:00:40',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Efficient Continuous-Time SLAM for 3D Lidar-based Online Mapping (ICRA 2018)\\nPaper:  https://arxiv.org/abs/1810.06802\\xa0\\nPaper:  https://ieeexplore.ieee.org/document/8461000\\xa0…\\nVideo: https://youtu.be/iG9MJLzja5g\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/1810.06802',\n",
       "   'https://ieeexplore.ieee.org/document/8461000',\n",
       "   'https://youtu.be/iG9MJLzja5g'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287945878561361920',\n",
       "  'created_at': 1595905200000,\n",
       "  'date': '2020-07-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287945878561361920,\n",
       "  'likes_count': 57,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287945878561361920',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ed-kCY2UMAEysLQ.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 13,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '離散時間ではなく連続時間で軌跡を推定する Continuous-Time SLAM の一手法。スキャン点群のスパース（疎）性と歪みの問題に対処。Surfel を用いた位置合わせで、複数解像度の局所地図を構築。局所地図ノードとロボット位置ノードからなる階層的ポーズグラフを相互に最適化。\\n https://arxiv.org/abs/1810.06802\\xa0 pic.twitter.com/LpD2xCvjUM',\n",
       "  'urls': ['https://arxiv.org/abs/1810.06802'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287584106847510528',\n",
       "  'created_at': 1595818963000,\n",
       "  'date': '2020-07-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287584171481632770,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287584171481632770',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:02:43',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM (arXiv)\\nPaper:  https://arxiv.org/abs/2007.11898\\xa0\\nProject:  https://github.com/UZ-SLAMLab/ORB_SLAM3\\xa0…\\nVideo:  https://youtu.be/HyLNq-98LRo\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2007.11898',\n",
       "   'https://github.com/UZ-SLAMLab/ORB_SLAM3',\n",
       "   'https://youtu.be/HyLNq-98LRo'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287584106847510528',\n",
       "  'created_at': 1595818947000,\n",
       "  'date': '2020-07-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287584106847510528,\n",
       "  'likes_count': 64,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287584106847510528',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 20,\n",
       "  'source': '',\n",
       "  'time': '03:02:27',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ORB-SLAMの最新バージョン．ORB-SLAM2との違いはマルチマップ・システムで，過去マップに対する自己位置同定だけではなく，自己位置ロスト後に新しいマップを生成し，Place Recognition成功後に過去マップと統合しロバスト性を向上．従来法にくらべ数倍の精度向上を達成． https://youtu.be/HyLNq-98LRo\\xa0',\n",
       "  'urls': ['https://youtu.be/HyLNq-98LRo'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287351687296892928',\n",
       "  'created_at': 1595763535000,\n",
       "  'date': '2020-07-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287351688802705408,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287351688802705408',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ed2Xc1yVoAYCXKC.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '11:38:55',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Privacy Preserving Structure-from-Motion (ECCV2020)\\nProject:  https://cvg.ethz.ch/research/privacy-preserving-sfm\\xa0…\\nSupp:  https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf\\xa0…\\nVideo:  https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4\\xa0… pic.twitter.com/ATTcyyT1PB',\n",
       "  'urls': ['https://cvg.ethz.ch/research/privacy-preserving-sfm',\n",
       "   'https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf',\n",
       "   'https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287351687296892928',\n",
       "  'created_at': 1595763534000,\n",
       "  'date': '2020-07-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287351687296892928,\n",
       "  'likes_count': 13,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287351687296892928',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ed2XW4DUcAImzDU.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '11:38:54',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'This paper proposes an incremental SfM system with privacy protection. The system uses only random 2D lines converted from 2D feature points for its initialization, camera pose estimation, triangulation, and bundle adjustment. \\n https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf\\xa0… pic.twitter.com/n9XrXyM4K5',\n",
       "  'urls': ['https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287253428670853121',\n",
       "  'created_at': 1595740108000,\n",
       "  'date': '2020-07-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287253430667341824,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287253430667341824',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ed0-JnEUYAAs4II.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '05:08:28',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Privacy Preserving Structure-from-Motion (ECCV2020)\\nProject:  https://www.cvg.ethz.ch/research/privacy-preserving-sfm\\xa0…\\nSupp:  https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf\\xa0…\\nVideo:  https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4\\xa0… pic.twitter.com/hxSGXSJnfV',\n",
       "  'urls': ['https://www.cvg.ethz.ch/research/privacy-preserving-sfm',\n",
       "   'https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf',\n",
       "   'https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1287253428670853121',\n",
       "  'created_at': 1595740108000,\n",
       "  'date': '2020-07-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1287253428670853121,\n",
       "  'likes_count': 38,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1287253428670853121',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ed09_7TU8Ag68FG.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 10,\n",
       "  'source': '',\n",
       "  'time': '05:08:28',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'プライバシー保護を考慮したIncremental SfMシステムを開発．2D特徴点から変換された2D直線のみを用いた，初期化，カメラ姿勢の推定，三角測量，バンドル調整を提案．2D特徴点を利用したSfMに近い精度と，Inverstion Attackに対するより高い頑健性を実現．\\n https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf\\xa0… pic.twitter.com/bzFGa7QKLZ',\n",
       "  'urls': ['https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286862906143854593',\n",
       "  'created_at': 1595647033000,\n",
       "  'date': '2020-07-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1286863043696058368,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286863043696058368',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdvbGKhUMAEhLsx.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:17:13',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Exploit Clues from Views: Self-Supervised and Regularized Learning for Multiview Object Recognition （CVPR2020）\\nPaper:  https://arxiv.org/abs/2003.12735\\xa0\\nProject:  https://chihhuiho.github.io/vispe_web/\\xa0\\nCode:  https://github.com/chihhuiho/VISPE\\xa0 pic.twitter.com/fbdVoPxoKS',\n",
       "  'urls': ['https://arxiv.org/abs/2003.12735',\n",
       "   'https://chihhuiho.github.io/vispe_web/',\n",
       "   'https://github.com/chihhuiho/VISPE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286862906143854593',\n",
       "  'created_at': 1595647000000,\n",
       "  'date': '2020-07-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1286862906143854593,\n",
       "  'likes_count': 20,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286862906143854593',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Edva-6wU8AUplEB.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:16:40',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '多視点物体認識のための特徴量抽出を自己教師あり学習する手法を提案．代理タスクとして，オブジェクトクラス分類を通し距離学習を行う．これにより視点に因らず同一オブジェクトならば埋め込み表現上でクラスターを形成．ダウンストリームタスクで他手法より高い精度を達成．\\n https://arxiv.org/abs/2003.12735\\xa0 pic.twitter.com/YyvMehYHKd',\n",
       "  'urls': ['https://arxiv.org/abs/2003.12735'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286520000627372032',\n",
       "  'created_at': 1595565245000,\n",
       "  'date': '2020-07-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1286520001906712576,\n",
       "  'likes_count': 7,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286520001906712576',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdqjAj8UMAEHJVl.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '04:34:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking (IROS 2020)\\nPaper:  https://arxiv.org/abs/2007.10743\\xa0 pic.twitter.com/tgcsuZcnpm',\n",
       "  'urls': ['https://arxiv.org/abs/2007.10743'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286520000627372032',\n",
       "  'created_at': 1595565245000,\n",
       "  'date': '2020-07-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1286520000627372032,\n",
       "  'likes_count': 22,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286520000627372032',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '04:34:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ステレオカメラの情報から動的な障害を検出，追跡するシステムの提案．ロボット周囲の物体について動的・静的の2クラスに分類．さらに動的な物体については人とそれ以外のクラスに分類する．ノイズの多いデータから高い精度の動的物体の検出,追跡が可能なことを実験で確認． https://youtu.be/AYjgeaQR8uQ\\xa0',\n",
       "  'urls': ['https://youtu.be/AYjgeaQR8uQ'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286133936716787713',\n",
       "  'created_at': 1595498343000,\n",
       "  'date': '2020-07-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1286239396195233793,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286239396195233793',\n",
       "  'mentions': ['ajddavison'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '1446792746', 'username': 'AjdDavison'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '09:59:03',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Thank you for your recommendation! We have already enjoyed the paper, especially for IPU, including your CVPR paper. We want to introduce them soon!',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286133936716787713',\n",
       "  'created_at': 1595473519000,\n",
       "  'date': '2020-07-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1286135276859883520,\n",
       "  'likes_count': 17,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286135276859883520',\n",
       "  'mentions': ['ajddavison'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '1446792746', 'username': 'AjdDavison'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:05:19',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Visual SLAMの第一人者 Andrew J. Davison ( @AjdDavison ) 先生が描くSLAMの未来\\nFutureMapping: The Computational Structure of Spatial AI Systems\\nPaper:  https://arxiv.org/abs/1803.11288\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/1803.11288'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1286133936716787713',\n",
       "  'created_at': 1595473200000,\n",
       "  'date': '2020-07-23',\n",
       "  'geo': '',\n",
       "  'hashtags': ['#spatialai', '#spatialai'],\n",
       "  'id': 1286133936716787713,\n",
       "  'likes_count': 123,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1286133936716787713',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdhgCeRU0AYBeua.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 2,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 37,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SLAM （自己位置推定と地図構築）を発展させ，シーンやオブジェクトの関係性を理解する #SpatialAI の開発が進められている ．#SpatialAI を実際のアプリケーションに応用する上で必要なアルゴリズムやプロセッサ，センサの連携などについて提唱された最初の論文．\\n https://arxiv.org/abs/1803.11288\\xa0 pic.twitter.com/bm9sSOj5qN',\n",
       "  'urls': ['https://arxiv.org/abs/1803.11288'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1285771549090308096',\n",
       "  'created_at': 1595387161000,\n",
       "  'date': '2020-07-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1285773063351484417,\n",
       "  'likes_count': 10,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1285773063351484417',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Edf7xgHU8AArojY.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:06:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3D Packing for Self-Supervised Monocular Depth Estimation (CVPR2020)\\nPaper:  https://arxiv.org/abs/1905.02693\\xa0\\nCode:  https://github.com/TRI-ML/packnet-sfm\\xa0…\\nDataset:  https://github.com/TRI-ML/DDAD\\xa0 pic.twitter.com/4avN57SBlw',\n",
       "  'urls': ['https://arxiv.org/abs/1905.02693',\n",
       "   'https://github.com/TRI-ML/packnet-sfm',\n",
       "   'https://github.com/TRI-ML/DDAD'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1285771549090308096',\n",
       "  'created_at': 1595386800000,\n",
       "  'date': '2020-07-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1285771549090308096,\n",
       "  'likes_count': 25,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1285771549090308096',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '自己教師あり学習で単眼画像のデプスを推定するPackNetを提案．ピクセルをチャンネル方向に並び替えるSpace2Depthを含むPackNetにより，重要な空間情報を保持した明瞭なデプスが推定可能．既存の教師あり学習と同程度の精度を達成． https://www.youtube.com/watch?v=b62iDkLgGSI\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=b62iDkLgGSI'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1285409161451380736',\n",
       "  'created_at': 1595300757000,\n",
       "  'date': '2020-07-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1285410658880643072,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1285410658880643072',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:05:57',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.06888\\xa0\\nCode: https://github.com/hzykent/JSENet\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2007.06888',\n",
       "   'https://github.com/hzykent/JSENet'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1285409161451380736',\n",
       "  'created_at': 1595300400000,\n",
       "  'date': '2020-07-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1285409161451380736,\n",
       "  'likes_count': 28,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1285409161451380736',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdYEOW7VAAAj2sM.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3次元点群に対してセグメンテーションとクラス境界の推定を同時に行う手法を提案．相互に関連する両タスクを同時に解くだけでなく，双方の推定結果を用いてさらに精緻化を行うNNモジュールを提案．屋内データ（S3DIS）でSOTAを達成．\\n https://arxiv.org/abs/2007.06888\\xa0 pic.twitter.com/O52VDKG07v',\n",
       "  'urls': ['https://arxiv.org/abs/2007.06888'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1285049816234422272',\n",
       "  'created_at': 1595214743000,\n",
       "  'date': '2020-07-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1285049890406494209,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1285049890406494209',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:12:23',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Free-Space Features: Global Localization in 2D Laser SLAM  Using Distance Function Maps (IROS2019)\\nPaper:  https://arxiv.org/abs/1908.01863\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/1908.01863'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1285049816234422272',\n",
       "  'created_at': 1595214725000,\n",
       "  'date': '2020-07-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1285049816234422272,\n",
       "  'likes_count': 54,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1285049816234422272',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdVp-HCUYAIGpyK.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:12:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '2D SLAMにおける地図表現にSDFを導入し，計測点が存在しない空間の情報を使った局所特徴(free-space features)を提案．曲率ベースの特徴点検出と方向付き勾配ヒストグラムを使った記述子を使い，従来手法より大域位置認識が高精度に行えることを示した．\\n https://arxiv.org/abs/1908.01863\\xa0 pic.twitter.com/GJSmADnz12',\n",
       "  'urls': ['https://arxiv.org/abs/1908.01863'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1284684384776462337',\n",
       "  'created_at': 1595127721000,\n",
       "  'date': '2020-07-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1284684893906038784,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1284684893906038784',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdQeDnKU8AAiXGQ.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:02:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.13867\\xa0\\nProject:  https://francisengelmann.github.io/3D-MPA/\\xa0 pic.twitter.com/kgBVirhajr',\n",
       "  'urls': ['https://arxiv.org/abs/2003.13867',\n",
       "   'https://francisengelmann.github.io/3D-MPA/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1284684384776462337',\n",
       "  'created_at': 1595127600000,\n",
       "  'date': '2020-07-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1284684384776462337,\n",
       "  'likes_count': 16,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1284684384776462337',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '三次元点群の意味的な特徴とCenter VotesからCenter Proposalを生成、GCNでProposalの特徴をリファイン、Proposalの合体によるInstance SegmentationのMulti Proposal Aggregation Network(MPA)を提案。既存手法のNon-Maximum-Suppression(NMS)と比べて、MPAの優位性を確認。 https://youtu.be/ifL8yTbRFDk\\xa0',\n",
       "  'urls': ['https://youtu.be/ifL8yTbRFDk'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1284322339836985344',\n",
       "  'created_at': 1595041314000,\n",
       "  'date': '2020-07-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1284322476151848960,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1284322476151848960',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:01:54',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'World-Consistent Video-to-Video Synthesis (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.08509\\xa0\\nProject:  https://nvlabs.github.io/wc-vid2vid/\\xa0\\nVideo: https://youtu.be/rlCh6-2NfSg\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2007.08509',\n",
       "   'https://nvlabs.github.io/wc-vid2vid/',\n",
       "   'https://youtu.be/rlCh6-2NfSg'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1284322339836985344',\n",
       "  'created_at': 1595041282000,\n",
       "  'date': '2020-07-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1284322339836985344,\n",
       "  'likes_count': 16,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1284322339836985344',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdLUT0hUMAIVyYt.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:01:22',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '大域的な一貫性を保ったvid2vid．直前の数フレームに基づきクエリ(セマンティクス画像)に対応する画像生成を行う従来法では，同じ位置に立ち戻る場合に一貫性が保証されない．提案手法では，SfMを利用して環境を逐次的に3次元復元し，その幾何をガイドとした画像生成を行う．\\n https://arxiv.org/abs/2007.08509\\xa0 pic.twitter.com/uaQO8g5ofF',\n",
       "  'urls': ['https://arxiv.org/abs/2007.08509'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283959611632959489',\n",
       "  'created_at': 1594954906000,\n",
       "  'date': '2020-07-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283960053540646913,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283960053540646913',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:01:46',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究\\nLeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain (IROS 2018)\\nPaper:  https://ieeexplore.ieee.org/document/8594299\\xa0…\\nCode:  https://github.com/RobustFieldAutonomyLab/LeGO-LOAM\\xa0…\\nVideo:  https://youtu.be/O3tz_ftHV48\\xa0',\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/8594299',\n",
       "   'https://github.com/RobustFieldAutonomyLab/LeGO-LOAM',\n",
       "   'https://youtu.be/O3tz_ftHV48'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283959611632959489',\n",
       "  'created_at': 1594954888000,\n",
       "  'date': '2020-07-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283959977057480704,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283959977057480704',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:01:28',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping (IROS 2020)\\nPaper:  https://arxiv.org/abs/2007.00258\\xa0\\nCode:  https://github.com/TixiaoShan/LIO-SAM\\xa0…\\nVideo:  https://youtu.be/A0H8CoORZJU\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2007.00258',\n",
       "   'https://github.com/TixiaoShan/LIO-SAM',\n",
       "   'https://youtu.be/A0H8CoORZJU'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283959611632959489',\n",
       "  'created_at': 1594954800000,\n",
       "  'date': '2020-07-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283959611632959489,\n",
       "  'likes_count': 118,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283959611632959489',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 32,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'IMU preintegration で点群を歪み補正して初期推定。エッジと平面の特徴点で LIDAR オドメトリ。GTSAM（iSAM2）で全体と局所のグラフ最適化。LIDAR-IMU のタイトカップリングでリアルタイム。ループ拘束は近傍マッチング。GNSS 拘束も。最長 19 km のデータで地図構築に成功。\\n https://arxiv.org/abs/2007.00258\\xa0 pic.twitter.com/ALNln6fwZX',\n",
       "  'urls': ['https://arxiv.org/abs/2007.00258'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 1},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283601093612548099',\n",
       "  'created_at': 1594869337000,\n",
       "  'date': '2020-07-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283601152563490819,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283601152563490819',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:15:37',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Tightly-coupled Fusion of Global Positional Measurements in Optimization-based Visual-Inertial Odometry (IROS2020)\\nPaper:  https://arxiv.org/abs/2003.04159\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2003.04159'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283601093612548099',\n",
       "  'created_at': 1594869323000,\n",
       "  'date': '2020-07-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283601093612548099,\n",
       "  'likes_count': 20,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283601093612548099',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EdBEUIHUcAE5_bz.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:15:23',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Pre-Integration を用いた Tight-coupled な Visual-Inertial Odometry (VIO) に、GPS等によるグローバル座標拘束を導入する手法を初めて提案．従来法はVIOの後段にカルマンフィルタ等を用いて分割して対処．提案手法はコスト関数に拘束を組込み一括で最適化．\\n https://arxiv.org/abs/2003.04159\\xa0 pic.twitter.com/Im1bHSvCyO',\n",
       "  'urls': ['https://arxiv.org/abs/2003.04159'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283234833829945345',\n",
       "  'created_at': 1594782879000,\n",
       "  'date': '2020-07-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283238520606228481,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283238520606228481',\n",
       "  'mentions': ['rmurai0610'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '1023078771073605632', 'username': 'rmurai0610'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:14:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '著者ツイート\\n@rmurai0610',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283234833829945345',\n",
       "  'created_at': 1594782879000,\n",
       "  'date': '2020-07-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283238519532486656,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283238519532486656',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:14:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'BIT-VO: Visual Odometry at 300 FPS using Binary Features from the Focal Plane (IROS2020)\\nPaper:  https://arxiv.org/abs/2004.11186\\xa0\\nProject:  https://rmurai0610.github.io/BIT-VO\\xa0\\nVideo:  https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2004.11186',\n",
       "   'https://rmurai0610.github.io/BIT-VO',\n",
       "   'https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1283234833829945345',\n",
       "  'created_at': 1594782000000,\n",
       "  'date': '2020-07-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1283234833829945345,\n",
       "  'likes_count': 29,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1283234833829945345',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画素毎に並列計算可能なFocal-plane Sensor-processor (FPSP) を用いたVisual OdometryアルゴリズムBIT-VOを提案．FPSP上で，アナログ信号の領域で2値のエッジ検出を行い，そのエッジからバイナリ特徴を計算．それらをホストデバイスに転送することで300fpsの6DOF VOを実現． https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1282872986467397633',\n",
       "  'created_at': 1594695729000,\n",
       "  'date': '2020-07-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1282872988400934912,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1282872988400934912',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ec2uHoIUEAAgF2a.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:02:09',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Self-Supervised Viewpoint Learning From Image Collections (CVPR2020)\\nPaper:  https://arxiv.org/abs/2004.01793\\xa0\\nProject:  https://research.nvidia.com/publication/2020-03_Self-Supervised-Viewpoint-Learning\\xa0…\\nCode:  https://github.com/NVlabs/SSV\\xa0 pic.twitter.com/9TgUW7zLPZ',\n",
       "  'urls': ['https://arxiv.org/abs/2004.01793',\n",
       "   'https://research.nvidia.com/publication/2020-03_Self-Supervised-Viewpoint-Learning',\n",
       "   'https://github.com/NVlabs/SSV'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1282872986467397633',\n",
       "  'created_at': 1594695729000,\n",
       "  'date': '2020-07-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1282872986467397633,\n",
       "  'likes_count': 27,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1282872986467397633',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ec2t_Y-UcAAYEyM.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:02:09',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像からの物体方向推定を，self-supervisedに学習する枠組みを提案．画像から3次元方向とスタイル特徴量を抽出し，それらの潜在変数を元に幾何学的変換を行うGenerator(GAN)を用いて学習．損失には一貫性と水平対称性を利用し，教師あり学習に匹敵する性能を達成．\\n https://arxiv.org/abs/2004.01793\\xa0 pic.twitter.com/Qr4LM3uTxA',\n",
       "  'urls': ['https://arxiv.org/abs/2004.01793'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1282510058660737024',\n",
       "  'created_at': 1594609461000,\n",
       "  'date': '2020-07-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1282511151562018816,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1282511151562018816',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:04:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ODE-CNN: Omnidirectional Depth Extension Networks (ICRA 2020)\\nPaper:  https://arxiv.org/abs/2007.01475\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2007.01475'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1282510058660737024',\n",
       "  'created_at': 1594609200000,\n",
       "  'date': '2020-07-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1282510058660737024,\n",
       "  'likes_count': 50,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1282510058660737024',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcuxFqyU4AAiQBT.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 12,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Kinect等のPerspectiveなdepthセンサーと全方位画像を用いて全方位のdepthを得る手法の提案.Encoderの最後の層でPerspective座標に変換し特徴量の学習難度を下げ、Decoderでequirectangular座標に戻す.他のSoTAな手法より優れていることを示した.\\n https://arxiv.org/abs/2007.01475\\xa0 pic.twitter.com/ZYPB7ysvaY',\n",
       "  'urls': ['https://arxiv.org/abs/2007.01475'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1282203799256350721',\n",
       "  'created_at': 1594536183000,\n",
       "  'date': '2020-07-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1282203801890373633,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1282203801890373633',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '06:43:03',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '360-Indoor: Towards Learning Real-World Objects in 360◦ Indoor Equirectangular Images (WACV2020)\\nPaper: \\n https://arxiv.org/abs/1910.01712\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/1910.01712'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1282203799256350721',\n",
       "  'created_at': 1594536182000,\n",
       "  'date': '2020-07-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1282203799256350721,\n",
       "  'likes_count': 77,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1282203799256350721',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EctNZZjVcAA-IRH.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 28,\n",
       "  'source': '',\n",
       "  'time': '06:43:02',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '360度屋内画像における物体検出とクラス認識に関するデータセットを提示．Equirectangular形式における極領域の歪みに対応するため，Sphere Netをはじめとする球状CNNを用いたモデルで評価したところ，透視投影画像によるデータセットでの学習よりも大きな改善が見られた． https://arxiv.org/abs/1910.01712\\xa0 pic.twitter.com/buiy0FHupl',\n",
       "  'urls': ['https://arxiv.org/abs/1910.01712'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1281794468920713217',\n",
       "  'created_at': 1594438591000,\n",
       "  'date': '2020-07-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1281794470577500160,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1281794470577500160',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:36:31',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction (ECCV2020 Poster)\\nVideo1:  https://youtu.be/MffXsKjy9W0\\xa0\\nVideo2:  https://youtu.be/OOPJpHexrdE\\xa0\\nVideo3:  https://youtu.be/PMYI9j5vHOw\\xa0\\nPaper:  https://arxiv.org/abs/2004.10681\\xa0',\n",
       "  'urls': ['https://youtu.be/MffXsKjy9W0',\n",
       "   'https://youtu.be/OOPJpHexrdE',\n",
       "   'https://youtu.be/PMYI9j5vHOw',\n",
       "   'https://arxiv.org/abs/2004.10681'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1281794468920713217',\n",
       "  'created_at': 1594438590000,\n",
       "  'date': '2020-07-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1281794468920713217,\n",
       "  'likes_count': 54,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1281794468920713217',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcnZCpAUEAEpnSG.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 12,\n",
       "  'source': '',\n",
       "  'time': '03:36:30',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'CNNで推定した擬似的なデプスマップを用いてRGB-D SLAMを行う．単眼デプス推定の欠点であるスケールの不整合性を，特徴点ベースのSLAMで作成された三次元点を用いてリファインする．これにより，両者の欠点を補った高精度な姿勢推定が可能となった．\\n https://arxiv.org/abs/2004.10681\\xa0 pic.twitter.com/OXOuG5onDG',\n",
       "  'urls': ['https://arxiv.org/abs/2004.10681'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1281422893826494464',\n",
       "  'created_at': 1594350682000,\n",
       "  'date': '2020-07-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1281425754194993152,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1281425754194993152',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:11:22',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'FPConv: Learning Local Flattening for Point Convolution (CVPR2020)\\nPaper:  https://arxiv.org/abs/2002.10701\\xa0\\nRelated work:  https://arxiv.org/abs/1807.02443\\xa0\\nCode: https://github.com/lyqun/FPConv\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2002.10701',\n",
       "   'https://arxiv.org/abs/1807.02443',\n",
       "   'https://github.com/lyqun/FPConv'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1281422893826494464',\n",
       "  'created_at': 1594350000000,\n",
       "  'date': '2020-07-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1281422893826494464,\n",
       "  'likes_count': 26,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1281422893826494464',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EceUhQ5VAAEEjKk.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '平面への投影を用いた点群畳み込みを提案．明示的に接平面を推定するTangentConvとは異なり，点群の投影と内挿を単一の重み行列で表現し，MLPを用いて学習ベースで推定する．Volmetricな畳み込みとの組み合わせでSoTA達成．\\n https://arxiv.org/abs/2002.10701\\xa0 pic.twitter.com/ZmJ3UuwIg1',\n",
       "  'urls': ['https://arxiv.org/abs/2002.10701'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1281074772625874946',\n",
       "  'created_at': 1594267039000,\n",
       "  'date': '2020-07-09',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1281074929438343168,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1281074929438343168',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:57:19',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image (RA-L/ICRA2019)\\nPaper:  https://ieeexplore.ieee.org/abstract/document/8633942\\xa0…\\nCode: https://github.com/irapkaist/scancontext\\xa0…',\n",
       "  'urls': ['https://ieeexplore.ieee.org/abstract/document/8633942',\n",
       "   'https://github.com/irapkaist/scancontext'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1281074772625874946',\n",
       "  'created_at': 1594267001000,\n",
       "  'date': '2020-07-09',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1281074772625874946,\n",
       "  'likes_count': 21,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1281074772625874946',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:56:41',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'LIDARを使った位置推定手法．極座標で作られた高さマップ(Scan Context)を入力とし，CNNで地図上での位置をクラスとして推定する．複数の実データセット上で，一日の学習データで一年通した長期位置推定が高精度に可能であることを示した． https://youtu.be/apmmduXTnaE\\xa0',\n",
       "  'urls': ['https://youtu.be/apmmduXTnaE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1280724052026392576',\n",
       "  'created_at': 1594183383000,\n",
       "  'date': '2020-07-08',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1280724053129486336,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1280724053129486336',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcYLq0PUYAEV8aE.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '04:43:03',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'BSP-Net: Generating Compact Meshes via Binary Space Partitioning (CVPR2020 Best Student Paper)\\nPaper:  https://arxiv.org/abs/1911.06971\\xa0\\nProject:  https://bsp-net.github.io/\\xa0\\nCode:  https://github.com/czq142857/BSP-NET-original\\xa0… pic.twitter.com/G9DKBtwEnm',\n",
       "  'urls': ['https://arxiv.org/abs/1911.06971',\n",
       "   'https://bsp-net.github.io/',\n",
       "   'https://github.com/czq142857/BSP-NET-original'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1280724052026392576',\n",
       "  'created_at': 1594183383000,\n",
       "  'date': '2020-07-08',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1280724052026392576,\n",
       "  'likes_count': 19,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1280724052026392576',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '04:43:03',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '教師なしでコンパクトかつウォータータイトな三次元メッシュ生成手法を提案。Binary Space Partitioning (BSP) で再帰的に入力形状を超平面に分解し、Constructive Solid Geometry (CSG) のブーリアン演算で、分解した超平面から複雑な表面やオブジェクトの生成が可能となる。 https://youtu.be/9-ixexpjN-8\\xa0',\n",
       "  'urls': ['https://youtu.be/9-ixexpjN-8'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1280345060442288129',\n",
       "  'created_at': 1594093109000,\n",
       "  'date': '2020-07-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1280345414881775618,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1280345414881775618',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcSzWMPU8AAhhcb.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:38:29',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery (RA-L2019)\\nPaper:  https://arxiv.org/abs/1903.00268\\xa0\\nCode:  https://github.com/ethz-asl/voxblox-plusplus\\xa0…\\nVideo:  https://youtu.be/Jvl42VJmYxg\\xa0 pic.twitter.com/CiR0JzN43I',\n",
       "  'urls': ['https://arxiv.org/abs/1903.00268',\n",
       "   'https://github.com/ethz-asl/voxblox-plusplus',\n",
       "   'https://youtu.be/Jvl42VJmYxg'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1280345060442288129',\n",
       "  'created_at': 1594093024000,\n",
       "  'date': '2020-07-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1280345060442288129,\n",
       "  'likes_count': 16,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1280345060442288129',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '03:37:04',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'インスタンスを意識したRGB-Dセンサによる地図生成．幾何的なセグメンテーション結果をMask R-CNNから補正し，Over-segmentationを抑制した個別の物体形状を獲得．各物体の幾何を大域地図上で関連付けていくことで，セマンティクス＆インスタンス情報も付与した地図を生成． https://youtu.be/Jvl42VJmYxg\\xa0',\n",
       "  'urls': ['https://youtu.be/Jvl42VJmYxg'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1279973343249281025',\n",
       "  'created_at': 1594004472000,\n",
       "  'date': '2020-07-06',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1279973645008334848,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1279973645008334848',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:01:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Visual-Inertial Mapping with Non-Linear Factor Recovery (RA-L & ICRA 2020)\\nPaper:  https://arxiv.org/abs/1904.06504\\xa0\\nProject:  https://vision.in.tum.de/research/vslam/basalt\\xa0…\\nCode:  https://gitlab.com/VladyslavUsenko/basalt\\xa0…\\nVideo:  https://youtu.be/r3CJ2JP75Tc\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/1904.06504',\n",
       "   'https://vision.in.tum.de/research/vslam/basalt',\n",
       "   'https://gitlab.com/VladyslavUsenko/basalt',\n",
       "   'https://youtu.be/r3CJ2JP75Tc'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1279973343249281025',\n",
       "  'created_at': 1594004400000,\n",
       "  'date': '2020-07-06',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1279973343249281025,\n",
       "  'likes_count': 31,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1279973343249281025',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcNbQNwVcAEqhUL.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Visual-Inertial SLAM の Basalt。IMU の preintegration ではなく、非線形因子復元を行って大域的に最適化。IMU の積分は誤差が大きい問題に対処。VIO の相対位置拘束とロール・ピッチ拘束、バンドル調整のループ拘束を統合。小さな最適化問題として定式化でき、精度も向上。\\n https://arxiv.org/abs/1904.06504\\xa0 pic.twitter.com/mQYfoQ4yjd',\n",
       "  'urls': ['https://arxiv.org/abs/1904.06504'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1279610956172017666',\n",
       "  'created_at': 1593918023000,\n",
       "  'date': '2020-07-05',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1279611053924466688,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1279611053924466688',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:00:23',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '2D Laser SLAM With General Features Represented by Implicit Functions (RA-L2020)\\nPaper:  https://ieeexplore.ieee.org/document/9099049\\xa0…',\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/9099049'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1279610956172017666',\n",
       "  'created_at': 1593918000000,\n",
       "  'date': '2020-07-05',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1279610956172017666,\n",
       "  'likes_count': 23,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1279610956172017666',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcIUVnyUwAARQnd.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '任意の環境形状を表せる陰関数表現を用い2D SLAMを定式化．陰関数に対する分散の導出や陰関数境界内外での最適化の安定化などを行い，楕円・直線モデルを用いた評価実験では従来のモデルフィッティングベース手法より良い精度を示した．\\n https://ieeexplore.ieee.org/document/9099049\\xa0… pic.twitter.com/HnpTxH7bL6',\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/9099049'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1279258907534213121',\n",
       "  'created_at': 1593834085000,\n",
       "  'date': '2020-07-04',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1279258988664635394,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1279258988664635394',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:41:25',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Statistical Outlier Identification in Multi-robot Visual SLAM using Expectation Maximization\\nPaper:  https://arxiv.org/abs/2002.02638\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2002.02638'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1279258907534213121',\n",
       "  'created_at': 1593834065000,\n",
       "  'date': '2020-07-04',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1279258907534213121,\n",
       "  'likes_count': 23,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1279258907534213121',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EcDXJCJUcAEXBxP.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:41:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '複数マップ間のループ検出における外れ値検出手法を提案．各ノード間の回転から閉ループの幾何的整合性をチェックすることで確率的に外れ値を検出．さらにEMアルゴリズムを用いてパラメータをfine-tune．確率伝搬法より高い精度を達成し，収束性も保証．\\n https://arxiv.org/abs/2002.02638\\xa0 pic.twitter.com/zugcqk2JzE',\n",
       "  'urls': ['https://arxiv.org/abs/2002.02638'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278904690546098176',\n",
       "  'created_at': 1593749613000,\n",
       "  'date': '2020-07-03',\n",
       "  'geo': '',\n",
       "  'hashtags': ['#eccv2020'],\n",
       "  'id': 1278904690546098176,\n",
       "  'likes_count': 9,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278904690546098176',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '04:13:33',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SLAM-Hub members got one paper accepted by #ECCV2020 !',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278904428246888448',\n",
       "  'created_at': 1593749551000,\n",
       "  'date': '2020-07-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278904428246888448,\n",
       "  'likes_count': 22,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278904428246888448',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '04:12:31',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278886178670796801',\n",
       "  'created_at': 1593745254000,\n",
       "  'date': '2020-07-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278886404580229122,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278886404580229122',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:00:54',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Vid2Curve: Simultaneous Camera Motion Estimation and Thin Structure Reconstruction from an RGB Video (SIGGRAPH 2020)\\nProject:  https://totoro97.github.io/projects/vid2curve/\\xa0…\\nCode:  https://github.com/Totoro97/Vid2Curve\\xa0…\\nPaper:  https://arxiv.org/abs/2005.03372\\xa0',\n",
       "  'urls': ['https://totoro97.github.io/projects/vid2curve/',\n",
       "   'https://github.com/Totoro97/Vid2Curve',\n",
       "   'https://arxiv.org/abs/2005.03372'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278886178670796801',\n",
       "  'created_at': 1593745200000,\n",
       "  'date': '2020-07-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278886178670796801,\n",
       "  'likes_count': 16,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278886178670796801',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ワイヤーフレームや電線など細い物体に対する形状復元の手法を提案．視点の追加ごとに，点群表現のカーブとカメラポーズを新しいマッチング手法に基づき交互に最適化。また，オクルージョンを検知し誤対応を防止．カーブの各部で太さを推定することで高品質な復元が可能に． https://youtu.be/dI2FZG_txN0\\xa0',\n",
       "  'urls': ['https://youtu.be/dI2FZG_txN0'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278523792042496000',\n",
       "  'created_at': 1593659425000,\n",
       "  'date': '2020-07-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278526411813253121,\n",
       "  'likes_count': 7,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278526411813253121',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:10:25',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3D Human Mesh Regression with Dense Correspondence\\nCode:  https://github.com/zengwang430521/DecoMR\\xa0…\\nPaper:  https://arxiv.org/abs/2006.05734\\xa0',\n",
       "  'urls': ['https://github.com/zengwang430521/DecoMR',\n",
       "   'https://arxiv.org/abs/2006.05734'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278523792042496000',\n",
       "  'created_at': 1593658800000,\n",
       "  'date': '2020-07-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278523792042496000,\n",
       "  'likes_count': 52,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278523792042496000',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Eb42BPFUcAAAe1E.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 14,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '一枚の画像から人体の3D Meshを推定する手法.画像ピクセルと表面間の密な対応を推定し,その対応により画像空間からUV空間へ局所的な特徴が移され,位置マップに回帰される.最後にマッピング関数により3D Meshを再構成する.3D Meshベースの従来手法より優れていることを示した.\\n https://arxiv.org/abs/2006.05734\\xa0 pic.twitter.com/Ku422DMyAp',\n",
       "  'urls': ['https://arxiv.org/abs/2006.05734'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278243781104463872',\n",
       "  'created_at': 1593592041000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278243785235816448,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278243785235816448',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:27:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '“Self-supervised Simultaneous Alignment and Change Detection”,\\nYukuko Furukawa, Kumiko Suzuki, Ryuhei Hamaguchi, Masaki Onishi, Ken Sakurada',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278243781104463872',\n",
       "  'created_at': 1593592041000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278243784141074432,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278243784141074432',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:27:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '\"Non-overlapping RGB-D Camera Network Calibration with Monocular Visual Odometry\",\\nKenji Koide, Emanuele Menegatti',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278243781104463872',\n",
       "  'created_at': 1593592041000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278243783147089920,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278243783147089920',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:27:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '\"C*: Cross-modal Simultaneous Tracking And Rendering for 6-DoF Monocular Camera Localization Beyond Modalities\",\\nShuji Oishi, Yasunori Kawamata, Masashi Yokozuka, Kenji Koide, Atsuhiko Banno, Jun Miura',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278243781104463872',\n",
       "  'created_at': 1593592040000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278243782102708224,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278243782102708224',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:27:20',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '”LiTAMIN: LiDAR based Tracking And MappINg by Stabilized ICP for Geometry Approximation with Normal Distributions”,\\nMasashi Yokozuka, Kenji Koide, Shuji Oishi, Atsuhiko Banno',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278243781104463872',\n",
       "  'created_at': 1593592040000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278243781104463872,\n",
       "  'likes_count': 9,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278243781104463872',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 2,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '08:27:20',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SLAM-Hub members got 4 papers accepted by IROS2020!',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278242853508898816',\n",
       "  'created_at': 1593591977000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278243514950672385,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278243514950672385',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '08:26:17',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '“Self-supervised Simultaneous Alignment and Change Detection”,\\nYukuko Furukawa, Kumiko Suzuki, Ryuhei Hamaguchi, Masaki Onishi, Ken Sakurada',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278242853508898816',\n",
       "  'created_at': 1593591820000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278242856235225088,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278242856235225088',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:23:40',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '\"Non-overlapping RGB-D Camera Network Calibration with Monocular Visual Odometry\",\\nKenji Koide, Emanuele Menegatti',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278242853508898816',\n",
       "  'created_at': 1593591819000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278242855295676417,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278242855295676417',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:23:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '\"C*: Cross-modal Simultaneous Tracking And Rendering for 6-DoF Monocular Camera Localization Beyond Modalities\",\\nShuji Oishi, Yasunori Kawamata, Masashi Yokozuka, Kenji Koide, Atsuhiko Banno, Jun Miura',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278242853508898816',\n",
       "  'created_at': 1593591819000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278242854318424074,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278242854318424074',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '08:23:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '”LiTAMIN: LiDAR based Tracking And MappINg by Stabilized ICP for Geometry Approximation with Normal Distributions”,\\nMasashi Yokozuka, Kenji Koide, Shuji Oishi, Atsuhiko Banno',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278242853508898816',\n",
       "  'created_at': 1593591819000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278242853508898816,\n",
       "  'likes_count': 50,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278242853508898816',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '08:23:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ロボティクス分野の国際会議IROS2020にSLAM-Hubのメンバーから4本の論文が採択されました！',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278188128444940288',\n",
       "  'created_at': 1593578772000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278188129539645440,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278188129539645440',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '04:46:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Object-Centric Learning with Slot Attention (Under review)\\nPaper :  https://arxiv.org/abs/2006.15055\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2006.15055'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1278188128444940288',\n",
       "  'created_at': 1593578772000,\n",
       "  'date': '2020-07-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1278188128444940288,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1278188128444940288',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '04:46:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'シーン分解や集合予測のアーキテクチャに統合可能な，オブジェクト中心の抽象表現を学習するSlot Attentionモジュールを提案．CNNの出力と構造表現間において，順列不変なk個のSlotを生成．反復的注意メカニズムでSlotのグループ化戦略を学習．点群やグラフのグループ化も可能 https://youtu.be/DYBmD88vpiA\\xa0',\n",
       "  'urls': ['https://youtu.be/DYBmD88vpiA'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1277799017053859842',\n",
       "  'created_at': 1593486061000,\n",
       "  'date': '2020-06-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1277799271010467842,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1277799271010467842',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:01:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'An Analysis of SVD for Deep Rotation Estimation\\nPaper:  https://arxiv.org/abs/2006.14616\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2006.14616'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1277799017053859842',\n",
       "  'created_at': 1593486000000,\n",
       "  'date': '2020-06-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1277799017053859842,\n",
       "  'likes_count': 33,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1277799017053859842',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbuUgMbU8AEbJUJ.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '深層学習における性質の良い回転表現を提案．\\n回転行列を一度9パラメータで表現し，SVDによる特殊直交化によりSO(3)空間へマップする．\\n深層学習タスクにおいてクォータニオンやangle-axisベクトルなどの他の回転表現より高精度に姿勢を求めることが可能．\\n https://arxiv.org/abs/2006.14616\\xa0 pic.twitter.com/WVhQcNGBad',\n",
       "  'urls': ['https://arxiv.org/abs/2006.14616'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1277436627485290496',\n",
       "  'created_at': 1593399764000,\n",
       "  'date': '2020-06-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1277437317704069127,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1277437317704069127',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:02:44',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Grid-GCN for Fast and Scalable Point Cloud Learning (CVPR2020)\\nPaper:  https://arxiv.org/abs/1912.02984\\xa0\\nSupp:  https://xharlie.github.io/papers/GGCN_supCamReady.pdf\\xa0…\\nCode: https://github.com/Xharlie/Grid-GCN\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/1912.02984',\n",
       "   'https://xharlie.github.io/papers/GGCN_supCamReady.pdf',\n",
       "   'https://github.com/Xharlie/Grid-GCN'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1277436627485290496',\n",
       "  'created_at': 1593399600000,\n",
       "  'date': '2020-06-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1277436627485290496,\n",
       "  'likes_count': 34,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1277436627485290496',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbpbVrhVcAM26gU.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '高速でスケーラブルな点群データの処理機構を提案．Voxelを用いることで高速かつカバー率の高いサンプリングを行い，Voxel内部でローカルにグラフを構築して畳み込む．点群の分類とセグメンテーションで従来手法より高速かつ高精度を達成．\\n https://arxiv.org/abs/1912.02984\\xa0 pic.twitter.com/nlIsA1sC2v',\n",
       "  'urls': ['https://arxiv.org/abs/1912.02984'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1277074304786546689',\n",
       "  'created_at': 1593313226000,\n",
       "  'date': '2020-06-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1277074348273102848,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1277074348273102848',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:00:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PointTriNet: Learned Triangulation of 3D Point Sets (arXiv)\\nPaper:  https://arxiv.org/abs/2005.02138\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2005.02138'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1277074304786546689',\n",
       "  'created_at': 1593313215000,\n",
       "  'date': '2020-06-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1277074304786546689,\n",
       "  'likes_count': 34,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1277074304786546689',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbkUPnoUEAElg-3.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:15',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PointNetに類似したネットワークを用い点群から三角形メッシュを生成．入力点群から三角形群を出力するネットと，入力三角形群の中から3Dモデルとして妥当な三角形を判定するネットを交互に適用し，メッシュモデルを復元する．\\n https://arxiv.org/abs/2005.02138\\xa0 pic.twitter.com/8JeOljgxXg',\n",
       "  'urls': ['https://arxiv.org/abs/2005.02138'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276742451806404608',\n",
       "  'created_at': 1593247184000,\n",
       "  'date': '2020-06-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276797351001124866,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276797351001124866',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbgYaE-U4AAueo_.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '08:39:44',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Meshlet Priors for 3D Mesh Reconstruction (CVPR2020)\\nPaper:  https://arxiv.org/abs/2001.01744\\xa0\\nCode:  https://github.com/NVlabs/meshlets\\xa0 pic.twitter.com/9pOqOHCVFd',\n",
       "  'urls': ['https://arxiv.org/abs/2001.01744',\n",
       "   'https://github.com/NVlabs/meshlets'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276742451806404608',\n",
       "  'created_at': 1593234095000,\n",
       "  'date': '2020-06-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276742451806404608,\n",
       "  'likes_count': 27,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276742451806404608',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '05:01:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'スパースまたノイジーな点群からメッシュ生成のため、ローカル幾何形状を表現するMeshletを提案。VAEでMeshletをポーズ不変な潜在空間にエンコード、点群と近いMeshlet(補助メッシュから取出)をデコード、変形の補助メッシュを利用、Meshlet間のグローバルな整合性を強める。 https://youtu.be/glZyJ66ktog\\xa0',\n",
       "  'urls': ['https://youtu.be/glZyJ66ktog'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276361064104341506',\n",
       "  'created_at': 1593143225000,\n",
       "  'date': '2020-06-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276361313686454272,\n",
       "  'likes_count': 8,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276361313686454272',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbaL1U6UcAIdpnz.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:47:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'NRMVS: Non-Rigid Multi-View Stereo (WACV2020)\\nVideo:  https://www.youtube.com/watch?v=B4YBWFuYBdE\\xa0…\\nPaper:  http://openaccess.thecvf.com/content_WACV_2020/papers/Innmann_NRMVS_Non-Rigid_Multi-view_Stereo_WACV_2020_paper.pdf\\xa0… pic.twitter.com/5uHD5WJ9gC',\n",
       "  'urls': ['https://www.youtube.com/watch?v=B4YBWFuYBdE',\n",
       "   'http://openaccess.thecvf.com/content_WACV_2020/papers/Innmann_NRMVS_Non-Rigid_Multi-view_Stereo_WACV_2020_paper.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276361064104341506',\n",
       "  'created_at': 1593143165000,\n",
       "  'date': '2020-06-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276361064104341506,\n",
       "  'likes_count': 24,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276361064104341506',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:46:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '非剛体変形下での多視点ステレオの提案．まず変形の程度の小さい画像ペアを一組決定し，対象の基本的な3次元構造を復元．さらに，その他の変形を伴う画像群に対してもDeformation graphを利用したJoint optimizationにより，DeformationとDepthの推定を同時に行っている． https://youtu.be/B4YBWFuYBdE\\xa0',\n",
       "  'urls': ['https://youtu.be/B4YBWFuYBdE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276006512670830593',\n",
       "  'created_at': 1593058682000,\n",
       "  'date': '2020-06-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276006716606279680,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276006716606279680',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': 'https://twitter.com/slam_hub/status/1272001420745535489',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '04:18:02',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究 https://twitter.com/slam_hub/status/1272001420745535489\\xa0…',\n",
       "  'urls': ['https://twitter.com/slam_hub/status/1272001420745535489'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276006512670830593',\n",
       "  'created_at': 1593058668000,\n",
       "  'date': '2020-06-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276006657017839617,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276006657017839617',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '04:17:48',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping (ICRA 2020)\\nVideo:  https://youtu.be/-5XxXRABXJs\\xa0\\nCode:  https://github.com/MIT-SPARK/Kimera\\xa0…\\nPaper:  https://arxiv.org/abs/1910.02490\\xa0',\n",
       "  'urls': ['https://youtu.be/-5XxXRABXJs',\n",
       "   'https://github.com/MIT-SPARK/Kimera',\n",
       "   'https://arxiv.org/abs/1910.02490'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1276006512670830593',\n",
       "  'created_at': 1593058634000,\n",
       "  'date': '2020-06-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1276006512670830593,\n",
       "  'likes_count': 31,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1276006512670830593',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbVJJlXUwAEYf24.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 14,\n",
       "  'source': '',\n",
       "  'time': '04:17:14',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Visual-Inertial SLAM の Kimera。IMU の on-manifold preintegration と画像の Shi-Tomasi コーナー特徴点で VIO。DBoW2 でループ検出、GTSAM（iSAM2）でグラフ最適化。メッシュ生成と TSDF での復元。画像でセマンティックラベリングして逆投影し、ボクセルをベイズで更新。\\n https://arxiv.org/abs/1910.02490\\xa0 pic.twitter.com/RSnDbyYV23',\n",
       "  'urls': ['https://arxiv.org/abs/1910.02490'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1275630109311950849',\n",
       "  'created_at': 1592968946000,\n",
       "  'date': '2020-06-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1275630335091281920,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1275630335091281920',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:22:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Atlas: End-to-End 3D Scene Reconstruction from Posed Images (arXiv)\\nPaper:  https://arxiv.org/abs/2003.10432\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2003.10432'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1275630109311950849',\n",
       "  'created_at': 1592968892000,\n",
       "  'date': '2020-06-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1275630109311950849,\n",
       "  'likes_count': 39,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1275630109311950849',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbPyxA_UYAYaXp2.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:21:32',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '複数視点の2D-CNN出力をVoxelにBack-projetして蓄積し，Voxel mapを3D-CNNに通すことでSemantic情報を含んだMulti-view Stereoを実現．2D-CNNにはResnet50-FPN，3D-CNNはSkip Connectionを持つEncoder-decoderを利用．実時間処理が可能．\\n https://arxiv.org/abs/2003.10432\\xa0 pic.twitter.com/e3e3J3TEua',\n",
       "  'urls': ['https://arxiv.org/abs/2003.10432'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1275262302594293762',\n",
       "  'created_at': 1592881608000,\n",
       "  'date': '2020-06-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1275264014029742080,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1275264014029742080',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbKly60UMAAJ4aO.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:06:48',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Privacy-Preserving Visual Feature Descriptors through Adversarial Affine Subspace Embedding (arXiv)\\nPaper:  https://arxiv.org/abs/2006.06634\\xa0 pic.twitter.com/3BksIPzqMs',\n",
       "  'urls': ['https://arxiv.org/abs/2006.06634'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1275262302594293762',\n",
       "  'created_at': 1592881200000,\n",
       "  'date': '2020-06-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1275262302594293762,\n",
       "  'likes_count': 21,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1275262302594293762',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbH-5qqUMAU4zcQ.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '特徴量を，それ自身を含む部分アフィン空間へ埋め込みことで，識別機能を保ちながらプライバシーアタックへの耐性を大幅に向上．部分空間同士の距離を導入し特徴マッチングを可能とした．元の特徴量と比較して，僅かな識別性能の低下により高いプラバシー保護性能を実現．\\n https://arxiv.org/abs/2006.06634\\xa0 pic.twitter.com/ex4qczr200',\n",
       "  'urls': ['https://arxiv.org/abs/2006.06634'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274899915655843840',\n",
       "  'created_at': 1592795296000,\n",
       "  'date': '2020-06-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274901994814095360,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274901994814095360',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbFcmujVcAE3RWm.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:08:16',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/SZBbuTPgIl',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274899915655843840',\n",
       "  'created_at': 1592795272000,\n",
       "  'date': '2020-06-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274901891248304128,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274901891248304128',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:07:52',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes (CVPR2020 Oral)\\nPaper:  http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\\xa0…\\nCode: https://github.com/lzqsd/TransparentShapeReconstruction\\xa0…',\n",
       "  'urls': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html',\n",
       "   'https://github.com/lzqsd/TransparentShapeReconstruction'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274899915655843840',\n",
       "  'created_at': 1592794801000,\n",
       "  'date': '2020-06-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274899915655843840,\n",
       "  'likes_count': 44,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274899915655843840',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EbFP0N1U4AIpIdb.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 18,\n",
       "  'source': '',\n",
       "  'time': '03:00:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '複数画像からガラスオブジェクトの3D形状を再構築するネットワークを提案．Visual hullで得た荒い形状を元に，各視点で屈折，反射点の法線を推論．環境マップでレンダリングした再投影誤差と，視点間を統合した点群とGTとの損失で学習．高品質な3D復元が可能なことを実証．\\n http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\\xa0… pic.twitter.com/uXXFhuXt98',\n",
       "  'urls': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274537524241797121',\n",
       "  'created_at': 1592708760000,\n",
       "  'date': '2020-06-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274539034413432834,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274539034413432834',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:06:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'VPLNet: Deep Single View Normal Estimation With Vanishing Points and Lines (CVPR2020)\\nPaper:  http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\\xa0…',\n",
       "  'urls': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274537524241797121',\n",
       "  'created_at': 1592708400000,\n",
       "  'date': '2020-06-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274537524241797121,\n",
       "  'likes_count': 23,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274537524241797121',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ea82XPmUMAUDsh2.png',\n",
       "   'https://pbs.twimg.com/media/Ea82Y6-UYAAo1YY.jpg',\n",
       "   'https://pbs.twimg.com/media/Ea82ZqnUwAAdcoT.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 10,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '単一画像での法線推定手法の提案.RGB画像とマンハッタン線マップを入力とし,マンハッタン方向に沿う領域を識別するマップと法線マップをネットワークで回帰,融合する.従来手法より優れた結果を示し未見のデータに対しても推定可能なことを示した.\\n http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\\xa0… pic.twitter.com/vRPvFPGMEV',\n",
       "  'urls': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274205587597737985',\n",
       "  'created_at': 1592629260000,\n",
       "  'date': '2020-06-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274205589107703811,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274205589107703811',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '05:01:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World (CVPR2020)\\nPaper:  https://arxiv.org/abs/2006.09348\\xa0\\nSupp:  http://openaccess.thecvf.com/content_CVPR_2020/html/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.html\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2006.09348',\n",
       "   'http://openaccess.thecvf.com/content_CVPR_2020/html/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.html'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1274205587597737985',\n",
       "  'created_at': 1592629260000,\n",
       "  'date': '2020-06-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1274205587597737985,\n",
       "  'likes_count': 42,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1274205587597737985',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ea7i_wTU4AAQXtM.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 12,\n",
       "  'source': '',\n",
       "  'time': '05:01:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Lidarによる実世界データを用いた，従来のCADモデルによる手法よりも多彩で現実感の高い自動運転用シミュレーションを提案．Lidar点群から動的物体や環境マップなどのアセットを作成後，物理レンダリングとDNNでドメインギャップの小さなセンサシミュレーションを行う． https://arxiv.org/abs/2006.09348\\xa0 pic.twitter.com/nrtVaRw315',\n",
       "  'urls': ['https://arxiv.org/abs/2006.09348'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1273816472230363138',\n",
       "  'created_at': 1592536488000,\n",
       "  'date': '2020-06-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1273816473966800896,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1273816473966800896',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:14:48',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Revisiting visual-inertial structure from motion for odometry and SLAM initialization (arXiv)\\nPaper:  https://arxiv.org/abs/2006.06017\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2006.06017'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1273816472230363138',\n",
       "  'created_at': 1592536488000,\n",
       "  'date': '2020-06-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1273816472230363138,\n",
       "  'likes_count': 36,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1273816472230363138',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Ea2BLHfU0AA7NHO.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:14:48',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'VIO, VI-SLAMにおける状態変数の初期化手法を提案．効率的に不要変数を除去しつつ，3つ以上の3D点の観測を平等に扱う新たな定式化．この線形ソルバはシンプルな構造ながら過去の手法と比較してモーション推定の精度を最大50%向上させ，非線形ソルバの反復回数も削減．\\n https://arxiv.org/abs/2006.06017\\xa0 pic.twitter.com/IYHoycp0k8',\n",
       "  'urls': ['https://arxiv.org/abs/2006.06017'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1273450361563435008',\n",
       "  'created_at': 1592449286000,\n",
       "  'date': '2020-06-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1273450723481579522,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1273450723481579522',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:01:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PointAugment: an Auto-Augmentation Framework for Point Cloud Classification (CVPR2020)\\nPaper:  https://arxiv.org/abs/2002.10876\\xa0\\nCode: https://github.com/liruihui/PointAugment/\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2002.10876',\n",
       "   'https://github.com/liruihui/PointAugment/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1273450361563435008',\n",
       "  'created_at': 1592449200000,\n",
       "  'date': '2020-06-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1273450361563435008,\n",
       "  'likes_count': 25,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1273450361563435008',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Eawp7NpUEAAGRP1.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 12,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'End-to-endに学習可能な点群データのAugmentorを提案．入力点群ごとに全体の変形量と個々の点の変位量を出力し，分類器にとってより難しい変換となるよう敵対的に学習する．複数のモデルでランダムな水増しより良い精度を達成．\\n https://arxiv.org/abs/2002.10876\\xa0 pic.twitter.com/Zso2mevGNk',\n",
       "  'urls': ['https://arxiv.org/abs/2002.10876'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1273118847114018816',\n",
       "  'created_at': 1592370198000,\n",
       "  'date': '2020-06-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1273119004232683520,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1273119004232683520',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '05:03:18',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'LiDAR-based vehicle localization on the satellite image via a neural network (Robotics and Autonomous Systems 2020)\\nPaper:  https://www.sciencedirect.com/science/article/pii/S0921889019305202\\xa0…\\nVideo: https://www.sciencedirect.com/science/article/pii/S0921889019305202#mmc1\\xa0…',\n",
       "  'urls': ['https://www.sciencedirect.com/science/article/pii/S0921889019305202',\n",
       "   'https://www.sciencedirect.com/science/article/pii/S0921889019305202#mmc1'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1273118847114018816',\n",
       "  'created_at': 1592370161000,\n",
       "  'date': '2020-06-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1273118847114018816,\n",
       "  'likes_count': 59,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1273118847114018816',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EasGz34WsAAWoBV.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 24,\n",
       "  'source': '',\n",
       "  'time': '05:02:41',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'LIDARスキャンを基に衛星画像上での自己位置推定を行う手法を提案．推定にはパーティクルフィルタを用い，各パーティクル位置の衛星画像とLIDARスキャンの一致度を測るネットワークによって評価する．衛星画像上での遮蔽・陰影に頑強な位置推定が可能であることを示した．\\n https://www.sciencedirect.com/science/article/pii/S0921889019305202\\xa0… pic.twitter.com/4PelxvyMge',\n",
       "  'urls': ['https://www.sciencedirect.com/science/article/pii/S0921889019305202'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272725587359076352',\n",
       "  'created_at': 1592280759000,\n",
       "  'date': '2020-06-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272743867096301568,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272743867096301568',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/Eamxy93UEAAoAWY.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '04:12:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/EGCDq1YNgd',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272725587359076352',\n",
       "  'created_at': 1592276547000,\n",
       "  'date': '2020-06-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272726203741290497,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272726203741290497',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:02:27',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020 submission)\\nPaper:  https://arxiv.org/abs/2006.05518\\xa0\\nProject:  https://research.nvidia.com/publication/2020-06_MVLidarNet\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2006.05518',\n",
       "   'https://research.nvidia.com/publication/2020-06_MVLidarNet'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272725587359076352',\n",
       "  'created_at': 1592276400000,\n",
       "  'date': '2020-06-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272725587359076352,\n",
       "  'likes_count': 23,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272725587359076352',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '三次元点群の透視投影画像でセマンティックセグメンテーションを行い、分割結果が反応したBEV画像から物体を検出する、シンプルかつ高効率な2-stage検出手法を提案。既存手法と精度の差が大きくない上で、組み込みGPUでもマルチクラス物体検出と道路の分割を150 FPSで実現。 https://youtu.be/2ck5_sToayc\\xa0',\n",
       "  'urls': ['https://youtu.be/2ck5_sToayc'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272363203498930176',\n",
       "  'created_at': 1592190203000,\n",
       "  'date': '2020-06-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272364047854219265,\n",
       "  'likes_count': 7,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272364047854219265',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:03:23',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Global visual localization in LiDAR-maps through shared 2D-3D embedding space (ICRA2020)\\nVideo:  https://www.facebook.com/iralabdisco/videos/icra2020-submission-global-visual-localization-in-lidar-maps-through-shared-2d-3/371792790436848/\\xa0…\\nPaper:  https://arxiv.org/abs/1910.04871\\xa0',\n",
       "  'urls': ['https://www.facebook.com/iralabdisco/videos/icra2020-submission-global-visual-localization-in-lidar-maps-through-shared-2d-3/371792790436848/',\n",
       "   'https://arxiv.org/abs/1910.04871'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272363203498930176',\n",
       "  'created_at': 1592190001000,\n",
       "  'date': '2020-06-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272363203498930176,\n",
       "  'likes_count': 34,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272363203498930176',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EahWh7MU0AELAKE.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 10,\n",
       "  'source': '',\n",
       "  'time': '03:00:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '広域なLiDAR地図における単眼カメラの大域位置同定．異種のデータに対するShared embedding spaceを獲得するため，2D-CNNと3D-DNNを一緒に学習する枠組みを提案．同種データ内で完結するSame-Modality lossに加え，異種データ間でCross-Modality lossを用いて学習を行った．\\n https://arxiv.org/abs/1910.04871\\xa0 pic.twitter.com/0Ft0zuZoH8',\n",
       "  'urls': ['https://arxiv.org/abs/1910.04871'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272001420745535489',\n",
       "  'created_at': 1592103784000,\n",
       "  'date': '2020-06-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272001581278326785,\n",
       "  'likes_count': 10,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272001581278326785',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:03:04',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究\\nKimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping (ICRA 2020)\\nVideo:  https://youtu.be/-5XxXRABXJs\\xa0\\nCode:  https://github.com/MIT-SPARK/Kimera\\xa0…\\nPaper:  https://arxiv.org/abs/1910.02490\\xa0',\n",
       "  'urls': ['https://youtu.be/-5XxXRABXJs',\n",
       "   'https://github.com/MIT-SPARK/Kimera',\n",
       "   'https://arxiv.org/abs/1910.02490'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272001420745535489',\n",
       "  'created_at': 1592103769000,\n",
       "  'date': '2020-06-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272001518644781056,\n",
       "  'likes_count': 11,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272001518644781056',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:02:49',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans (RSS 2020)\\nVideo:  https://youtu.be/SWbofjhyPzI\\xa0\\nPaper:  https://arxiv.org/abs/2002.06289\\xa0',\n",
       "  'urls': ['https://youtu.be/SWbofjhyPzI', 'https://arxiv.org/abs/2002.06289'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1272001420745535489',\n",
       "  'created_at': 1592103746000,\n",
       "  'date': '2020-06-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1272001420745535489,\n",
       "  'likes_count': 311,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1272001420745535489',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EacObyPUwAAwARv.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 87,\n",
       "  'source': '',\n",
       "  'time': '03:02:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '建物、部屋、物体などの関係を表すシーングラフを動的環境に拡張し、3D Dynamic Scene Graphs を提案。Visual-Inertial SLAM の Kimera を用いてセマンティックマッピング。さらに移動物体（人のメッシュモデル）をトラッキングして、時空間の物体モデル構造を階層的に表現。\\n https://arxiv.org/abs/2002.06289\\xa0 pic.twitter.com/F5yOdQlh7F',\n",
       "  'urls': ['https://arxiv.org/abs/2002.06289'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1271651964275683329',\n",
       "  'created_at': 1592020452000,\n",
       "  'date': '2020-06-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1271652061159907328,\n",
       "  'likes_count': 10,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1271652061159907328',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EaXQzLHVcAEA1ad.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:54:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'OccuSeg: Occupancy-aware 3D Instance Segmentation (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.06537\\xa0\\nyoutube:  https://youtu.be/co7y6LQ7Kqc\\xa0 pic.twitter.com/PyloRJb7Z8',\n",
       "  'urls': ['https://arxiv.org/abs/2003.06537', 'https://youtu.be/co7y6LQ7Kqc'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1271651964275683329',\n",
       "  'created_at': 1592020429000,\n",
       "  'date': '2020-06-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1271651964275683329,\n",
       "  'likes_count': 27,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1271651964275683329',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:53:49',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Voxel ベースの U-Net でsemanticsを推定し，Super-voxel間の類似度を計算して3D instance segmentationを実現．U-Netで各Instanceに対するVoxelの占有数(＝体積)を推定し，適切にSuper-voxelをクラスタリングして Instance を生成．ScanNet Benchmark の現在１位． https://youtu.be/co7y6LQ7Kqc\\xa0',\n",
       "  'urls': ['https://youtu.be/co7y6LQ7Kqc'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1271276037117218816',\n",
       "  'created_at': 1591931804000,\n",
       "  'date': '2020-06-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1271280247061676039,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1271280247061676039',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:16:44',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3D Photography using Context-aware Layered Depth Inpainting (CVPR2020)\\nPaper:  https://arxiv.org/abs/2004.04727\\xa0\\nProject:  https://shihmengli.github.io/3D-Photo-Inpainting/\\xa0…\\nCode: https://github.com/vt-vl-lab/3d-photo-inpainting\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2004.04727',\n",
       "   'https://shihmengli.github.io/3D-Photo-Inpainting/',\n",
       "   'https://github.com/vt-vl-lab/3d-photo-inpainting'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1271276037117218816',\n",
       "  'created_at': 1591930801000,\n",
       "  'date': '2020-06-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1271276037117218816,\n",
       "  'likes_count': 22,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1271276037117218816',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:00:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RGB-D単眼画像を入力とし，視点を変えると発生する空白領域をインペインティングするモデルの提案．Depthの断層と層状のDepth表現という着想をベースに，各層で背景を外側へ補完するようにRGB-Dを推定．Mesh表現に変換することで，エッジデバイスでも軽快に動作可能． https://www.youtube.com/watch?v=D0JObXCfxv0\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=D0JObXCfxv0'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270913650086178817',\n",
       "  'created_at': 1591845790000,\n",
       "  'date': '2020-06-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270919475919900674,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270919475919900674',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:23:10',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Point2Mesh: A Self-Prior for Deformable Meshes (SIGGRAPH2020)\\nProject:\\n https://ranahanocka.github.io/point2mesh/\\xa0\\nCode:\\n https://github.com/ranahanocka/Point2Mesh/\\xa0…\\nYoutube: https://www.youtube.com/watch?v=AySwwJuPqOk&feature=emb_title\\xa0…',\n",
       "  'urls': ['https://ranahanocka.github.io/point2mesh/',\n",
       "   'https://github.com/ranahanocka/Point2Mesh/',\n",
       "   'https://www.youtube.com/watch?v=AySwwJuPqOk&feature=emb_title'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270913650086178817',\n",
       "  'created_at': 1591844401000,\n",
       "  'date': '2020-06-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270913650086178817,\n",
       "  'likes_count': 28,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270913650086178817',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '03:00:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ノイズや欠損を含む点群から高精細な水密メッシュモデルを生成する手法を提案．coarse-to-fineで初期メッシュのエッジの移動量を推定し，入力点群自身と損失を計算して反復的に誤差逆伝搬することでself priorを学習．平滑化仮定では生成できない微細なメッシュも生成可能．\\n https://arxiv.org/abs/2005.11084\\xa0 pic.twitter.com/QLCq7LsaEX',\n",
       "  'urls': ['https://arxiv.org/abs/2005.11084'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 1},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270551260056387585',\n",
       "  'created_at': 1591758755000,\n",
       "  'date': '2020-06-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270554426483212290,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270554426483212290',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EaHqfneVAAEJIqZ.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:12:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/LECto0vyTT',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270551260056387585',\n",
       "  'created_at': 1591758755000,\n",
       "  'date': '2020-06-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270554425287774209,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270554425287774209',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EaHqedJU4AEvBDV.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:12:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/UAT3HsEmrY',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270551260056387585',\n",
       "  'created_at': 1591758755000,\n",
       "  'date': '2020-06-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270554424159563778,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270554424159563778',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:12:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Novel Object Viewpoint Estimation through Reconstruction Alignment (CVPR 2020)\\nProject:  https://mbanani.github.io/novelviewpoints/\\xa0…\\nCode: https://github.com/mbanani/novelviewpoints\\xa0…',\n",
       "  'urls': ['https://mbanani.github.io/novelviewpoints/',\n",
       "   'https://github.com/mbanani/novelviewpoints'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270551260056387585',\n",
       "  'created_at': 1591758000000,\n",
       "  'date': '2020-06-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270551260056387585,\n",
       "  'likes_count': 21,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270551260056387585',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EaE9eV9UwAEPo9z.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '未知物体を撮影した画像間の相対姿勢の推定. 学習に用いられた物体以外の視点の推定は困難であったが２枚の画像を3D特徴グリッドにマッピングする学習を行い位置を合わせることで相対的な位置を推定する.学習時と大きく異なる物体で推論する際に従来手法より良い精度を示した.\\n https://arxiv.org/abs/2006.03586\\xa0 pic.twitter.com/SktdplZMb1',\n",
       "  'urls': ['https://arxiv.org/abs/2006.03586'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270188872232660993',\n",
       "  'created_at': 1591671744000,\n",
       "  'date': '2020-06-09',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270189473901371400,\n",
       "  'likes_count': 10,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270189473901371400',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:02:24',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '4D Visualization of Dynamic Events from Unconstrained Multi-View Videos (CVPR2020)\\nProject:  http://www.cs.cmu.edu/~aayushb/Open4D/\\xa0…\\nCode:  https://github.com/aayushbansal/Open4D\\xa0…\\nPaper:  https://arxiv.org/abs/2005.13532\\xa0',\n",
       "  'urls': ['http://www.cs.cmu.edu/~aayushb/Open4D/',\n",
       "   'https://github.com/aayushbansal/Open4D',\n",
       "   'https://arxiv.org/abs/2005.13532'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1270188872232660993',\n",
       "  'created_at': 1591671600000,\n",
       "  'date': '2020-06-09',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1270188872232660993,\n",
       "  'likes_count': 39,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1270188872232660993',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 15,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '複数カメラで撮影された動的イベントに対して，視点と時間を移動可能な4次元時空間可視化を行うシステム．シーン特化のself-supervisedなCNNを用いて静的・動的部分の抽出を行う．SfMによる既存手法で困難であった非ランバート面や，テクスチャレスな領域もキャプチャ可能に． https://youtu.be/sq2hhkHgtb0\\xa0',\n",
       "  'urls': ['https://youtu.be/sq2hhkHgtb0'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1269826483599618049',\n",
       "  'created_at': 1591585693000,\n",
       "  'date': '2020-06-08',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1269828549495492611,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1269828549495492611',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:08:13',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'An Efficient Planar Bundle Adjustment Algorithm\\nPaper:  https://arxiv.org/abs/2006.00187\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2006.00187'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1269826483599618049',\n",
       "  'created_at': 1591585200000,\n",
       "  'date': '2020-06-08',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1269826483599618049,\n",
       "  'likes_count': 35,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1269826483599618049',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZ6nHJqUwAA9OC7.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '点群が平面状に分布する制約を加えたPlaner Bundle Adjustmentを提案．ヤコビアン行列のコンパクトな表現を含む新たな定式化によって精度向上と計算量の削減を両立．評価実験で同問題設定のSOTAと比較して高速，高精度に，そして初期値にロバストなことが示された．\\n https://arxiv.org/abs/2006.00187\\xa0 pic.twitter.com/rVKDwkK3LH',\n",
       "  'urls': ['https://arxiv.org/abs/2006.00187'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1269519361787613184',\n",
       "  'created_at': 1591511977000,\n",
       "  'date': '2020-06-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1269519363230449665,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1269519363230449665',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '06:39:37',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.01251\\xa0\\nCode: https://github.com/WeijingShi/Point-GNN\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2003.01251',\n",
       "   'https://github.com/WeijingShi/Point-GNN'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1269519361787613184',\n",
       "  'created_at': 1591511977000,\n",
       "  'date': '2020-06-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1269519361787613184,\n",
       "  'likes_count': 44,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1269519361787613184',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZ483GVU0AEdfeG.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '06:39:37',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'GNNを用いた三次元点群からの物体検出手法を提案．近傍点を結んだグラフからGNNで特徴抽出し，点ごとに所属する物体クラスとBBOXを推定．最後に重複したBBOXを中央値で統合する．KITTTIデータセットで従来手法を上回る精度を達成．\\n https://arxiv.org/abs/2003.01251\\xa0 pic.twitter.com/Qu1jOIW3xo',\n",
       "  'urls': ['https://arxiv.org/abs/2003.01251'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1269104347004022785',\n",
       "  'created_at': 1591413072000,\n",
       "  'date': '2020-06-06',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1269104524221755395,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1269104524221755395',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZzDyfDUcAEfnyk.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:11:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking (RSS2019)\\nPaper:  https://arxiv.org/abs/1905.09304\\xa0\\nPresentation:  https://youtu.be/pknL_nyirZ4?t=295\\xa0… pic.twitter.com/c5IaCqhBli',\n",
       "  'urls': ['https://arxiv.org/abs/1905.09304',\n",
       "   'https://youtu.be/pknL_nyirZ4?t=295'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1269104347004022785',\n",
       "  'created_at': 1591413029000,\n",
       "  'date': '2020-06-06',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1269104347004022785,\n",
       "  'likes_count': 25,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1269104347004022785',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:10:29',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '6DoF物体姿勢推定のためのRBPFを提案．姿勢分布を分解し，平行移動はサンプリング，回転は物体の各回転に対するEmbeddingを予め計算しておき，パーティクルのEmbeddingをこれと比較することで評価．6DoFを200パーティクル程度でロバストに推定しSOTA精度． https://youtu.be/lE5gjzRKWuA\\xa0',\n",
       "  'urls': ['https://youtu.be/lE5gjzRKWuA'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268739321369989121',\n",
       "  'created_at': 1591326347000,\n",
       "  'date': '2020-06-05',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268740773882654720,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268740773882654720',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZt4zsqU4AEOSMB.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:05:47',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds (ICLR2020)\\nProject:  https://ge.in.tum.de/publications/2020-iclr-prantl/\\xa0…\\nCode:  https://gitlab.com/Prantl/NeuralParticles\\xa0…\\nPaper:  https://openreview.net/forum?id=BJeKh3VYDH\\xa0… pic.twitter.com/ZcMRH1HQqc',\n",
       "  'urls': ['https://ge.in.tum.de/publications/2020-iclr-prantl/',\n",
       "   'https://gitlab.com/Prantl/NeuralParticles',\n",
       "   'https://openreview.net/forum?id=BJeKh3VYDH'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268739321369989121',\n",
       "  'created_at': 1591326000000,\n",
       "  'date': '2020-06-05',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268739321369989121,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268739321369989121',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '安定的にフレキシブルな時系列点群の生成手法を提案。既存手法の安定性と多様性の両立し難い問題を改善するため、新たなTemporal Lossを導入、点群から時間的一貫性がある特徴を学習し、変形可能な数が多い点群にたしても有効性を示す。 https://youtu.be/6OoRZrqfSJ4\\xa0',\n",
       "  'urls': ['https://youtu.be/6OoRZrqfSJ4'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268387507969785856',\n",
       "  'created_at': 1591242155000,\n",
       "  'date': '2020-06-04',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268387646407012352,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268387646407012352',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:42:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'FroDO: From Detections to 3D Objects (CVPR2020)\\nProject:  https://research.fb.com/publications/frodo-from-detections-to-3d-objects/\\xa0…\\nPaper:  https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\\xa0…',\n",
       "  'urls': ['https://research.fb.com/publications/frodo-from-detections-to-3d-objects/',\n",
       "   'https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268387507969785856',\n",
       "  'created_at': 1591242122000,\n",
       "  'date': '2020-06-04',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268387507969785856,\n",
       "  'likes_count': 26,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268387507969785856',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZo3gFiUcAAONHP.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 11,\n",
       "  'source': '',\n",
       "  'time': '03:42:02',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '多視点のRGB画像列に基づく物体の3次元復元手法を提案．Point / Surface による相補的な形状デコードにより形状表現の効率性と記述力を両立させており，より高速な形状復元を実現している．\\n https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\\xa0… pic.twitter.com/Y0iFz7omo2',\n",
       "  'urls': ['https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268013854698557440',\n",
       "  'created_at': 1591156451000,\n",
       "  'date': '2020-06-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268028179727302659,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268028179727302659',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': 'https://twitter.com/slam_hub/status/1256059179724271616',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:54:11',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究 https://twitter.com/slam_hub/status/1256059179724271616\\xa0…',\n",
       "  'urls': ['https://twitter.com/slam_hub/status/1256059179724271616'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268013854698557440',\n",
       "  'created_at': 1591153185000,\n",
       "  'date': '2020-06-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268014480849395712,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268014480849395712',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZjkY-5U4AEWXYV.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '02:59:45',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'OverlapNet: Loop Closing for LiDAR-based SLAM (RSS 2020)\\nProject:  https://www.ipb.uni-bonn.de/people/xieyuanli-chen/\\xa0…\\nCode (coming soon):  https://github.com/PRBonn/OverlapNet\\xa0…\\nPaper (pdf):  https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf\\xa0… pic.twitter.com/8Yy5coRclW',\n",
       "  'urls': ['https://www.ipb.uni-bonn.de/people/xieyuanli-chen/',\n",
       "   'https://github.com/PRBonn/OverlapNet',\n",
       "   'https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1268013854698557440',\n",
       "  'created_at': 1591153036000,\n",
       "  'date': '2020-06-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1268013854698557440,\n",
       "  'likes_count': 45,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1268013854698557440',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 11,\n",
       "  'source': '',\n",
       "  'time': '02:57:16',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SLAM で重要なループ検出を、位置合わせなしで end-to-end に実現。3D-LIDAR の距離画像、法線、受光強度、セマンティクスを入力。2つのスキャンの重複率とヨー角を推定。ループ拘束は SLAM 側で求める。重なりが小さくても適切にループ検出し、SuMa より高精度な地図を構築。 https://youtu.be/YTfliBco6aw\\xa0',\n",
       "  'urls': ['https://youtu.be/YTfliBco6aw'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1267727467101315072',\n",
       "  'created_at': 1591084810000,\n",
       "  'date': '2020-06-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1267727696982732802,\n",
       "  'likes_count': 8,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1267727696982732802',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZffmu9UMAAhiAU.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '08:00:10',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'A Modular Optimization Framework for Localization and Mapping (RSS2019)\\nCode:  https://github.com/MOLAorg/mola\\xa0\\nPaper:  http://www.roboticsproceedings.org/rss15/p43.pdf\\xa0\\nPresentation:  https://youtu.be/qwh8hGEJSlA\\xa0 pic.twitter.com/KLDOehbecC',\n",
       "  'urls': ['https://github.com/MOLAorg/mola',\n",
       "   'http://www.roboticsproceedings.org/rss15/p43.pdf',\n",
       "   'https://youtu.be/qwh8hGEJSlA'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1267727467101315072',\n",
       "  'created_at': 1591084756000,\n",
       "  'date': '2020-06-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1267727467101315072,\n",
       "  'likes_count': 34,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1267727467101315072',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 18,\n",
       "  'source': '',\n",
       "  'time': '07:59:16',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SLAMを構成要素(入出力, フロント/バックエンド, マップストレージ)に分割し，センサ種類・個数，マッピング方式(global map vs local submaps)，状態空間(SE2/SE3/SE3+vel)などの違いを包括的に扱えるミドルウェア寄りのライブラリを提案． http://youtu.be/Bb92aMBJR44\\xa0',\n",
       "  'urls': ['http://youtu.be/Bb92aMBJR44'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1267289768544460800',\n",
       "  'created_at': 1590980738000,\n",
       "  'date': '2020-06-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1267291184252112896,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1267291184252112896',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:05:38',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Towards Better Generalization: Joint Depth-Pose Learning without PoseNet (CVPR2020)\\nGitHub https://github.com/B1ueber2y/TrianFlow\\xa0…',\n",
       "  'urls': ['https://github.com/B1ueber2y/TrianFlow'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1267289768544460800',\n",
       "  'created_at': 1590980400000,\n",
       "  'date': '2020-06-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1267289768544460800,\n",
       "  'likes_count': 50,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1267289768544460800',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZVwVrFUMAIJfoT.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 14,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'オプティカルフローを経由して8点法により直接推定した相対姿勢，さらにそこから計算した３次元点を自己教師とすることで，スケールの推定をネットワークから分離し，高い汎化性能とスケールの一貫性を実現．屋内外のデータセットでORB-SLAMや学習ベースの手法を凌駕．\\n https://arxiv.org/abs/2004.01314\\xa0 pic.twitter.com/XU3nSsMtX4',\n",
       "  'urls': ['https://arxiv.org/abs/2004.01314'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266927379089248256',\n",
       "  'created_at': 1590894053000,\n",
       "  'date': '2020-05-31',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266927603731935232,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266927603731935232',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:00:53',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-motion from Monocular Videos (IROS 2020)',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266927379089248256',\n",
       "  'created_at': 1590894000000,\n",
       "  'date': '2020-05-31',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266927379089248256,\n",
       "  'likes_count': 36,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266927379089248256',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZRdRrBVcAckBzq.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 12,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Unsupervised単眼Depth推定で精度を改善する2つの機構の提案．特定のDepth誤りを，フォトメトリックエラーをもとにした外れ値Maskを導入し対処．また，重み付きマルチスケール機構でアーティファクトを除去．簡単に追加できる機構で，他手法よりも高い精度を達成．\\n https://arxiv.org/abs/2003.01360\\xa0 pic.twitter.com/I0JA9KV4Lo',\n",
       "  'urls': ['https://arxiv.org/abs/2003.01360'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266564992805076997',\n",
       "  'created_at': 1590808087000,\n",
       "  'date': '2020-05-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266567037158977537,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266567037158977537',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZO_76lUMAATF-p.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:08:07',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/Fs98y7CS2Z',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266564992805076997',\n",
       "  'created_at': 1590808087000,\n",
       "  'date': '2020-05-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266567035976212481,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266567035976212481',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZO_60XUEAEUP-Q.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:08:07',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/96fNlTaANF',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266564992805076997',\n",
       "  'created_at': 1590808087000,\n",
       "  'date': '2020-05-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266567034449432577,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266567034449432577',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZO_3FyUcAAH5l_.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:08:07',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Deep Implicit Volume Compression (CVPR 2020 Oral)\\nProject  https://augmentedperception.github.io/deep_volume_compression/\\xa0…\\nPaper\\n https://arxiv.org/abs/2005.08877\\xa0 pic.twitter.com/AujHzInHUf',\n",
       "  'urls': ['https://augmentedperception.github.io/deep_volume_compression/',\n",
       "   'https://arxiv.org/abs/2005.08877'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266564992805076997',\n",
       "  'created_at': 1590807600000,\n",
       "  'date': '2020-05-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266564992805076997,\n",
       "  'likes_count': 22,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266564992805076997',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Volumetricな表現で使用されるTSDFとそれに対応したテクスチャを圧縮する新しい方法を提案．End-to-Endで訓練されたニューラルネットを用い，トポロジカルなエラーを防ぐためにTSDFの符号を失わずに圧縮する．従来手法より優れた圧縮率と歪みのトレードオフを得た． https://youtu.be/GuLzjnFGDKs\\xa0',\n",
       "  'urls': ['https://youtu.be/GuLzjnFGDKs'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266202604868075520',\n",
       "  'created_at': 1590721555000,\n",
       "  'date': '2020-05-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266204094286323712,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266204094286323712',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:05:55',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'VDO-SLAM: A Visual Dynamic Object-aware SLAM System (submitted to International Journal of Robotics Research)\\nCode: https://github.com/halajun/vdo_slam\\xa0…',\n",
       "  'urls': ['https://github.com/halajun/vdo_slam'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1266202604868075520',\n",
       "  'created_at': 1590721200000,\n",
       "  'date': '2020-05-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1266202604868075520,\n",
       "  'likes_count': 47,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1266202604868075520',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZJizyPUYAE03aP.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 2,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 11,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'モーションセグメンテーション，動的物体追跡，カメラ姿勢，シーン剛体の姿勢変化や速度の計算を全て行い，実世界の屋外シナリオで実証可能な世界初の動的SLAMシステムを提案．ロバスト性の向上の為，カメラと物体の動きの推定はOptical Flowの改良と合わせて因子グラフ最適化\\n https://arxiv.org/abs/2005.11052\\xa0 pic.twitter.com/esR5t04wPi',\n",
       "  'urls': ['https://arxiv.org/abs/2005.11052'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265849825678913537',\n",
       "  'created_at': 1590637092000,\n",
       "  'date': '2020-05-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265849828862386176,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265849828862386176',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:38:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究\\nSuperPoint: Self-Supervised Interest Point Detection and Description\\nPaper:  https://arxiv.org/abs/1712.07629\\xa0\\nNeural-Guided RANSAC: Learning Where to Sample Model Hypotheses\\nPaper:  https://arxiv.org/abs/1905.04132\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/1712.07629',\n",
       "   'https://arxiv.org/abs/1905.04132'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265849825678913537',\n",
       "  'created_at': 1590637091000,\n",
       "  'date': '2020-05-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265849826740064257,\n",
       "  'likes_count': 7,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265849826740064257',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EZEzlAbUwAAzZhV.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:38:11',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Reinforced Feature Points:Optimizing Feature Detection and Description for a High-Level Task (CVPR2020 Oral)\\nPaper:  https://arxiv.org/abs/1912.00623\\xa0 pic.twitter.com/bGcxPKEvcA',\n",
       "  'urls': ['https://arxiv.org/abs/1912.00623'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265849825678913537',\n",
       "  'created_at': 1590637091000,\n",
       "  'date': '2020-05-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265849825678913537,\n",
       "  'likes_count': 26,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265849825678913537',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:38:11',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像の特徴点検出と記述子表現をend-to-endに学習する手法を提案．特徴点マッチングで誤差伝搬できないため，画像間の相対姿勢誤差を負の報酬ににした強化学習で特徴点検出CNNと記述子推定CNNをトレーニングする．学習ベースの局所特徴量抽出器としてSOTAを達成． https://youtu.be/Zttl3eDjNyc\\xa0',\n",
       "  'urls': ['https://youtu.be/Zttl3eDjNyc'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265477828973416448',\n",
       "  'created_at': 1590548633000,\n",
       "  'date': '2020-05-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265478805390389249,\n",
       "  'likes_count': 12,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265478805390389249',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EY_iMYUUwAEQBz-.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:03:53',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SampleNet: Differentiable Point Cloud Sampling (CVPR2020)\\nCode:  https://github.com/itailang/SampleNet\\xa0…\\nPaper:  https://arxiv.org/abs/1912.03663\\xa0\\nRelated:  https://arxiv.org/abs/1812.01659\\xa0 pic.twitter.com/grZNpshhiI',\n",
       "  'urls': ['https://github.com/itailang/SampleNet',\n",
       "   'https://arxiv.org/abs/1912.03663',\n",
       "   'https://arxiv.org/abs/1812.01659'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265477828973416448',\n",
       "  'created_at': 1590548400000,\n",
       "  'date': '2020-05-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265477828973416448,\n",
       "  'likes_count': 27,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265477828973416448',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '微分可能な三次元点群のサンプリング手法を提案．NNによって入力点群を簡素化するDovratらの手法を拡張．最近傍サンプリングをk近傍の重み付き和で近似することで，簡素化した点群を基に入力点群をサンプリングするステップを微分可能にした． https://www.youtube.com/watch?v=JHz_ImeI8HE\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=JHz_ImeI8HE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265120071728283648',\n",
       "  'created_at': 1590463130000,\n",
       "  'date': '2020-05-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265120180503363584,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265120180503363584',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EY6cF5YVcAExBlu.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:18:50',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'G2L-Net: Global to Local Network for Real-time 6D Pose Estimation withEmbedding Vector Features (CVPR2020)\\nVideo:  https://youtu.be/a5JWe6mOAEs\\xa0\\nCode:  https://github.com/DC1991/G2L_Net\\xa0\\nPaper:  https://arxiv.org/abs/2003.11089\\xa0 pic.twitter.com/lghuSEkzDC',\n",
       "  'urls': ['https://youtu.be/a5JWe6mOAEs',\n",
       "   'https://github.com/DC1991/G2L_Net',\n",
       "   'https://arxiv.org/abs/2003.11089'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1265120071728283648',\n",
       "  'created_at': 1590463104000,\n",
       "  'date': '2020-05-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1265120071728283648,\n",
       "  'likes_count': 40,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1265120071728283648',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 14,\n",
       "  'source': '',\n",
       "  'time': '03:18:24',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RGB-D画像から特定の物体の6DoF姿勢を3段階で推定．1.2D物体認識で対象物を含む点群を抽出．2.PointNetを利用し詳細な物体抽出と並進量を推定．3.並進後，回転量をPointNetで推定．回転量推定の学習には，各点に付加した方向ベクトルが真値の方向になるように学習． https://youtu.be/a5JWe6mOAEs\\xa0',\n",
       "  'urls': ['https://youtu.be/a5JWe6mOAEs'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1264753052122308615',\n",
       "  'created_at': 1590376401000,\n",
       "  'date': '2020-05-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1264756414146211841,\n",
       "  'likes_count': 7,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1264756414146211841',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:13:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes (CVPR2020)\\nPaper:  https://arxiv.org/abs/2001.10692\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2001.10692'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1264753052122308615',\n",
       "  'created_at': 1590375600000,\n",
       "  'date': '2020-05-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1264753052122308615,\n",
       "  'likes_count': 50,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1264753052122308615',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYw3pD7U4AESaXj.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像からgeometric、semantic、textureのVote特徴を抽出、三次元点群のVote特徴と融合し、3D物体検出の手法を提案。Multi-modalデータ融合を改善するにmulti-towerとgradient blendingの構造を使用し、SUNRGB-Dで既存SOTAより5.7mAPの精度を向上させ、SLAMようなSparse点群に対する有効性も確認。 pic.twitter.com/6zfURbMEPw',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1264390814454607872',\n",
       "  'created_at': 1590289251000,\n",
       "  'date': '2020-05-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1264390880292507648,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1264390880292507648',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:00:51',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PrimiTect: Fast Continuous Hough Voting for Primitive Detection (ICRA2020)\\nCode:  https://github.com/c-sommer/primitect\\xa0…\\nPaper:  https://arxiv.org/abs/2005.07457\\xa0',\n",
       "  'urls': ['https://github.com/c-sommer/primitect',\n",
       "   'https://arxiv.org/abs/2005.07457'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1264390814454607872',\n",
       "  'created_at': 1590289236000,\n",
       "  'date': '2020-05-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1264390814454607872,\n",
       "  'likes_count': 44,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1264390814454607872',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYwEihiU8AAOrk6.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:36',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RGB-Dカメラ等で計測された3次元点群における幾何プリミティブ(円柱，円錐，球)の検出．Point Pair Feature に各幾何プリミティブの形状に応じた拘束を導入，またLinear interpolation votingを幾何プリミティブ用に改良し，ハフ変換による低計算量での頑健な検出を実現．\\n https://arxiv.org/abs/2005.07457\\xa0 pic.twitter.com/AylPuLfTAw',\n",
       "  'urls': ['https://arxiv.org/abs/2005.07457'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1264027851038154753',\n",
       "  'created_at': 1590202735000,\n",
       "  'date': '2020-05-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1264028005694763008,\n",
       "  'likes_count': 11,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1264028005694763008',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '02:58:55',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Visual Odometry Revisited: What Should Be Learnt? (ICRA 2020)\\nVideo:  https://youtu.be/Nl8mFU4SJKY\\xa0\\nCode:  https://github.com/Huangying-Zhan/DF-VO\\xa0…\\nPaper:  https://arxiv.org/abs/1909.09803\\xa0',\n",
       "  'urls': ['https://youtu.be/Nl8mFU4SJKY',\n",
       "   'https://github.com/Huangying-Zhan/DF-VO',\n",
       "   'https://arxiv.org/abs/1909.09803'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1264027851038154753',\n",
       "  'created_at': 1590202698000,\n",
       "  'date': '2020-05-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1264027851038154753,\n",
       "  'likes_count': 122,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1264027851038154753',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 24,\n",
       "  'source': '',\n",
       "  'time': '02:58:18',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'End-to-End 深層学習の Visual Odometry (VO) の性能は、まだ幾何ベースの手法に及ばない。そこで VO の基礎を再検討し、エピポーラ幾何や PnP と深層学習を組み合わせる手法を提案。スケール整合性のある単眼深度推定とオプティカルフローの2つの CNN を利用。KITTI dataset の評価で従来手法を凌駕。 pic.twitter.com/9C4yJ9i1gv',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 1},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1263670128400404480',\n",
       "  'created_at': 1590117462000,\n",
       "  'date': '2020-05-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1263670343962554368,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1263670343962554368',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:17:42',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences (CVPR2020)\\nPaper\\n https://arxiv.org/abs/2005.01014\\xa0\\nCode (中身はまだ未公開？) https://github.com/XiaoshuiHuang/fmr\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2005.01014',\n",
       "   'https://github.com/XiaoshuiHuang/fmr'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1263670128400404480',\n",
       "  'created_at': 1590117411000,\n",
       "  'date': '2020-05-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1263670128400404480,\n",
       "  'likes_count': 34,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1263670128400404480',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYl1RBmU8AIEVoz.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:16:51',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '二つの点群について，特徴空間上で並進・回転に対するヤコビアンを数値微分で求めLucus-Kanade法で位置合わせを実行．また点群に対するEncoder-Decorderを構築し特徴をUn-supervisedまたはSemi-supervisedで学習を可能にした．\\n https://arxiv.org/abs/2005.01014\\xa0 pic.twitter.com/JZeCcd11hS',\n",
       "  'urls': ['https://arxiv.org/abs/2005.01014'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1263303500836069376',\n",
       "  'created_at': 1590030313000,\n",
       "  'date': '2020-05-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1263304812994543616,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1263304812994543616',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:05:13',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Self-Supervised Scene De-occlusion (CVPR2020 Oral)\\nParper:  https://arxiv.org/abs/2004.02788\\xa0\\nProject:  https://xiaohangzhan.github.io/projects/deocclusion/\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2004.02788',\n",
       "   'https://xiaohangzhan.github.io/projects/deocclusion/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1263303500836069376',\n",
       "  'created_at': 1590030000000,\n",
       "  'date': '2020-05-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1263303500836069376,\n",
       "  'likes_count': 24,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1263303500836069376',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'self-supervisedに学習可能な，画像のオクルージョン領域を復元するモデルの提案．物体ごとの被オクルージョン領域推定で，増加範囲からオクルージョン関係のグラフを構築．その後推定した領域MaskからRGBを復元．教師あり学習に匹敵するパフォーマンスを達成． https://www.youtube.com/watch?v=xIHCyyaB5gU\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=xIHCyyaB5gU'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1262941112664117248',\n",
       "  'created_at': 1589943712000,\n",
       "  'date': '2020-05-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1262941585102311425,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1262941585102311425',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:01:52',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Self-Supervised Deep Visual Odometry with Online Adaptation (CVPR 2020 Oral)',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1262941112664117248',\n",
       "  'created_at': 1589943600000,\n",
       "  'date': '2020-05-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1262941112664117248,\n",
       "  'likes_count': 30,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1262941112664117248',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYX8UGzUcAErmZZ.png',\n",
       "   'https://pbs.twimg.com/media/EYX8UySU0AEph5J.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '自己教師Visual Odometry手法を提案．畳み込みLSTMを利用することで過去の経験を活かして推定をし，未知のシーンにオンラインで適応することが可能．さらにオープンワールドでの環境の変化に対応するために特徴量を揃える手法を提案．既存の手法を大きく上回ることを実験で確認 https://arxiv.org/abs/2005.06136\\xa0 pic.twitter.com/YMw3oYqEK3',\n",
       "  'urls': ['https://arxiv.org/abs/2005.06136'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1262595837172973569',\n",
       "  'created_at': 1589861280000,\n",
       "  'date': '2020-05-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1262595838611615744,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1262595838611615744',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '04:08:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection (CVPR2020)\\nGitHub https://github.com/mileyan/pseudo-LiDAR_e2e\\xa0…',\n",
       "  'urls': ['https://github.com/mileyan/pseudo-LiDAR_e2e'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1262595837172973569',\n",
       "  'created_at': 1589861280000,\n",
       "  'date': '2020-05-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1262595837172973569,\n",
       "  'likes_count': 43,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1262595837172973569',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYWkGGCUcAAMG4l.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '04:08:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像ベースの深度マップを擬似LiDARに変換し3次元物体検出するというパイプライン全体を，End-to-Endで学習するフレームワーク．既存手法では深度推定と物体検出で別学習していたが，間の表現変化をプーリングと量子化の工夫で微分可能にし実現．PointRCNNと組み合わせでSOTA.\\n https://arxiv.org/abs/2004.03080\\xa0 pic.twitter.com/7TfZbZ079s',\n",
       "  'urls': ['https://arxiv.org/abs/2004.03080'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1262216336865976320',\n",
       "  'created_at': 1589772630000,\n",
       "  'date': '2020-05-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1262224011578101760,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1262224011578101760',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:30:30',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SuperGlue: Learning Feature Matching with Graph Neural Networks (CVPR2020 Oral)\\nProject:\\n https://psarlin.com/superglue/\\xa0\\nGitHub: https://github.com/magicleap/SuperGluePretrainedNetwork\\xa0…',\n",
       "  'urls': ['https://psarlin.com/superglue/',\n",
       "   'https://github.com/magicleap/SuperGluePretrainedNetwork'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1262216336865976320',\n",
       "  'created_at': 1589770800000,\n",
       "  'date': '2020-05-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1262216336865976320,\n",
       "  'likes_count': 60,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1262216336865976320',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYN-WuXUcAABCpb.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 10,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像間の局所特徴量をマッチングするGNNを用いたアルゴリズムを提案．視点の大幅な違いにも適用可能．2種類のアテンション機構により画像内，画像間でユニークな特徴量を活用する．GPUでリアルタイム動作し，既存手法と比べ屋内外のシーンで大幅に性能向上． https://arxiv.org/abs/1911.11763\\xa0 pic.twitter.com/4yuITowWG7',\n",
       "  'urls': ['https://arxiv.org/abs/1911.11763'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1261853950791135233',\n",
       "  'created_at': 1589684563000,\n",
       "  'date': '2020-05-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1261854633867177985,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1261854633867177985',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:02:43',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PointRend: Image Segmentation as Rendering (arXiv)\\nCode:  https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend\\xa0…',\n",
       "  'urls': ['https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1261853950791135233',\n",
       "  'created_at': 1589684400000,\n",
       "  'date': '2020-05-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1261853950791135233,\n",
       "  'likes_count': 36,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1261853950791135233',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYL4G6eUEAAfBx6.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'CGレンダリングにヒントを得たセグメンテーションの精緻化モジュールを提案．不確かな点をサンプリングし，MLPで推定し直すことで適応的に物体境界を精緻化．Mask-RCNNやFCNに取り付けることで，少ない計算コストで精度向上．\\n https://arxiv.org/abs/1912.08193\\xa0 pic.twitter.com/KwQ6ASEwyf',\n",
       "  'urls': ['https://arxiv.org/abs/1912.08193'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1261492927231975425',\n",
       "  'created_at': 1589598345000,\n",
       "  'date': '2020-05-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1261493010274988033,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1261493010274988033',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:05:45',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'To Learn or Not to Learn: Visual Localization from Essential Matrices (ICRA2020)\\nPaper:  https://arxiv.org/abs/1908.01293\\xa0\\nCode: https://github.com/GrumpyZhou/visloc-relapose\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/1908.01293',\n",
       "   'https://github.com/GrumpyZhou/visloc-relapose'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1261492927231975425',\n",
       "  'created_at': 1589598325000,\n",
       "  'date': '2020-05-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1261492927231975425,\n",
       "  'likes_count': 50,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1261492927231975425',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYG5FPiVcAECQWL.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 15,\n",
       "  'source': '',\n",
       "  'time': '03:05:25',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像による自己位置推定の精度評価．Data-drivenな手法は，精度面でIndirect法等の従来手法に劣ることが通説となっている．本論文では，特徴量抽出や基礎行列計算等の各フェーズをハンドクラフトからData-drivenまで程度を変え，各組み合わせにおける精度を検証している．\\n https://arxiv.org/abs/1908.01293\\xa0 pic.twitter.com/Rt4IfVGCGK',\n",
       "  'urls': ['https://arxiv.org/abs/1908.01293'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1261129175722799106',\n",
       "  'created_at': 1589512710000,\n",
       "  'date': '2020-05-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1261133832171819009,\n",
       "  'likes_count': 9,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1261133832171819009',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EYByay2U8AEUoY0.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:18:30',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RPM-Net: Robust Point Matching using Learned Features (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.13479\\xa0\\nProject:  https://github.com/yewzijian/RPMNet\\xa0… pic.twitter.com/P627OysyT8',\n",
       "  'urls': ['https://arxiv.org/abs/2003.13479',\n",
       "   'https://github.com/yewzijian/RPMNet'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1261129175722799106',\n",
       "  'created_at': 1589511600000,\n",
       "  'date': '2020-05-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1261129175722799106,\n",
       "  'likes_count': 20,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1261129175722799106',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '微分可能なSinkhornレイヤーを使い、hybrid特徴から点と点のソフトな対応を取り、 誤対応やoverlapが少ない点群ペアでも対処できる学習ベースRobust Point Matching点群位置合わせ手法を提案。ModelNet40での実験結果で(rule-&learning-based)既存手法より優れた性能を示す。 https://youtu.be/7hxGmMk4MZ0\\xa0',\n",
       "  'urls': ['https://youtu.be/7hxGmMk4MZ0'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1260767145731821568',\n",
       "  'created_at': 1589425336000,\n",
       "  'date': '2020-05-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1260767355321192451,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1260767355321192451',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EX8lMwAU4AE4pk9.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:02:16',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'DOOR-SLAM: Distributed, Online, and Outlier Resilient SLAM for Robotic Teams (RA-L)\\npaper:  https://arxiv.org/abs/1909.12198\\xa0\\nyoutube:  https://www.youtube.com/watch?v=h0bqURQlZGA\\xa0… pic.twitter.com/TTS8dMExZq',\n",
       "  'urls': ['https://arxiv.org/abs/1909.12198',\n",
       "   'https://www.youtube.com/watch?v=h0bqURQlZGA'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1260767145731821568',\n",
       "  'created_at': 1589425286000,\n",
       "  'date': '2020-05-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1260767145731821568,\n",
       "  'likes_count': 22,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1260767145731821568',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:01:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '複数ロボット上で走るStereoVOを統合する分散グラフSLAMを提案．ロボット間ループ制約の中から一貫性を保つ最大集合を探す最大クリーク問題を解いて誤検出を除去し，積極的にループ追加を行う方針を採用．大量のループ誤検出を除去し，一貫した地図を生成できることを示した． https://www.youtube.com/watch?v=h0bqURQlZGA\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=h0bqURQlZGA'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1260418715004416009',\n",
       "  'created_at': 1589342251000,\n",
       "  'date': '2020-05-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1260418874496999425,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1260418874496999425',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EX3oQznXYAYcEbG.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:57:31',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Voxgraph: Globally Consistent, Volumetric Mapping using Signed Distance Function Submaps (RA-L 2020)\\nCode:  https://github.com/ethz-asl/voxgraph\\xa0…\\nPaper:  https://ieeexplore.ieee.org/document/8903279\\xa0… pic.twitter.com/kudUjBy2vy',\n",
       "  'urls': ['https://github.com/ethz-asl/voxgraph',\n",
       "   'https://ieeexplore.ieee.org/document/8903279'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1260418715004416009',\n",
       "  'created_at': 1589342213000,\n",
       "  'date': '2020-05-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1260418715004416009,\n",
       "  'likes_count': 25,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1260418715004416009',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:56:53',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Volumetric な地図表現のグラフベース SLAM 手法．SDF サブマップの集合で環境形状を表現．SDF を利用した位置合わせで隣接拘束を生成してポーズグラフ最適化．対応付け不要なので計算コストが低い．ループ閉じ込みは，外部から DBoW などを利用してループ拘束を与える． https://youtu.be/N9p1_Fkxxro\\xa0',\n",
       "  'urls': ['https://youtu.be/N9p1_Fkxxro'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1260043290415489025',\n",
       "  'created_at': 1589252735000,\n",
       "  'date': '2020-05-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1260043416378855424,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1260043416378855424',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:05:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems (ICRA2020)\\nPaper\\n https://arxiv.org/abs/2003.08056\\xa0\\nYoutube\\n https://youtu.be/RFhH4j0gzsI\\xa0\\nRelated work: OmniMVS, ROVO\\n https://arxiv.org/abs/1908.06257\\xa0\\n https://arxiv.org/abs/1902.11154\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2003.08056',\n",
       "   'https://youtu.be/RFhH4j0gzsI',\n",
       "   'https://arxiv.org/abs/1908.06257',\n",
       "   'https://arxiv.org/abs/1902.11154'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1260043290415489025',\n",
       "  'created_at': 1589252705000,\n",
       "  'date': '2020-05-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1260043290415489025,\n",
       "  'likes_count': 44,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1260043290415489025',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EXySm7iUcAASOh8.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 13,\n",
       "  'source': '',\n",
       "  'time': '03:05:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '4つの魚眼カメラを用いてロバストなオドメトリ、全方位のデプス画像生成、密な環境復元を実現．全方位のデプス画像生成にはEnd-to-Endの学習ベースによるOmniMVSを用いて生成．推定した全方位デプス画像をTSDFで統合して密な環境復元を行う．\\n https://youtu.be/RFhH4j0gzsI\\xa0 pic.twitter.com/JfqoF0LKsA',\n",
       "  'urls': ['https://youtu.be/RFhH4j0gzsI'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1259679622674870272',\n",
       "  'created_at': 1589166467000,\n",
       "  'date': '2020-05-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1259681583398715392,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1259681583398715392',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:07:47',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Learning to Explore using Active Neural SLAM (ICLR2020)\\nPaper\\n https://openreview.net/pdf?id=HklXn1BKDH\\xa0…\\nProject\\n https://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html\\xa0…\\nCode\\n https://github.com/devendrachaplot/Neural-SLAM\\xa0…\\nYoutube https://youtu.be/yl9eQkVdZco\\xa0',\n",
       "  'urls': ['https://openreview.net/pdf?id=HklXn1BKDH',\n",
       "   'https://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html',\n",
       "   'https://github.com/devendrachaplot/Neural-SLAM',\n",
       "   'https://youtu.be/yl9eQkVdZco'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1259679622674870272',\n",
       "  'created_at': 1589166000000,\n",
       "  'date': '2020-05-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1259679622674870272,\n",
       "  'likes_count': 63,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1259679622674870272',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 16,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像列から占有格子地図の推定と未知領域探索の行動選択を学習するための強化学習フレームワークを提案．地図と姿勢を推定する\"Neraul SLAM module\"，長期的ゴールを決定する\"Global policy\"，行動を決定する\"Local policy\"の3つの要素で構成．CVPR 2019 Habitat PointGoal Navigation Challenで優勝． pic.twitter.com/onV54zPZDC',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 1},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1259387630472409088',\n",
       "  'created_at': 1589096406000,\n",
       "  'date': '2020-05-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1259387726324830208,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1259387726324830208',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '07:40:06',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Weakly Supervised Semantic Segmentation in 3D Graph-Structured Point Clouds of Wild Scenes',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1259387630472409088',\n",
       "  'created_at': 1589096384000,\n",
       "  'date': '2020-05-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1259387630472409088,\n",
       "  'likes_count': 44,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1259387630472409088',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EXo-QIDUYAALT0S.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '07:39:44',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '3D点群のクラス推定を2Dのセグメンテーションマップのみを教師として学習するモデルの提案．各点のクラスとvisibilityを推論し，これらで合成した2Dセグメンテーションマップと教師との損失をもとに学習する．複数の物体を含む大規模なシーンでも高い性能を達成． https://arxiv.org/abs/2004.12498v1\\xa0… pic.twitter.com/C4Gefp6dFI',\n",
       "  'urls': ['https://arxiv.org/abs/2004.12498v1'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1258592458109321220',\n",
       "  'created_at': 1588906926000,\n",
       "  'date': '2020-05-08',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1258592989351260160,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1258592989351260160',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EXdrmJ-U0AA-NVc.png',\n",
       "   'https://pbs.twimg.com/media/EXdrm5DU8AAoLNG.png',\n",
       "   'https://pbs.twimg.com/media/EXdrnqNUMAAy9fo.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:02:06',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization (ICRA2020) pic.twitter.com/isbHbJFRDF',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1258592458109321220',\n",
       "  'created_at': 1588906800000,\n",
       "  'date': '2020-05-08',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1258592458109321220,\n",
       "  'likes_count': 33,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1258592458109321220',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '単眼SLAMの初期化手法を提案.Homography推定後,Global Plane Optimization (GPO)で最適化しカメラ姿勢と平面の法線を取得.複数フレームの平面情報を組み合わせることで三角測量やHomography分解の計算負荷を減らすことが可能で,精度とリアルタイム性で優れていることを示した.\\n https://arxiv.org/abs/2004.12051\\xa0',\n",
       "  'urls': ['https://arxiv.org/abs/2004.12051'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1258230071900475393',\n",
       "  'created_at': 1588820659000,\n",
       "  'date': '2020-05-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1258231159890165762,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1258231159890165762',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:04:19',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences (submitted to IROS2020)\\nPaper\\n https://arxiv.org/abs/2004.00740\\xa0\\nGithub\\n https://github.com/levenberg/2D-3D-pose-tracking\\xa0…',\n",
       "  'urls': ['https://arxiv.org/abs/2004.00740',\n",
       "   'https://github.com/levenberg/2D-3D-pose-tracking'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1258230071900475393',\n",
       "  'created_at': 1588820400000,\n",
       "  'date': '2020-05-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1258230071900475393,\n",
       "  'likes_count': 38,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1258230071900475393',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 10,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '事前のLiDARマップを用いた単眼カメラ定位手法．マップからofflineで3D線を，AFMでビデオからonlineで2D線を検出．VINS-Monoからのカメラ動き予測により，2D-3D線の対応を取得．その2D-3D対応を用いたポーズ最適化により，ループクローズなしで、VIOのドリフトを低減させた． https://youtu.be/H80Bnxm8IPE\\xa0',\n",
       "  'urls': ['https://youtu.be/H80Bnxm8IPE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1257867682533330947',\n",
       "  'created_at': 1588734141000,\n",
       "  'date': '2020-05-06',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1257868274164887552,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1257868274164887552',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:02:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Learning Feature Descriptors using Camera Pose Supervision\\nProject\\n https://qianqianwang68.github.io/DescfromPose/\\xa0',\n",
       "  'urls': ['https://qianqianwang68.github.io/DescfromPose/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1257867682533330947',\n",
       "  'created_at': 1588734000000,\n",
       "  'date': '2020-05-06',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1257867682533330947,\n",
       "  'likes_count': 45,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1257867682533330947',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EXQuefGU0AAZ1ZD.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 13,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像間の相対姿勢のみを用いた弱教師あり学習による特徴量抽出手法を提案．中間特徴の相関に基づく微分可能なマッチング層やcourse-to-fine構造のネットワークを用い，エピポーラ幾何の拘束を組み込んだ損失関数で学習を行う．教師あり学習による既存手法の精度を上回った．\\n https://arxiv.org/abs/2004.13324\\xa0 pic.twitter.com/CDPfuoZCOm',\n",
       "  'urls': ['https://arxiv.org/abs/2004.13324'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1257506731942273024',\n",
       "  'created_at': 1588647995000,\n",
       "  'date': '2020-05-05',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1257506952956936192,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1257506952956936192',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EXOPyvjVAAA_0Xq.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:06:35',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation (CVPR2020 Oral)\\n https://arxiv.org/abs/2004.01658v1\\xa0… pic.twitter.com/VZFyGePMms',\n",
       "  'urls': ['https://arxiv.org/abs/2004.01658v1'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1257506731942273024',\n",
       "  'created_at': 1588647942000,\n",
       "  'date': '2020-05-05',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1257506731942273024,\n",
       "  'likes_count': 25,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1257506731942273024',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '03:05:42',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '三次元点群のインスタンスセグメンテーション手法の研究．VoxelベースのU-Netで各点のクラスラベルと物体中心へのオフセットを推定．推定された座標について点群をクラスタリングすることで物体候補を生成し，後段のNNでスコアを出力する． https://www.youtube.com/watch?v=HMetye3gmAs\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=HMetye3gmAs'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1257146424707280896',\n",
       "  'created_at': 1588562064000,\n",
       "  'date': '2020-05-04',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1257146529996869634,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1257146529996869634',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:14:24',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Real-Time Global Registration for Globally Consistent RGB-D SLAM (IEEE Transactions on Robotics)\\nPaper:  https://ieeexplore.ieee.org/document/8606275\\xa0…',\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/8606275'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1257146424707280896',\n",
       "  'created_at': 1588562038000,\n",
       "  'date': '2020-05-04',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1257146424707280896,\n",
       "  'likes_count': 26,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1257146424707280896',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EXJH4VjUcAA_hcR.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:13:58',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'リアルタイムでGlobally ConsistentなRGB-D SLAMのため，最適化空間を線形な特徴量部分と非線形な姿勢部分に分解．線形部分を特徴量の二次統計量で表すことで，効率的に計算を行う．処理時間をフレーム数に対してほぼリニアな増加に抑えつつ，SOTA精度を達成．\\nPaper:  https://ieeexplore.ieee.org/document/8606275\\xa0… pic.twitter.com/2VsW10Ca6T',\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/8606275'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256726778132828162',\n",
       "  'created_at': 1588514971000,\n",
       "  'date': '2020-05-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256949008087433216,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256949008087433216',\n",
       "  'mentions': ['amadeussvx'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '99270922', 'username': 'AmadeusSVX'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '14:09:31',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '複数視点の画像を利用しているのは学習時のみで，推定は常に単眼です．（各視点の画像を独立に推定しているのに，動画として整合性が取れているのがポイントです）',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256780519343140864',\n",
       "  'created_at': 1588474800000,\n",
       "  'date': '2020-05-03',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256780519343140864,\n",
       "  'likes_count': 48,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256780519343140864',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EW_xwgBUwAAJz8C.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 11,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '三次元点群のOne-Stage物体検出Hybrid Voxel Network (HVNet)手法を提案。Pointwiseでのmulti-scale特徴をHybrid Voxel Feature Extraction(HVFE)で抽出、Voxelwise attention featureにエンコード、Pseudo-Image Featureへデカップル、リアルタイムの31HzでSOTAを達成。\\n https://arxiv.org/abs/2003.00186\\xa0 pic.twitter.com/0yyB3iaOkU',\n",
       "  'urls': ['https://arxiv.org/abs/2003.00186'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256466307647602693',\n",
       "  'created_at': 1588399887000,\n",
       "  'date': '2020-05-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256466311225290752,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256466311225290752',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EW_chyaUcAIKXvq.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '06:11:27',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/YVJknvUhHF',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256466307647602693',\n",
       "  'created_at': 1588399886000,\n",
       "  'date': '2020-05-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256466309916667904,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256466309916667904',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EW_cb0TVAAAqgk7.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '06:11:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': ' pic.twitter.com/fDFNfyDBvc',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256466307647602693',\n",
       "  'created_at': 1588399886000,\n",
       "  'date': '2020-05-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256466308813602816,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256466308813602816',\n",
       "  'mentions': ['xuanluo14'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '1164375486169923584', 'username': 'XuanLuo14'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '06:11:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Consistent Video Depth Estimation (SIGGRAPH 2020)\\n@XuanLuo14 \\nProject\\n https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\\xa0…\\nPaper\\n https://arxiv.org/abs/2004.15021\\xa0',\n",
       "  'urls': ['https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/',\n",
       "   'https://arxiv.org/abs/2004.15021'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256466307647602693',\n",
       "  'created_at': 1588399886000,\n",
       "  'date': '2020-05-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256466307647602693,\n",
       "  'likes_count': 45,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256466307647602693',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 10,\n",
       "  'source': '',\n",
       "  'time': '06:11:26',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '単眼デプス推定モデルに対して，SfMで離れた画像ペアを選択し，MVSとOptical Flowの結果から奥行と画像座標の距離を損失としてfine-tuneすることで，動画に対し一貫性のある推定を実現．学習の前処理でMVSの結果からスケールを調整．動物体による誤差の影響でSOTAに近い精度． https://youtu.be/5Tia2oblJAg\\xa0',\n",
       "  'urls': ['https://youtu.be/5Tia2oblJAg'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256416173610041344',\n",
       "  'created_at': 1588387967000,\n",
       "  'date': '2020-05-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256416317764071424,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256416317764071424',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '02:52:47',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Footprints and Free Space from a Single Color Image (CVPR2020)\\nCode:  https://github.com/nianticlabs/footprints\\xa0…\\nPaper:  https://arxiv.org/abs/2004.06376\\xa0',\n",
       "  'urls': ['https://github.com/nianticlabs/footprints',\n",
       "   'https://arxiv.org/abs/2004.06376'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256416173610041344',\n",
       "  'created_at': 1588387933000,\n",
       "  'date': '2020-05-02',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256416173610041344,\n",
       "  'likes_count': 62,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256416173610041344',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EW-vxpMU4AAsxxf.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 13,\n",
       "  'source': '',\n",
       "  'time': '02:52:13',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '単眼画像からの自由空間の推定．多視点のステレオ画像から得られた対象環境の幾何情報を単眼画像に集約し学習することで，単一視点からでは不可視な領域に対してもTraversabilityやDepthの評価が可能に．\\n https://arxiv.org/abs/2004.06376\\xa0 pic.twitter.com/7xkhPWGGu8',\n",
       "  'urls': ['https://arxiv.org/abs/2004.06376'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588304672000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256066952667095040,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256066952667095040',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:44:32',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Video: https://youtu.be/wuokg7MFZyU\\xa0',\n",
       "  'urls': ['https://youtu.be/wuokg7MFZyU'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588304655000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256066878394392578,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256066878394392578',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:44:15',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究2（RangeNet++）\\nRangeNet++: Fast and Accurate LiDAR Semantic Segmentation (IROS 2019)\\nCode:  https://github.com/PRBonn/lidar-bonnetal\\xa0…\\nPaper:  https://ieeexplore.ieee.org/document/8967762\\xa0…',\n",
       "  'urls': ['https://github.com/PRBonn/lidar-bonnetal',\n",
       "   'https://ieeexplore.ieee.org/document/8967762'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588304641000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256066819732828160,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256066819732828160',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:44:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Video: https://youtu.be/-AEX203rXkE\\xa0',\n",
       "  'urls': ['https://youtu.be/-AEX203rXkE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588304623000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256066745376239617,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256066745376239617',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:43:43',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '関連研究1（SuMa）\\nEfficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments (RSS 2018)\\nProject:  http://jbehley.github.io/projects/surfel_mapping/\\xa0…\\nCode:  https://github.com/jbehley/SuMa\\xa0\\nPaper:  http://www.roboticsproceedings.org/rss14/p16.html\\xa0',\n",
       "  'urls': ['http://jbehley.github.io/projects/surfel_mapping/',\n",
       "   'https://github.com/jbehley/SuMa',\n",
       "   'http://www.roboticsproceedings.org/rss14/p16.html'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588304142000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256064728041787393,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256064728041787393',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:35:42',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Video: https://youtu.be/uo3ZuLuFAzk\\xa0',\n",
       "  'urls': ['https://youtu.be/uo3ZuLuFAzk'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588304132000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256064686878896130,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256064686878896130',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:35:32',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SuMa++: Efficient LiDAR-based Semantic SLAM (IROS 2019)\\nCode:  https://github.com/PRBonn/semantic_suma\\xa0…\\nPaper:  https://ieeexplore.ieee.org/document/8967704\\xa0…',\n",
       "  'urls': ['https://github.com/PRBonn/semantic_suma',\n",
       "   'https://ieeexplore.ieee.org/document/8967704'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1256059179724271616',\n",
       "  'created_at': 1588302819000,\n",
       "  'date': '2020-05-01',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1256059179724271616,\n",
       "  'likes_count': 30,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1256059179724271616',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EW5q4e_VcAATkwL.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '03:13:39',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '地図を surfel で表現するグラフベース SLAM 手法の SuMa を拡張。3D LIDAR 点群を距離画像に変換し、FCN でセマンティックセグメンテーション。セマンティクスの整合性を重みとする。静止している車は位置合わせに利用される。移動している車が多い KITTI dataset の高速道路でも高精度な推定を実現。 pic.twitter.com/M286GNv7WB',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1255704202178752513',\n",
       "  'created_at': 1588218234000,\n",
       "  'date': '2020-04-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1255704402347753472,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1255704402347753472',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:43:54',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1255704202178752513',\n",
       "  'created_at': 1588218186000,\n",
       "  'date': '2020-04-30',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1255704202178752513,\n",
       "  'likes_count': 50,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1255704202178752513',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EW0oDztVAAIjhWL.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 14,\n",
       "  'source': '',\n",
       "  'time': '03:43:06',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Kinect Fusionに利用されているTSDFを学習器に置き換えたDeepSDFを局所適用し，詳細な形状表現を可能にした．DeepSDFは全体を関数近似するのに対し，提案手法はVoxel単位で関数近似．DeepSDFが8日かかった形状復元が，提案手法では1分と大幅に短縮．\\n https://arxiv.org/pdf/2003.10983.pdf\\xa0… pic.twitter.com/Y89OJ9FDXh',\n",
       "  'urls': ['https://arxiv.org/pdf/2003.10983.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1255331095656239105',\n",
       "  'created_at': 1588136416000,\n",
       "  'date': '2020-04-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1255361235232620544,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1255361235232620544',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '05:00:16',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '第一著者自身の引用論文\\n\\nHigh-dimensional Convolutional Networks for Geometric Pattern Recognition (CVPR2020)\\nPaper:\\n http://vladlen.info/papers/HDConvNets.pdf\\xa0… …\\n\\nFully Convolutional Geometric Features (ICCV2019)\\nPaper:\\n https://node1.chrischoy.org/data/publications/fcgf/fcgf.pdf\\xa0… …\\nGithub: https://github.com/chrischoy/FCGF\\xa0',\n",
       "  'urls': ['http://vladlen.info/papers/HDConvNets.pdf',\n",
       "   'https://node1.chrischoy.org/data/publications/fcgf/fcgf.pdf',\n",
       "   'https://github.com/chrischoy/FCGF'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1255331095656239105',\n",
       "  'created_at': 1588132682000,\n",
       "  'date': '2020-04-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1255345574418755585,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1255345574418755585',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:58:02',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Deep Global Registration (CVPR2020 Oral)\\nProject https://chrischoy.github.io/publication/dgr/\\xa0…',\n",
       "  'urls': ['https://chrischoy.github.io/publication/dgr/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1255331095656239105',\n",
       "  'created_at': 1588129230000,\n",
       "  'date': '2020-04-29',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1255331095656239105,\n",
       "  'likes_count': 144,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1255331095656239105',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 42,\n",
       "  'source': '',\n",
       "  'time': '03:00:30',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '点群位置合わせの微分可能なフレームワークを提案．6D ConvNetで推定した対応点のInlier確率を重みとし，その重みで微分した勾配をガイドとするProcrustes法により密な対応点を利用した高精度な位置合せが可能．finetuneを追加しEnd-to-endで学習．精度，頑健性，速度でSOTA．\\n https://arxiv.org/abs/2004.11540\\xa0 pic.twitter.com/33W0tiDzCe',\n",
       "  'urls': ['https://arxiv.org/abs/2004.11540'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 1},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254968580136820736',\n",
       "  'created_at': 1588043554000,\n",
       "  'date': '2020-04-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1254971744839852033,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254971744839852033',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '03:12:34',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SynSin: End-to-end View Synthesis from a Single Image (CVPR2020 Oral)\\nProject\\n http://www.robots.ox.ac.uk/~ow/synsin.html\\xa0\\nGithub https://github.com/facebookresearch/synsin\\xa0…',\n",
       "  'urls': ['http://www.robots.ox.ac.uk/~ow/synsin.html',\n",
       "   'https://github.com/facebookresearch/synsin'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254968580136820736',\n",
       "  'created_at': 1588042800000,\n",
       "  'date': '2020-04-28',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1254968580136820736,\n",
       "  'likes_count': 19,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254968580136820736',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWm4H8TUMAQ2gk5.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '任意解像度の単一画像から任意のビューを合成するend-to-endなネットワークの提案．推論したFeatureとDepthを用い点群を構築，微分可能な点群レンダラーとリファインメントネットワークを通すことで欠損のないビューを合成．\\n https://arxiv.org/abs/1912.08804\\xa0 pic.twitter.com/cp2TcgoURr',\n",
       "  'urls': ['https://arxiv.org/abs/1912.08804'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254608188801875968',\n",
       "  'created_at': 1587956952000,\n",
       "  'date': '2020-04-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1254608507430526976,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254608507430526976',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:09:12',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Online LiDAR-SLAM for Legged Robots with Deep-Learned Loop Closure (ICRA2020)\\nProject\\n https://ori.ox.ac.uk/lidar-slam/\\xa0\\nPaper\\n https://arxiv.org/abs/2001.10249\\xa0',\n",
       "  'urls': ['https://ori.ox.ac.uk/lidar-slam/',\n",
       "   'https://arxiv.org/abs/2001.10249'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254608188801875968',\n",
       "  'created_at': 1587956876000,\n",
       "  'date': '2020-04-27',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1254608188801875968,\n",
       "  'likes_count': 36,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254608188801875968',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 7,\n",
       "  'source': '',\n",
       "  'time': '03:07:56',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '深層学習を用いた特徴量ベースのループ検出器を組み込んだグラフLiDAR-SLAMシステムを提案．四脚ロボットでも動作するように浅いネットワークを用いておりCPUで推論可能．kd-treeを用いた点群繋ぎ合わせの高速な検証方法を提案．屋内外の産業環境でロバスト性を実証． https://youtu.be/djf7vGtf7CA\\xa0',\n",
       "  'urls': ['https://youtu.be/djf7vGtf7CA'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254607258375213056',\n",
       "  'created_at': 1587956654000,\n",
       "  'date': '2020-04-27',\n",
       "  'geo': '',\n",
       "  'hashtags': ['#slamhub'],\n",
       "  'id': 1254607258375213056,\n",
       "  'likes_count': 19,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254607258375213056',\n",
       "  'mentions': ['ystk_hara'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '1113412223001452545', 'username': 'ystk_hara'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '03:04:14',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SLAM-Hubに千葉工大 fuRoの原先生 @ystk_hara が加入されました．千葉周辺にお住まいのSLAM-Hubにご興味のある方は原先生へ気軽にご相談ください．#slamhub',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254246604090961920',\n",
       "  'created_at': 1587870693000,\n",
       "  'date': '2020-04-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1254246709997170694,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254246709997170694',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:11:33',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'YOLOv4: Optimal Speed and Accuracy of Object Detection\\nGithub https://github.com/AlexeyAB/darknet\\xa0…',\n",
       "  'urls': ['https://github.com/AlexeyAB/darknet'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1254246604090961920',\n",
       "  'created_at': 1587870667000,\n",
       "  'date': '2020-04-26',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1254246604090961920,\n",
       "  'likes_count': 93,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1254246604090961920',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWf6iLgVAAAldSl.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 28,\n",
       "  'source': '',\n",
       "  'time': '03:11:07',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'CNNによる高速な物体検出器YOLOの最新版YOLOv4を提案．検出器の学習における，Bag of freebiesやBag of specialsによる効果を検証．バッチ正規化や残差スキップ接続など，モデルやデータセットに関して普遍的で効果のよい手法を用いることで精度を向上させた．\\n https://arxiv.org/abs/2004.10934\\xa0 pic.twitter.com/G3cdiZTGMZ',\n",
       "  'urls': ['https://arxiv.org/abs/2004.10934'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1253881416191897600',\n",
       "  'created_at': 1587795500000,\n",
       "  'date': '2020-04-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1253931329139249153,\n",
       "  'likes_count': 8,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1253931329139249153',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '06:18:20',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Least Squares Optimization: from Theory to Practice\\nGithub https://github.com/srrg-sapienza/srrg2_solver\\xa0…',\n",
       "  'urls': ['https://github.com/srrg-sapienza/srrg2_solver'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1253881416191897600',\n",
       "  'created_at': 1587783600000,\n",
       "  'date': '2020-04-25',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1253881416191897600,\n",
       "  'likes_count': 53,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1253881416191897600',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWagb_EUYAEJgO6.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 16,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '反復法による最小自乗を解く新たな最適化システムを提案．既存の問題を統一的に解けるようにソルバを設計することで，疎/密，動的/静的な要素にシームレスに対応した．様々な観点で比較評価を行い，提案手法が既存システムに対し同等以上の速度，精度性能を達成した．\\n https://arxiv.org/abs/2002.11051\\xa0 pic.twitter.com/FUqWsYd55A',\n",
       "  'urls': ['https://arxiv.org/abs/2002.11051'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1253519029291094020',\n",
       "  'created_at': 1587697200000,\n",
       "  'date': '2020-04-24',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1253519029291094020,\n",
       "  'likes_count': 19,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1253519029291094020',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWRrybDUcAAkDj3.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'グラフ描画のアルゴリズムを用いて3D点群を2D画像に投影する手法を提案。投影した画像にU-Netを適用し、3D点群のセグメンテーションでSOTAを達成。階層的クラスタリングで得られた部分点群ごとに投影することで計算コストを削減。\\n http://arxiv.org/abs/2003.05593\\xa0 pic.twitter.com/4Mt0buWYu9',\n",
       "  'urls': ['http://arxiv.org/abs/2003.05593'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1253182486252670978',\n",
       "  'created_at': 1587617000000,\n",
       "  'date': '2020-04-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1253182647519461378,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1253182647519461378',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '04:43:20',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Unsupervised Geometry-Aware Deep LiDAR Odometry (ICRA2020)\\nproject: https://sites.google.com/view/deeplo\\xa0',\n",
       "  'urls': ['https://sites.google.com/view/deeplo'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1253182486252670978',\n",
       "  'created_at': 1587616962000,\n",
       "  'date': '2020-04-23',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1253182486252670978,\n",
       "  'likes_count': 19,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1253182486252670978',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWQy0LjUwAAj205.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '04:42:42',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '2つのLIDARスキャン間の相対姿勢推定をUnsupervisedに学習．全周画像上で特徴量抽出や最終的なICP誤差のロスを定義することで対応付けを避けて学習に適した枠組みとしている．\\nvideo: https://www.youtube.com/watch?v=-imRJXq6ZuE\\xa0… pic.twitter.com/WV2OHJKXzA',\n",
       "  'urls': ['https://www.youtube.com/watch?v=-imRJXq6ZuE'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1252794252284891136',\n",
       "  'created_at': 1587526188000,\n",
       "  'date': '2020-04-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1252801752589496320,\n",
       "  'likes_count': 4,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1252801752589496320',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWLYYBEUYAY3X0C.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:29:48',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans (CVPR2020)\\nPaper: \\n https://arxiv.org/abs/1912.00036\\xa0\\nProject:  https://www.3dunderstanding.org/papers/2020/dai2020sgnn/\\xa0… pic.twitter.com/uPOIFDJRom',\n",
       "  'urls': ['https://arxiv.org/abs/1912.00036',\n",
       "   'https://www.3dunderstanding.org/papers/2020/dai2020sgnn/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1252794252284891136',\n",
       "  'created_at': 1587524400000,\n",
       "  'date': '2020-04-22',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1252794252284891136,\n",
       "  'likes_count': 22,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1252794252284891136',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ターゲット点群の一部スキャンを削除、更に不完全な点群を入力として、自己教師ありで点群補完を学習させるSG-NNを提案。点群はスパースTSDFで表現、encoder-decoderを使って、見たこともない三次元形状にデコードでき、ターゲット点群より高い分解能点群を補完することが可能 https://youtu.be/rN6D3QmMNuU\\xa0',\n",
       "  'urls': ['https://youtu.be/rN6D3QmMNuU'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1252430913985642496',\n",
       "  'created_at': 1587437793000,\n",
       "  'date': '2020-04-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1252430995665518600,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1252430995665518600',\n",
       "  'mentions': ['ohtake_i', 'kanejaki'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '370013804', 'username': 'ohtake_i'},\n",
       "   {'user_id': '107693067', 'username': 'kanejaki'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '02:56:33',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Efficient Exploration in Constrained Environments with Goal-Oriented Reference Path (arXiv)\\n@ohtake_i @kanejaki\\nProject: https://keiohta.github.io/publications/2020-03-01_gai_navigation\\xa0…',\n",
       "  'urls': ['https://keiohta.github.io/publications/2020-03-01_gai_navigation'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1252430913985642496',\n",
       "  'created_at': 1587437773000,\n",
       "  'date': '2020-04-21',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1252430913985642496,\n",
       "  'likes_count': 80,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1252430913985642496',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWGHIEwUYAI5H7e.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 17,\n",
       "  'source': '',\n",
       "  'time': '02:56:13',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '2D地図を入力とした目的地への経路・行動生成手法の提案．GOSELOと呼ばれる地図表現を介しCNNによるWaypointを生成(教師あり学習)，さらにWaypointに沿うような操作量を生成する層(強化学習)を後段に追加することで安全なナビゲーションを実現している．\\n https://arxiv.org/abs/2003.01641\\xa0 pic.twitter.com/yskq5iMDHZ',\n",
       "  'urls': ['https://arxiv.org/abs/2003.01641'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1252071153817903108',\n",
       "  'created_at': 1587352016000,\n",
       "  'date': '2020-04-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1252071221438451713,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1252071221438451713',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:06:56',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points (NeurIPS2019)',\n",
       "  'urls': [],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1252071153817903108',\n",
       "  'created_at': 1587352000000,\n",
       "  'date': '2020-04-20',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1252071153817903108,\n",
       "  'likes_count': 46,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1252071153817903108',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EWA_xsTU0AAck4E.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 8,\n",
       "  'source': '',\n",
       "  'time': '03:06:40',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RGB画像1枚から物体の3D Bounding Box(3DBB)と6DoF姿勢推定．3DBB投影時のコーナー点位置をテンプレートの重み付け和で表現し，その重みを推定する枠組み．同時に3DBBの3次元位置姿勢を推定し投影点上でのLossを定義し学習する．\\n https://arxiv.org/abs/1912.07744\\xa0 pic.twitter.com/lVKH8vErzB',\n",
       "  'urls': ['https://arxiv.org/abs/1912.07744'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1251707090277548033',\n",
       "  'created_at': 1587266384000,\n",
       "  'date': '2020-04-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1251712055456829440,\n",
       "  'likes_count': 5,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1251712055456829440',\n",
       "  'mentions': ['sucaredgar', 'wkentaro_', 'ajddavison'],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'},\n",
       "   {'user_id': '858493718952804352', 'username': 'SucarEdgar'},\n",
       "   {'user_id': '782291244', 'username': 'wkentaro_'},\n",
       "   {'user_id': '1446792746', 'username': 'AjdDavison'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '03:19:44',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction\\n@SucarEdgar @wkentaro_ @AjdDavison\\nProject\\n https://edgarsucar.github.io/NodeSLAM/\\xa0\\nPaper\\n https://arxiv.org/abs/2004.04485\\xa0',\n",
       "  'urls': ['https://edgarsucar.github.io/NodeSLAM/',\n",
       "   'https://arxiv.org/abs/2004.04485'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1251707090277548033',\n",
       "  'created_at': 1587265200000,\n",
       "  'date': '2020-04-19',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1251707090277548033,\n",
       "  'likes_count': 39,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1251707090277548033',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 14,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'VAEを用いて学習した各物体カテゴリの3D形状特徴量を，トラッキングしながらカメラ姿勢と同時に最適化することで，単一あるいは複数視点のRGB-D画像から，オクルージョンがあっても欠損がない物体の高精度な3D形状を復元する手法を提案． https://www.youtube.com/watch?time_continue=279&v=zPzMtXU-0JE&feature=emb_logo\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?time_continue=279&v=zPzMtXU-0JE&feature=emb_logo'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1251344702227353600',\n",
       "  'created_at': 1587193769000,\n",
       "  'date': '2020-04-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1251407484897554432,\n",
       "  'likes_count': 2,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1251407484897554432',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EV3kby_UEAAn3ca.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '07:09:29',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Redesigning SLAM for Arbitrary Multi-Camera Systems (ICRA2020)\\n https://arxiv.org/abs/2003.02014\\xa0 pic.twitter.com/eMBMAKlj8e',\n",
       "  'urls': ['https://arxiv.org/abs/2003.02014'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1251344702227353600',\n",
       "  'created_at': 1587178800000,\n",
       "  'date': '2020-04-18',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1251344702227353600,\n",
       "  'likes_count': 49,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1251344702227353600',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 13,\n",
       "  'source': '',\n",
       "  'time': '03:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '既存のVisual SLAM手法を任意の複数カメラシステムに拡張する手法を提案．適応的な初期化，センサに依存しないキーフレーム選択，voxelベースのマップ管理法を用いることで，精度を保ちセンサ固有の改良なしでの動作を実現. https://www.youtube.com/watch?v=JGL4H93BiNw\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=JGL4H93BiNw'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1250976017079922688',\n",
       "  'created_at': 1587090899000,\n",
       "  'date': '2020-04-17',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1250976017079922688,\n",
       "  'likes_count': 48,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1250976017079922688',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVxb1NiU4AEJwkl.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 23,\n",
       "  'source': '',\n",
       "  'time': '02:34:59',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ワイドベースラインカメラで撮影した6枚の画像から高品質な幾何形状とSVBRDFを復元する学習ベースの手法を提案．各画像ごとに拡散・鏡面アルベド，法線，鏡面粗さをネットワークで推定し，推定結果を融合して幾何形状とSVBRDF得る．従来難しかった疎な画像からの復元に成功．\\n https://arxiv.org/abs/2003.12642\\xa0 pic.twitter.com/Qn5OoQkR0s',\n",
       "  'urls': ['https://arxiv.org/abs/2003.12642'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1250589729210449920',\n",
       "  'created_at': 1587003425000,\n",
       "  'date': '2020-04-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1250609125467189249,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1250609125467189249',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVsOUMIU0AE-bYa.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '02:17:05',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings (CVPR2020)\\n https://arxiv.org/abs/2003.12980\\xa0 pic.twitter.com/TD0uMjecFV',\n",
       "  'urls': ['https://arxiv.org/abs/2003.12980'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1250589729210449920',\n",
       "  'created_at': 1586998800000,\n",
       "  'date': '2020-04-16',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1250589729210449920,\n",
       "  'likes_count': 17,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1250589729210449920',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '01:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'ステレオカメラを用いて，物体上の特徴点のクラスター化と，自身と物体の動きの推定を同時に行うシステムを提案．クラスター化は，物体検出による特徴点のクラスラベルを用いて，3次元位置も考慮したCRFにより実装．シーンに依存せず，オンラインでの動作を可能にした． https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1250227339776217088',\n",
       "  'created_at': 1586916964000,\n",
       "  'date': '2020-04-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1250246481644122112,\n",
       "  'likes_count': 6,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1250246481644122112',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVnEelhU0AA3cwA.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '02:16:04',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'DeepFactors: Real-Time Probabilistic Dense Monocular SLAM (RA-L)\\n https://arxiv.org/abs/2001.05049\\xa0 pic.twitter.com/raqAYPgSje',\n",
       "  'urls': ['https://arxiv.org/abs/2001.05049'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1250227339776217088',\n",
       "  'created_at': 1586912400000,\n",
       "  'date': '2020-04-15',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1250227339776217088,\n",
       "  'likes_count': 21,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1250227339776217088',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '01:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'CodeSLAMをベースにした新たな深層学習ベースのVisual SLAMシステムを提案．既存のコードによるコンパクトなデプスマップ表現に加え，損失関数の改善，ループクロージングと全体最適化の追加により，精度とロバストを向上．さらにリアルタイム動作を実現した． https://www.youtube.com/watch?v=htnRuGKZmZw\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=htnRuGKZmZw'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1249864951092506624',\n",
       "  'created_at': 1586829418000,\n",
       "  'date': '2020-04-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1249879286372118528,\n",
       "  'likes_count': 0,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1249879286372118528',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '01:56:58',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Github: https://github.com/QingyongHu/RandLA-Net\\xa0…',\n",
       "  'urls': ['https://github.com/QingyongHu/RandLA-Net'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1249864951092506624',\n",
       "  'created_at': 1586829378000,\n",
       "  'date': '2020-04-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1249879119694680064,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1249879119694680064',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '01:56:18',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds (CVPR2020)\\nVideo: https://www.youtube.com/watch?v=Ar3eY_lwzMk&feature=youtu.be\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=Ar3eY_lwzMk&feature=youtu.be'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1249864951092506624',\n",
       "  'created_at': 1586826000000,\n",
       "  'date': '2020-04-14',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1249864951092506624,\n",
       "  'likes_count': 24,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1249864951092506624',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVfbCzoU4AEISU9.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 9,\n",
       "  'source': '',\n",
       "  'time': '01:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '点数が百万を超える大規模三次元点群のセグメンテーションにおいては、従来の高コストな点群サンプリング手法よりもランダムサンプリングが有効であることを示した．KNNとアテンションを用いて積極的に受容野を拡大することで、サンプリングによる点群の欠損に対処．\\n https://arxiv.org/abs/1911.11236\\xa0 pic.twitter.com/xjpFO7WABl',\n",
       "  'urls': ['https://arxiv.org/abs/1911.11236'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1249517662847283201',\n",
       "  'created_at': 1586747161000,\n",
       "  'date': '2020-04-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1249534277026373636,\n",
       "  'likes_count': 1,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1249534277026373636',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 0,\n",
       "  'source': '',\n",
       "  'time': '03:06:01',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'R-LINS: A Robocentric Lidar-Inertial State Estimator for Robust and Efficient Navigation (ICRA2020)\\nvideo: https://www.youtube.com/watch?v=Nmr1blC09qw\\xa0…',\n",
       "  'urls': ['https://www.youtube.com/watch?v=Nmr1blC09qw'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1249517662847283201',\n",
       "  'created_at': 1586743200000,\n",
       "  'date': '2020-04-13',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1249517662847283201,\n",
       "  'likes_count': 38,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1249517662847283201',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVaW8PaUMAAm7RO.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 6,\n",
       "  'source': '',\n",
       "  'time': '02:00:00',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'LIDAR-IMUのオドメトリ推定手法．Error State Kalman Filter上でTight-couplingに最適化を行うことで，従来のグラフベース手法と近い精度を維持しつつ大幅に処理速度を向上させた．\\npaper: https://arxiv.org/abs/1907.02233\\xa0 pic.twitter.com/IQ4YAzhL8q',\n",
       "  'urls': ['https://arxiv.org/abs/1907.02233'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1249155400122994688',\n",
       "  'created_at': 1586656830000,\n",
       "  'date': '2020-04-12',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1249155400122994688,\n",
       "  'likes_count': 8,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1249155400122994688',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 2,\n",
       "  'source': '',\n",
       "  'time': '02:00:30',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Object Levelより細かく，新たな形状表現Local Implicit Grid (LIG)を提案．AutoencoderでPartの形状をlatent vectorにエンコード，入力点群と復元ロス最小なlatent vectorを最適化，LIGでの内挿により形状にデコード、機械学習で点群からScene Levelの三次元形状復元が可能． https://youtu.be/XCyl1-vxfII\\xa0',\n",
       "  'urls': ['https://youtu.be/XCyl1-vxfII'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1248796223961624576',\n",
       "  'created_at': 1586571264000,\n",
       "  'date': '2020-04-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1248796509975359488,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1248796509975359488',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '02:14:24',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'EPOS: Estimating 6D Pose of Objects with Symmetries (CVPR2020)\\nProject:  http://cmp.felk.cvut.cz/epos/\\xa0',\n",
       "  'urls': ['http://cmp.felk.cvut.cz/epos/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1248796223961624576',\n",
       "  'created_at': 1586571195000,\n",
       "  'date': '2020-04-11',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1248796223961624576,\n",
       "  'likes_count': 23,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1248796223961624576',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVSdYxHU0AIFkpg.jpg'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 5,\n",
       "  'source': '',\n",
       "  'time': '02:13:15',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'RGB画像における物体の6DoF姿勢推定．Surface fragmentによる3次元モデル表現を介し，物体のPoseを各PixelがどのようなインスタンスやSurface fragmentに対応しうるかを学習．得られた多対多な2D-3D対応をPnP-RANSACによりロバスト化．\\n https://arxiv.org/pdf/2004.00605.pdf\\xa0… pic.twitter.com/Qn2Etv1Nst',\n",
       "  'urls': ['https://arxiv.org/pdf/2004.00605.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1248436573865041920',\n",
       "  'created_at': 1586492481000,\n",
       "  'date': '2020-04-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1248466071301935110,\n",
       "  'likes_count': 3,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1248466071301935110',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '04:21:21',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching [ICRA2020]\\n\\nVideo:  https://www.ais.uni-bonn.de/videos/ICRA_2020_Gradient_Dissimilarity/\\xa0…',\n",
       "  'urls': ['https://www.ais.uni-bonn.de/videos/ICRA_2020_Gradient_Dissimilarity/'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1248436573865041920',\n",
       "  'created_at': 1586485448000,\n",
       "  'date': '2020-04-10',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1248436573865041920,\n",
       "  'likes_count': 20,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1248436573865041920',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVNWXyhUUAAYvJo.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 4,\n",
       "  'source': '',\n",
       "  'time': '02:24:08',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': '画像の位置合わせに必要なエラーについて，輝度差に代わる新たなメトリックSGFを提案．SGFは勾配画像のコントラストを局所的に正規化し，勾配方向の内積を利用．DSOにSGFを適用したところ精度が改善．\\n https://arxiv.org/pdf/2004.04090.pdf\\xa0… pic.twitter.com/HL3hkOLo61',\n",
       "  'urls': ['https://arxiv.org/pdf/2004.04090.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1248070308080177152',\n",
       "  'created_at': 1586429890000,\n",
       "  'date': '2020-04-09',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1248203546060681217,\n",
       "  'likes_count': 7,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1248203546060681217',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 1,\n",
       "  'source': '',\n",
       "  'time': '10:58:10',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Spatial Hashing (メモリ効率の良いボクセル表現)に関する引用文献\\nReal-time 3D Reconstruction at Scale using Voxel Hashing\\n https://niessnerlab.org/papers/2013/4hashing/niessner2013hashing.pdf\\xa0…\\nOptimized Spatial Hashing for Collision Detection of Deformable Objects\\n https://matthias-research.github.io/pages/publications/tetraederCollision.pdf\\xa0…',\n",
       "  'urls': ['https://niessnerlab.org/papers/2013/4hashing/niessner2013hashing.pdf',\n",
       "   'https://matthias-research.github.io/pages/publications/tetraederCollision.pdf'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1248070308080177152',\n",
       "  'created_at': 1586398124000,\n",
       "  'date': '2020-04-09',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1248070308080177152,\n",
       "  'likes_count': 30,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1248070308080177152',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': ['https://pbs.twimg.com/media/EVII8izVAAE-WVi.png'],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 1,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 3,\n",
       "  'source': '',\n",
       "  'time': '02:08:44',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'Visual SLAMで用いられる従来のキーフレーム表現（covisibility graph）では，3D点の遮蔽関係を記述できないため，ボクセルハッシングと視錐台表現を用いたレイキャスティングにより，高速かつ省メモリに幾何的関係性の記述を可能とした．\\n https://arxiv.org/abs/2003.02247\\xa0 pic.twitter.com/MMeABTUKVP',\n",
       "  'urls': ['https://arxiv.org/abs/2003.02247'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0},\n",
       " {'cashtags': [],\n",
       "  'conversation_id': '1247356052657455105',\n",
       "  'created_at': 1586227832000,\n",
       "  'date': '2020-04-07',\n",
       "  'geo': '',\n",
       "  'hashtags': [],\n",
       "  'id': 1247356052657455105,\n",
       "  'likes_count': 185,\n",
       "  'link': 'https://twitter.com/slam_hub/status/1247356052657455105',\n",
       "  'mentions': [],\n",
       "  'name': 'SLAM-Hub',\n",
       "  'near': '',\n",
       "  'photos': [],\n",
       "  'place': '',\n",
       "  'quote_url': '',\n",
       "  'replies_count': 0,\n",
       "  'reply_to': [{'user_id': '1244129482132209664', 'username': 'slam_hub'}],\n",
       "  'retweet': False,\n",
       "  'retweet_date': '',\n",
       "  'retweet_id': '',\n",
       "  'retweets_count': 66,\n",
       "  'source': '',\n",
       "  'time': '02:50:32',\n",
       "  'timezone': 'UTC',\n",
       "  'trans_dest': '',\n",
       "  'trans_src': '',\n",
       "  'translate': '',\n",
       "  'tweet': 'R&Dコミュニティ「SLAM-Hub」を立ち上げました！\\n一緒に研究開発してくれるメンバーを募集しています． https://slamhub.xslam.org/?0\\xa0',\n",
       "  'urls': ['https://slamhub.xslam.org/?0'],\n",
       "  'user_id': 1244129482132209664,\n",
       "  'user_rt': '',\n",
       "  'user_rt_id': '',\n",
       "  'username': 'slam_hub',\n",
       "  'video': 0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1247356052657455105': 1,\n",
       "         '1248070308080177152': 2,\n",
       "         '1248436573865041920': 2,\n",
       "         '1248796223961624576': 2,\n",
       "         '1249155400122994688': 1,\n",
       "         '1249517662847283201': 2,\n",
       "         '1249864951092506624': 3,\n",
       "         '1250227339776217088': 2,\n",
       "         '1250589729210449920': 2,\n",
       "         '1250976017079922688': 1,\n",
       "         '1251344702227353600': 2,\n",
       "         '1251707090277548033': 2,\n",
       "         '1252071153817903108': 2,\n",
       "         '1252430913985642496': 2,\n",
       "         '1252794252284891136': 2,\n",
       "         '1253182486252670978': 2,\n",
       "         '1253519029291094020': 1,\n",
       "         '1253881416191897600': 2,\n",
       "         '1254246604090961920': 2,\n",
       "         '1254607258375213056': 1,\n",
       "         '1254608188801875968': 2,\n",
       "         '1254968580136820736': 2,\n",
       "         '1255331095656239105': 3,\n",
       "         '1255704202178752513': 2,\n",
       "         '1256059179724271616': 7,\n",
       "         '1256416173610041344': 2,\n",
       "         '1256466307647602693': 4,\n",
       "         '1256726778132828162': 1,\n",
       "         '1256780519343140864': 1,\n",
       "         '1257146424707280896': 2,\n",
       "         '1257506731942273024': 2,\n",
       "         '1257867682533330947': 2,\n",
       "         '1258230071900475393': 2,\n",
       "         '1258592458109321220': 2,\n",
       "         '1259387630472409088': 2,\n",
       "         '1259679622674870272': 2,\n",
       "         '1260043290415489025': 2,\n",
       "         '1260418715004416009': 2,\n",
       "         '1260767145731821568': 2,\n",
       "         '1261129175722799106': 2,\n",
       "         '1261492927231975425': 2,\n",
       "         '1261853950791135233': 2,\n",
       "         '1262216336865976320': 2,\n",
       "         '1262595837172973569': 2,\n",
       "         '1262941112664117248': 2,\n",
       "         '1263303500836069376': 2,\n",
       "         '1263670128400404480': 2,\n",
       "         '1264027851038154753': 2,\n",
       "         '1264390814454607872': 2,\n",
       "         '1264753052122308615': 2,\n",
       "         '1265120071728283648': 2,\n",
       "         '1265477828973416448': 2,\n",
       "         '1265849825678913537': 3,\n",
       "         '1266202604868075520': 2,\n",
       "         '1266564992805076997': 4,\n",
       "         '1266927379089248256': 2,\n",
       "         '1267289768544460800': 2,\n",
       "         '1267727467101315072': 2,\n",
       "         '1268013854698557440': 3,\n",
       "         '1268387507969785856': 2,\n",
       "         '1268739321369989121': 2,\n",
       "         '1269104347004022785': 2,\n",
       "         '1269519361787613184': 2,\n",
       "         '1269826483599618049': 2,\n",
       "         '1270188872232660993': 2,\n",
       "         '1270551260056387585': 4,\n",
       "         '1270913650086178817': 2,\n",
       "         '1271276037117218816': 2,\n",
       "         '1271651964275683329': 2,\n",
       "         '1272001420745535489': 3,\n",
       "         '1272363203498930176': 2,\n",
       "         '1272725587359076352': 3,\n",
       "         '1273118847114018816': 2,\n",
       "         '1273450361563435008': 2,\n",
       "         '1273816472230363138': 2,\n",
       "         '1274205587597737985': 2,\n",
       "         '1274537524241797121': 2,\n",
       "         '1274899915655843840': 3,\n",
       "         '1275262302594293762': 2,\n",
       "         '1275630109311950849': 2,\n",
       "         '1276006512670830593': 3,\n",
       "         '1276361064104341506': 2,\n",
       "         '1276742451806404608': 2,\n",
       "         '1277074304786546689': 2,\n",
       "         '1277436627485290496': 2,\n",
       "         '1277799017053859842': 2,\n",
       "         '1278188128444940288': 2,\n",
       "         '1278242853508898816': 5,\n",
       "         '1278243781104463872': 5,\n",
       "         '1278523792042496000': 2,\n",
       "         '1278886178670796801': 2,\n",
       "         '1278904428246888448': 1,\n",
       "         '1278904690546098176': 1,\n",
       "         '1279258907534213121': 2,\n",
       "         '1279610956172017666': 2,\n",
       "         '1279973343249281025': 2,\n",
       "         '1280345060442288129': 2,\n",
       "         '1280724052026392576': 2,\n",
       "         '1281074772625874946': 2,\n",
       "         '1281422893826494464': 2,\n",
       "         '1281794468920713217': 2,\n",
       "         '1282203799256350721': 2,\n",
       "         '1282510058660737024': 2,\n",
       "         '1282872986467397633': 2,\n",
       "         '1283234833829945345': 3,\n",
       "         '1283601093612548099': 2,\n",
       "         '1283959611632959489': 3,\n",
       "         '1284322339836985344': 2,\n",
       "         '1284684384776462337': 2,\n",
       "         '1285049816234422272': 2,\n",
       "         '1285409161451380736': 2,\n",
       "         '1285771549090308096': 2,\n",
       "         '1286133936716787713': 3,\n",
       "         '1286520000627372032': 2,\n",
       "         '1286862906143854593': 2,\n",
       "         '1287253428670853121': 2,\n",
       "         '1287351687296892928': 2,\n",
       "         '1287584106847510528': 2,\n",
       "         '1287945878561361920': 2,\n",
       "         '1288308266947272704': 2,\n",
       "         '1288666567933190144': 2,\n",
       "         '1289033039637557249': 2,\n",
       "         '1289191063937196036': 1,\n",
       "         '1289395426723016708': 2})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at conversation_id\n",
    "ids = []\n",
    "for dic in sdic:\n",
    "    ids.append(dic['conversation_id'])\n",
    "\n",
    "from collections import Counter\n",
    "Counter(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# independent tweet number\n",
    "len(Counter(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "ids = []\n",
    "\n",
    "for dic in sdic:\n",
    "    # if there are already same tweets ids\n",
    "    if dic['conversation_id'] in ids:\n",
    "        indx = ids.index(dic['conversation_id'])\n",
    "        tweets[indx]\n",
    "        \n",
    "        # add txt\n",
    "        tweets[indx]['text'].append(dic['tweet'])\n",
    "        # add url\n",
    "        for url in dic['urls']:\n",
    "            tweets[indx]['urls'].append(url)\n",
    "            \n",
    "    # New id \n",
    "    else:\n",
    "        ids.append(dic['conversation_id'])\n",
    "        newtweet = {}\n",
    "        newtweet['text'] = []\n",
    "        newtweet['urls'] = []\n",
    "        \n",
    "        newtweet['conversation_id'] = dic['conversation_id']\n",
    "        # add txt\n",
    "        newtweet['text'].append(dic['tweet'])\n",
    "        # add url\n",
    "        for url in dic['urls']:\n",
    "            newtweet['urls'].append(url)\n",
    "        tweets.append(newtweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'conversation_id': '1289395426723016708',\n",
       "  'text': ['PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.10985\\xa0',\n",
       "   '点群データに対する教師なしの事前学習手法を提案．ScanNetから抽出した視点の異なる2つの点群に対し，点のマッチングを基に距離学習を行う．事前学習済みモデルを転移することで屋内外を含む6種の広範なデータセットで精度向上が得られた． https://arxiv.org/abs/2007.10985\\xa0 pic.twitter.com/gEDAQZD95r'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.10985',\n",
       "   'https://arxiv.org/abs/2007.10985']},\n",
       " {'conversation_id': '1289191063937196036',\n",
       "  'text': ['@ossyaritoori ご連絡ありがとうございます．とても面白い記事ですね．転載されているのは僅かでしたしご連絡頂いたので問題ございません．今後も更新していくので引き続きご覧頂けますと幸いです．'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1289033039637557249',\n",
       "  'text': ['Neural Topological SLAM for Visual Navigation, CVPR2020\\nProject:  https://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html\\xa0… pic.twitter.com/CVya3XyiuY',\n",
       "   '目標画像位置を未知環境内で探すトポロジカルグラフSLAM＋探索手法を提案．各地点(ノード)での周囲の移動可能領域・目的地到達可能性を教師あり学習し，それをもとにグラフ上で探索行動を生成する．強化・教師なし学習ベース手法と比較し安定した学習・高精度な探索を実現． https://youtu.be/vubX97owdjQ\\xa0'],\n",
       "  'urls': ['https://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html',\n",
       "   'https://youtu.be/vubX97owdjQ']},\n",
       " {'conversation_id': '1288666567933190144',\n",
       "  'text': ['Points2Surf：Learning Implicit Surfaces from Point Clouds (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.10453\\xa0\\nProject:  https://www.cg.tuwien.ac.at/research/publications/2020/erler-p2s/\\xa0…\\nCode:  https://github.com/ErlerPhilipp/points2surf\\xa0…',\n",
       "   'ノイズや不均一な点群から物体のimplicit表面の推定手法を提案。クエリ点に対する点群のGlobalとLocal情報から、sign logitとabsolute distanceを別々に推定するネットワークを学習させ、signed distance field (SDF)を得て、TSDF へ変換、Marching Cubesで表面を生成する。\\n https://arxiv.org/abs/2007.10453\\xa0 pic.twitter.com/bWgcYMUvDy'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.10453',\n",
       "   'https://www.cg.tuwien.ac.at/research/publications/2020/erler-p2s/',\n",
       "   'https://github.com/ErlerPhilipp/points2surf',\n",
       "   'https://arxiv.org/abs/2007.10453']},\n",
       " {'conversation_id': '1288308266947272704',\n",
       "  'text': ['DeepCap: Monocular Human Performance Capture Using Weak Supervision (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.08325\\xa0\\nProject:  https://people.mpi-inf.mpg.de/~mhaberma/projects/2020-cvpr-deepcap/\\xa0…\\nVideo: https://www.youtube.com/watch?v=C4eDrvJ9aBs\\xa0…',\n",
       "   '単一の画像からの衣服を含む人体形状復元．PoseNetによる関節位置の推定とDefNetによる表面形状の変形推定を組み合わせたロス関数を設計し学習．単一画像のみから，多視点画像を利用する従来手法と競合する復元精度を実現している．\\n https://arxiv.org/abs/2003.08325\\xa0 pic.twitter.com/hH7pC6l1xs'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.08325',\n",
       "   'https://people.mpi-inf.mpg.de/~mhaberma/projects/2020-cvpr-deepcap/',\n",
       "   'https://www.youtube.com/watch?v=C4eDrvJ9aBs',\n",
       "   'https://arxiv.org/abs/2003.08325']},\n",
       " {'conversation_id': '1287945878561361920',\n",
       "  'text': ['Efficient Continuous-Time SLAM for 3D Lidar-based Online Mapping (ICRA 2018)\\nPaper:  https://arxiv.org/abs/1810.06802\\xa0\\nPaper:  https://ieeexplore.ieee.org/document/8461000\\xa0…\\nVideo: https://youtu.be/iG9MJLzja5g\\xa0',\n",
       "   '離散時間ではなく連続時間で軌跡を推定する Continuous-Time SLAM の一手法。スキャン点群のスパース（疎）性と歪みの問題に対処。Surfel を用いた位置合わせで、複数解像度の局所地図を構築。局所地図ノードとロボット位置ノードからなる階層的ポーズグラフを相互に最適化。\\n https://arxiv.org/abs/1810.06802\\xa0 pic.twitter.com/LpD2xCvjUM'],\n",
       "  'urls': ['https://arxiv.org/abs/1810.06802',\n",
       "   'https://ieeexplore.ieee.org/document/8461000',\n",
       "   'https://youtu.be/iG9MJLzja5g',\n",
       "   'https://arxiv.org/abs/1810.06802']},\n",
       " {'conversation_id': '1287584106847510528',\n",
       "  'text': ['ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM (arXiv)\\nPaper:  https://arxiv.org/abs/2007.11898\\xa0\\nProject:  https://github.com/UZ-SLAMLab/ORB_SLAM3\\xa0…\\nVideo:  https://youtu.be/HyLNq-98LRo\\xa0',\n",
       "   'ORB-SLAMの最新バージョン．ORB-SLAM2との違いはマルチマップ・システムで，過去マップに対する自己位置同定だけではなく，自己位置ロスト後に新しいマップを生成し，Place Recognition成功後に過去マップと統合しロバスト性を向上．従来法にくらべ数倍の精度向上を達成． https://youtu.be/HyLNq-98LRo\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.11898',\n",
       "   'https://github.com/UZ-SLAMLab/ORB_SLAM3',\n",
       "   'https://youtu.be/HyLNq-98LRo',\n",
       "   'https://youtu.be/HyLNq-98LRo']},\n",
       " {'conversation_id': '1287351687296892928',\n",
       "  'text': ['Privacy Preserving Structure-from-Motion (ECCV2020)\\nProject:  https://cvg.ethz.ch/research/privacy-preserving-sfm\\xa0…\\nSupp:  https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf\\xa0…\\nVideo:  https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4\\xa0… pic.twitter.com/ATTcyyT1PB',\n",
       "   'This paper proposes an incremental SfM system with privacy protection. The system uses only random 2D lines converted from 2D feature points for its initialization, camera pose estimation, triangulation, and bundle adjustment. \\n https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf\\xa0… pic.twitter.com/n9XrXyM4K5'],\n",
       "  'urls': ['https://cvg.ethz.ch/research/privacy-preserving-sfm',\n",
       "   'https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf',\n",
       "   'https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4',\n",
       "   'https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf']},\n",
       " {'conversation_id': '1287253428670853121',\n",
       "  'text': ['Privacy Preserving Structure-from-Motion (ECCV2020)\\nProject:  https://www.cvg.ethz.ch/research/privacy-preserving-sfm\\xa0…\\nSupp:  https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf\\xa0…\\nVideo:  https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4\\xa0… pic.twitter.com/hxSGXSJnfV',\n",
       "   'プライバシー保護を考慮したIncremental SfMシステムを開発．2D特徴点から変換された2D直線のみを用いた，初期化，カメラ姿勢の推定，三角測量，バンドル調整を提案．2D特徴点を利用したSfMに近い精度と，Inverstion Attackに対するより高い頑健性を実現．\\n https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf\\xa0… pic.twitter.com/bzFGa7QKLZ'],\n",
       "  'urls': ['https://www.cvg.ethz.ch/research/privacy-preserving-sfm',\n",
       "   'https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp.pdf',\n",
       "   'https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV_supp_video.mp4',\n",
       "   'https://www.cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf']},\n",
       " {'conversation_id': '1286862906143854593',\n",
       "  'text': ['Exploit Clues from Views: Self-Supervised and Regularized Learning for Multiview Object Recognition （CVPR2020）\\nPaper:  https://arxiv.org/abs/2003.12735\\xa0\\nProject:  https://chihhuiho.github.io/vispe_web/\\xa0\\nCode:  https://github.com/chihhuiho/VISPE\\xa0 pic.twitter.com/fbdVoPxoKS',\n",
       "   '多視点物体認識のための特徴量抽出を自己教師あり学習する手法を提案．代理タスクとして，オブジェクトクラス分類を通し距離学習を行う．これにより視点に因らず同一オブジェクトならば埋め込み表現上でクラスターを形成．ダウンストリームタスクで他手法より高い精度を達成．\\n https://arxiv.org/abs/2003.12735\\xa0 pic.twitter.com/YyvMehYHKd'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.12735',\n",
       "   'https://chihhuiho.github.io/vispe_web/',\n",
       "   'https://github.com/chihhuiho/VISPE',\n",
       "   'https://arxiv.org/abs/2003.12735']},\n",
       " {'conversation_id': '1286520000627372032',\n",
       "  'text': ['Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking (IROS 2020)\\nPaper:  https://arxiv.org/abs/2007.10743\\xa0 pic.twitter.com/tgcsuZcnpm',\n",
       "   'ステレオカメラの情報から動的な障害を検出，追跡するシステムの提案．ロボット周囲の物体について動的・静的の2クラスに分類．さらに動的な物体については人とそれ以外のクラスに分類する．ノイズの多いデータから高い精度の動的物体の検出,追跡が可能なことを実験で確認． https://youtu.be/AYjgeaQR8uQ\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.10743',\n",
       "   'https://youtu.be/AYjgeaQR8uQ']},\n",
       " {'conversation_id': '1286133936716787713',\n",
       "  'text': ['Thank you for your recommendation! We have already enjoyed the paper, especially for IPU, including your CVPR paper. We want to introduce them soon!',\n",
       "   'Visual SLAMの第一人者 Andrew J. Davison ( @AjdDavison ) 先生が描くSLAMの未来\\nFutureMapping: The Computational Structure of Spatial AI Systems\\nPaper:  https://arxiv.org/abs/1803.11288\\xa0',\n",
       "   'SLAM （自己位置推定と地図構築）を発展させ，シーンやオブジェクトの関係性を理解する #SpatialAI の開発が進められている ．#SpatialAI を実際のアプリケーションに応用する上で必要なアルゴリズムやプロセッサ，センサの連携などについて提唱された最初の論文．\\n https://arxiv.org/abs/1803.11288\\xa0 pic.twitter.com/bm9sSOj5qN'],\n",
       "  'urls': ['https://arxiv.org/abs/1803.11288',\n",
       "   'https://arxiv.org/abs/1803.11288']},\n",
       " {'conversation_id': '1285771549090308096',\n",
       "  'text': ['3D Packing for Self-Supervised Monocular Depth Estimation (CVPR2020)\\nPaper:  https://arxiv.org/abs/1905.02693\\xa0\\nCode:  https://github.com/TRI-ML/packnet-sfm\\xa0…\\nDataset:  https://github.com/TRI-ML/DDAD\\xa0 pic.twitter.com/4avN57SBlw',\n",
       "   '自己教師あり学習で単眼画像のデプスを推定するPackNetを提案．ピクセルをチャンネル方向に並び替えるSpace2Depthを含むPackNetにより，重要な空間情報を保持した明瞭なデプスが推定可能．既存の教師あり学習と同程度の精度を達成． https://www.youtube.com/watch?v=b62iDkLgGSI\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/1905.02693',\n",
       "   'https://github.com/TRI-ML/packnet-sfm',\n",
       "   'https://github.com/TRI-ML/DDAD',\n",
       "   'https://www.youtube.com/watch?v=b62iDkLgGSI']},\n",
       " {'conversation_id': '1285409161451380736',\n",
       "  'text': ['JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.06888\\xa0\\nCode: https://github.com/hzykent/JSENet\\xa0',\n",
       "   '3次元点群に対してセグメンテーションとクラス境界の推定を同時に行う手法を提案．相互に関連する両タスクを同時に解くだけでなく，双方の推定結果を用いてさらに精緻化を行うNNモジュールを提案．屋内データ（S3DIS）でSOTAを達成．\\n https://arxiv.org/abs/2007.06888\\xa0 pic.twitter.com/O52VDKG07v'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.06888',\n",
       "   'https://github.com/hzykent/JSENet',\n",
       "   'https://arxiv.org/abs/2007.06888']},\n",
       " {'conversation_id': '1285049816234422272',\n",
       "  'text': ['Free-Space Features: Global Localization in 2D Laser SLAM  Using Distance Function Maps (IROS2019)\\nPaper:  https://arxiv.org/abs/1908.01863\\xa0',\n",
       "   '2D SLAMにおける地図表現にSDFを導入し，計測点が存在しない空間の情報を使った局所特徴(free-space features)を提案．曲率ベースの特徴点検出と方向付き勾配ヒストグラムを使った記述子を使い，従来手法より大域位置認識が高精度に行えることを示した．\\n https://arxiv.org/abs/1908.01863\\xa0 pic.twitter.com/GJSmADnz12'],\n",
       "  'urls': ['https://arxiv.org/abs/1908.01863',\n",
       "   'https://arxiv.org/abs/1908.01863']},\n",
       " {'conversation_id': '1284684384776462337',\n",
       "  'text': ['3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.13867\\xa0\\nProject:  https://francisengelmann.github.io/3D-MPA/\\xa0 pic.twitter.com/kgBVirhajr',\n",
       "   '三次元点群の意味的な特徴とCenter VotesからCenter Proposalを生成、GCNでProposalの特徴をリファイン、Proposalの合体によるInstance SegmentationのMulti Proposal Aggregation Network(MPA)を提案。既存手法のNon-Maximum-Suppression(NMS)と比べて、MPAの優位性を確認。 https://youtu.be/ifL8yTbRFDk\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.13867',\n",
       "   'https://francisengelmann.github.io/3D-MPA/',\n",
       "   'https://youtu.be/ifL8yTbRFDk']},\n",
       " {'conversation_id': '1284322339836985344',\n",
       "  'text': ['World-Consistent Video-to-Video Synthesis (ECCV2020)\\nPaper:  https://arxiv.org/abs/2007.08509\\xa0\\nProject:  https://nvlabs.github.io/wc-vid2vid/\\xa0\\nVideo: https://youtu.be/rlCh6-2NfSg\\xa0',\n",
       "   '大域的な一貫性を保ったvid2vid．直前の数フレームに基づきクエリ(セマンティクス画像)に対応する画像生成を行う従来法では，同じ位置に立ち戻る場合に一貫性が保証されない．提案手法では，SfMを利用して環境を逐次的に3次元復元し，その幾何をガイドとした画像生成を行う．\\n https://arxiv.org/abs/2007.08509\\xa0 pic.twitter.com/uaQO8g5ofF'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.08509',\n",
       "   'https://nvlabs.github.io/wc-vid2vid/',\n",
       "   'https://youtu.be/rlCh6-2NfSg',\n",
       "   'https://arxiv.org/abs/2007.08509']},\n",
       " {'conversation_id': '1283959611632959489',\n",
       "  'text': ['関連研究\\nLeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain (IROS 2018)\\nPaper:  https://ieeexplore.ieee.org/document/8594299\\xa0…\\nCode:  https://github.com/RobustFieldAutonomyLab/LeGO-LOAM\\xa0…\\nVideo:  https://youtu.be/O3tz_ftHV48\\xa0',\n",
       "   'LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping (IROS 2020)\\nPaper:  https://arxiv.org/abs/2007.00258\\xa0\\nCode:  https://github.com/TixiaoShan/LIO-SAM\\xa0…\\nVideo:  https://youtu.be/A0H8CoORZJU\\xa0',\n",
       "   'IMU preintegration で点群を歪み補正して初期推定。エッジと平面の特徴点で LIDAR オドメトリ。GTSAM（iSAM2）で全体と局所のグラフ最適化。LIDAR-IMU のタイトカップリングでリアルタイム。ループ拘束は近傍マッチング。GNSS 拘束も。最長 19 km のデータで地図構築に成功。\\n https://arxiv.org/abs/2007.00258\\xa0 pic.twitter.com/ALNln6fwZX'],\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/8594299',\n",
       "   'https://github.com/RobustFieldAutonomyLab/LeGO-LOAM',\n",
       "   'https://youtu.be/O3tz_ftHV48',\n",
       "   'https://arxiv.org/abs/2007.00258',\n",
       "   'https://github.com/TixiaoShan/LIO-SAM',\n",
       "   'https://youtu.be/A0H8CoORZJU',\n",
       "   'https://arxiv.org/abs/2007.00258']},\n",
       " {'conversation_id': '1283601093612548099',\n",
       "  'text': ['Tightly-coupled Fusion of Global Positional Measurements in Optimization-based Visual-Inertial Odometry (IROS2020)\\nPaper:  https://arxiv.org/abs/2003.04159\\xa0',\n",
       "   'Pre-Integration を用いた Tight-coupled な Visual-Inertial Odometry (VIO) に、GPS等によるグローバル座標拘束を導入する手法を初めて提案．従来法はVIOの後段にカルマンフィルタ等を用いて分割して対処．提案手法はコスト関数に拘束を組込み一括で最適化．\\n https://arxiv.org/abs/2003.04159\\xa0 pic.twitter.com/Im1bHSvCyO'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.04159',\n",
       "   'https://arxiv.org/abs/2003.04159']},\n",
       " {'conversation_id': '1283234833829945345',\n",
       "  'text': ['著者ツイート\\n@rmurai0610',\n",
       "   'BIT-VO: Visual Odometry at 300 FPS using Binary Features from the Focal Plane (IROS2020)\\nPaper:  https://arxiv.org/abs/2004.11186\\xa0\\nProject:  https://rmurai0610.github.io/BIT-VO\\xa0\\nVideo:  https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title\\xa0…',\n",
       "   '画素毎に並列計算可能なFocal-plane Sensor-processor (FPSP) を用いたVisual OdometryアルゴリズムBIT-VOを提案．FPSP上で，アナログ信号の領域で2値のエッジ検出を行い，そのエッジからバイナリ特徴を計算．それらをホストデバイスに転送することで300fpsの6DOF VOを実現． https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.11186',\n",
       "   'https://rmurai0610.github.io/BIT-VO',\n",
       "   'https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title',\n",
       "   'https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title']},\n",
       " {'conversation_id': '1282872986467397633',\n",
       "  'text': ['Self-Supervised Viewpoint Learning From Image Collections (CVPR2020)\\nPaper:  https://arxiv.org/abs/2004.01793\\xa0\\nProject:  https://research.nvidia.com/publication/2020-03_Self-Supervised-Viewpoint-Learning\\xa0…\\nCode:  https://github.com/NVlabs/SSV\\xa0 pic.twitter.com/9TgUW7zLPZ',\n",
       "   '画像からの物体方向推定を，self-supervisedに学習する枠組みを提案．画像から3次元方向とスタイル特徴量を抽出し，それらの潜在変数を元に幾何学的変換を行うGenerator(GAN)を用いて学習．損失には一貫性と水平対称性を利用し，教師あり学習に匹敵する性能を達成．\\n https://arxiv.org/abs/2004.01793\\xa0 pic.twitter.com/Qr4LM3uTxA'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.01793',\n",
       "   'https://research.nvidia.com/publication/2020-03_Self-Supervised-Viewpoint-Learning',\n",
       "   'https://github.com/NVlabs/SSV',\n",
       "   'https://arxiv.org/abs/2004.01793']},\n",
       " {'conversation_id': '1282510058660737024',\n",
       "  'text': ['ODE-CNN: Omnidirectional Depth Extension Networks (ICRA 2020)\\nPaper:  https://arxiv.org/abs/2007.01475\\xa0',\n",
       "   'Kinect等のPerspectiveなdepthセンサーと全方位画像を用いて全方位のdepthを得る手法の提案.Encoderの最後の層でPerspective座標に変換し特徴量の学習難度を下げ、Decoderでequirectangular座標に戻す.他のSoTAな手法より優れていることを示した.\\n https://arxiv.org/abs/2007.01475\\xa0 pic.twitter.com/ZYPB7ysvaY'],\n",
       "  'urls': ['https://arxiv.org/abs/2007.01475',\n",
       "   'https://arxiv.org/abs/2007.01475']},\n",
       " {'conversation_id': '1282203799256350721',\n",
       "  'text': ['360-Indoor: Towards Learning Real-World Objects in 360◦ Indoor Equirectangular Images (WACV2020)\\nPaper: \\n https://arxiv.org/abs/1910.01712\\xa0',\n",
       "   '360度屋内画像における物体検出とクラス認識に関するデータセットを提示．Equirectangular形式における極領域の歪みに対応するため，Sphere Netをはじめとする球状CNNを用いたモデルで評価したところ，透視投影画像によるデータセットでの学習よりも大きな改善が見られた． https://arxiv.org/abs/1910.01712\\xa0 pic.twitter.com/buiy0FHupl'],\n",
       "  'urls': ['https://arxiv.org/abs/1910.01712',\n",
       "   'https://arxiv.org/abs/1910.01712']},\n",
       " {'conversation_id': '1281794468920713217',\n",
       "  'text': ['Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction (ECCV2020 Poster)\\nVideo1:  https://youtu.be/MffXsKjy9W0\\xa0\\nVideo2:  https://youtu.be/OOPJpHexrdE\\xa0\\nVideo3:  https://youtu.be/PMYI9j5vHOw\\xa0\\nPaper:  https://arxiv.org/abs/2004.10681\\xa0',\n",
       "   'CNNで推定した擬似的なデプスマップを用いてRGB-D SLAMを行う．単眼デプス推定の欠点であるスケールの不整合性を，特徴点ベースのSLAMで作成された三次元点を用いてリファインする．これにより，両者の欠点を補った高精度な姿勢推定が可能となった．\\n https://arxiv.org/abs/2004.10681\\xa0 pic.twitter.com/OXOuG5onDG'],\n",
       "  'urls': ['https://youtu.be/MffXsKjy9W0',\n",
       "   'https://youtu.be/OOPJpHexrdE',\n",
       "   'https://youtu.be/PMYI9j5vHOw',\n",
       "   'https://arxiv.org/abs/2004.10681',\n",
       "   'https://arxiv.org/abs/2004.10681']},\n",
       " {'conversation_id': '1281422893826494464',\n",
       "  'text': ['FPConv: Learning Local Flattening for Point Convolution (CVPR2020)\\nPaper:  https://arxiv.org/abs/2002.10701\\xa0\\nRelated work:  https://arxiv.org/abs/1807.02443\\xa0\\nCode: https://github.com/lyqun/FPConv\\xa0',\n",
       "   '平面への投影を用いた点群畳み込みを提案．明示的に接平面を推定するTangentConvとは異なり，点群の投影と内挿を単一の重み行列で表現し，MLPを用いて学習ベースで推定する．Volmetricな畳み込みとの組み合わせでSoTA達成．\\n https://arxiv.org/abs/2002.10701\\xa0 pic.twitter.com/ZmJ3UuwIg1'],\n",
       "  'urls': ['https://arxiv.org/abs/2002.10701',\n",
       "   'https://arxiv.org/abs/1807.02443',\n",
       "   'https://github.com/lyqun/FPConv',\n",
       "   'https://arxiv.org/abs/2002.10701']},\n",
       " {'conversation_id': '1281074772625874946',\n",
       "  'text': ['1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image (RA-L/ICRA2019)\\nPaper:  https://ieeexplore.ieee.org/abstract/document/8633942\\xa0…\\nCode: https://github.com/irapkaist/scancontext\\xa0…',\n",
       "   'LIDARを使った位置推定手法．極座標で作られた高さマップ(Scan Context)を入力とし，CNNで地図上での位置をクラスとして推定する．複数の実データセット上で，一日の学習データで一年通した長期位置推定が高精度に可能であることを示した． https://youtu.be/apmmduXTnaE\\xa0'],\n",
       "  'urls': ['https://ieeexplore.ieee.org/abstract/document/8633942',\n",
       "   'https://github.com/irapkaist/scancontext',\n",
       "   'https://youtu.be/apmmduXTnaE']},\n",
       " {'conversation_id': '1280724052026392576',\n",
       "  'text': ['BSP-Net: Generating Compact Meshes via Binary Space Partitioning (CVPR2020 Best Student Paper)\\nPaper:  https://arxiv.org/abs/1911.06971\\xa0\\nProject:  https://bsp-net.github.io/\\xa0\\nCode:  https://github.com/czq142857/BSP-NET-original\\xa0… pic.twitter.com/G9DKBtwEnm',\n",
       "   '教師なしでコンパクトかつウォータータイトな三次元メッシュ生成手法を提案。Binary Space Partitioning (BSP) で再帰的に入力形状を超平面に分解し、Constructive Solid Geometry (CSG) のブーリアン演算で、分解した超平面から複雑な表面やオブジェクトの生成が可能となる。 https://youtu.be/9-ixexpjN-8\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/1911.06971',\n",
       "   'https://bsp-net.github.io/',\n",
       "   'https://github.com/czq142857/BSP-NET-original',\n",
       "   'https://youtu.be/9-ixexpjN-8']},\n",
       " {'conversation_id': '1280345060442288129',\n",
       "  'text': ['Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery (RA-L2019)\\nPaper:  https://arxiv.org/abs/1903.00268\\xa0\\nCode:  https://github.com/ethz-asl/voxblox-plusplus\\xa0…\\nVideo:  https://youtu.be/Jvl42VJmYxg\\xa0 pic.twitter.com/CiR0JzN43I',\n",
       "   'インスタンスを意識したRGB-Dセンサによる地図生成．幾何的なセグメンテーション結果をMask R-CNNから補正し，Over-segmentationを抑制した個別の物体形状を獲得．各物体の幾何を大域地図上で関連付けていくことで，セマンティクス＆インスタンス情報も付与した地図を生成． https://youtu.be/Jvl42VJmYxg\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/1903.00268',\n",
       "   'https://github.com/ethz-asl/voxblox-plusplus',\n",
       "   'https://youtu.be/Jvl42VJmYxg',\n",
       "   'https://youtu.be/Jvl42VJmYxg']},\n",
       " {'conversation_id': '1279973343249281025',\n",
       "  'text': ['Visual-Inertial Mapping with Non-Linear Factor Recovery (RA-L & ICRA 2020)\\nPaper:  https://arxiv.org/abs/1904.06504\\xa0\\nProject:  https://vision.in.tum.de/research/vslam/basalt\\xa0…\\nCode:  https://gitlab.com/VladyslavUsenko/basalt\\xa0…\\nVideo:  https://youtu.be/r3CJ2JP75Tc\\xa0',\n",
       "   'Visual-Inertial SLAM の Basalt。IMU の preintegration ではなく、非線形因子復元を行って大域的に最適化。IMU の積分は誤差が大きい問題に対処。VIO の相対位置拘束とロール・ピッチ拘束、バンドル調整のループ拘束を統合。小さな最適化問題として定式化でき、精度も向上。\\n https://arxiv.org/abs/1904.06504\\xa0 pic.twitter.com/mQYfoQ4yjd'],\n",
       "  'urls': ['https://arxiv.org/abs/1904.06504',\n",
       "   'https://vision.in.tum.de/research/vslam/basalt',\n",
       "   'https://gitlab.com/VladyslavUsenko/basalt',\n",
       "   'https://youtu.be/r3CJ2JP75Tc',\n",
       "   'https://arxiv.org/abs/1904.06504']},\n",
       " {'conversation_id': '1279610956172017666',\n",
       "  'text': ['2D Laser SLAM With General Features Represented by Implicit Functions (RA-L2020)\\nPaper:  https://ieeexplore.ieee.org/document/9099049\\xa0…',\n",
       "   '任意の環境形状を表せる陰関数表現を用い2D SLAMを定式化．陰関数に対する分散の導出や陰関数境界内外での最適化の安定化などを行い，楕円・直線モデルを用いた評価実験では従来のモデルフィッティングベース手法より良い精度を示した．\\n https://ieeexplore.ieee.org/document/9099049\\xa0… pic.twitter.com/HnpTxH7bL6'],\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/9099049',\n",
       "   'https://ieeexplore.ieee.org/document/9099049']},\n",
       " {'conversation_id': '1279258907534213121',\n",
       "  'text': ['Statistical Outlier Identification in Multi-robot Visual SLAM using Expectation Maximization\\nPaper:  https://arxiv.org/abs/2002.02638\\xa0',\n",
       "   '複数マップ間のループ検出における外れ値検出手法を提案．各ノード間の回転から閉ループの幾何的整合性をチェックすることで確率的に外れ値を検出．さらにEMアルゴリズムを用いてパラメータをfine-tune．確率伝搬法より高い精度を達成し，収束性も保証．\\n https://arxiv.org/abs/2002.02638\\xa0 pic.twitter.com/zugcqk2JzE'],\n",
       "  'urls': ['https://arxiv.org/abs/2002.02638',\n",
       "   'https://arxiv.org/abs/2002.02638']},\n",
       " {'conversation_id': '1278904690546098176',\n",
       "  'text': ['SLAM-Hub members got one paper accepted by #ECCV2020 !'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1278904428246888448',\n",
       "  'text': ['コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1278886178670796801',\n",
       "  'text': ['Vid2Curve: Simultaneous Camera Motion Estimation and Thin Structure Reconstruction from an RGB Video (SIGGRAPH 2020)\\nProject:  https://totoro97.github.io/projects/vid2curve/\\xa0…\\nCode:  https://github.com/Totoro97/Vid2Curve\\xa0…\\nPaper:  https://arxiv.org/abs/2005.03372\\xa0',\n",
       "   'ワイヤーフレームや電線など細い物体に対する形状復元の手法を提案．視点の追加ごとに，点群表現のカーブとカメラポーズを新しいマッチング手法に基づき交互に最適化。また，オクルージョンを検知し誤対応を防止．カーブの各部で太さを推定することで高品質な復元が可能に． https://youtu.be/dI2FZG_txN0\\xa0'],\n",
       "  'urls': ['https://totoro97.github.io/projects/vid2curve/',\n",
       "   'https://github.com/Totoro97/Vid2Curve',\n",
       "   'https://arxiv.org/abs/2005.03372',\n",
       "   'https://youtu.be/dI2FZG_txN0']},\n",
       " {'conversation_id': '1278523792042496000',\n",
       "  'text': ['3D Human Mesh Regression with Dense Correspondence\\nCode:  https://github.com/zengwang430521/DecoMR\\xa0…\\nPaper:  https://arxiv.org/abs/2006.05734\\xa0',\n",
       "   '一枚の画像から人体の3D Meshを推定する手法.画像ピクセルと表面間の密な対応を推定し,その対応により画像空間からUV空間へ局所的な特徴が移され,位置マップに回帰される.最後にマッピング関数により3D Meshを再構成する.3D Meshベースの従来手法より優れていることを示した.\\n https://arxiv.org/abs/2006.05734\\xa0 pic.twitter.com/Ku422DMyAp'],\n",
       "  'urls': ['https://github.com/zengwang430521/DecoMR',\n",
       "   'https://arxiv.org/abs/2006.05734',\n",
       "   'https://arxiv.org/abs/2006.05734']},\n",
       " {'conversation_id': '1278243781104463872',\n",
       "  'text': ['“Self-supervised Simultaneous Alignment and Change Detection”,\\nYukuko Furukawa, Kumiko Suzuki, Ryuhei Hamaguchi, Masaki Onishi, Ken Sakurada',\n",
       "   '\"Non-overlapping RGB-D Camera Network Calibration with Monocular Visual Odometry\",\\nKenji Koide, Emanuele Menegatti',\n",
       "   '\"C*: Cross-modal Simultaneous Tracking And Rendering for 6-DoF Monocular Camera Localization Beyond Modalities\",\\nShuji Oishi, Yasunori Kawamata, Masashi Yokozuka, Kenji Koide, Atsuhiko Banno, Jun Miura',\n",
       "   '”LiTAMIN: LiDAR based Tracking And MappINg by Stabilized ICP for Geometry Approximation with Normal Distributions”,\\nMasashi Yokozuka, Kenji Koide, Shuji Oishi, Atsuhiko Banno',\n",
       "   'SLAM-Hub members got 4 papers accepted by IROS2020!'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1278242853508898816',\n",
       "  'text': ['“Self-supervised Simultaneous Alignment and Change Detection”,\\nYukuko Furukawa, Kumiko Suzuki, Ryuhei Hamaguchi, Masaki Onishi, Ken Sakurada',\n",
       "   '\"Non-overlapping RGB-D Camera Network Calibration with Monocular Visual Odometry\",\\nKenji Koide, Emanuele Menegatti',\n",
       "   '\"C*: Cross-modal Simultaneous Tracking And Rendering for 6-DoF Monocular Camera Localization Beyond Modalities\",\\nShuji Oishi, Yasunori Kawamata, Masashi Yokozuka, Kenji Koide, Atsuhiko Banno, Jun Miura',\n",
       "   '”LiTAMIN: LiDAR based Tracking And MappINg by Stabilized ICP for Geometry Approximation with Normal Distributions”,\\nMasashi Yokozuka, Kenji Koide, Shuji Oishi, Atsuhiko Banno',\n",
       "   'ロボティクス分野の国際会議IROS2020にSLAM-Hubのメンバーから4本の論文が採択されました！'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1278188128444940288',\n",
       "  'text': ['Object-Centric Learning with Slot Attention (Under review)\\nPaper :  https://arxiv.org/abs/2006.15055\\xa0',\n",
       "   'シーン分解や集合予測のアーキテクチャに統合可能な，オブジェクト中心の抽象表現を学習するSlot Attentionモジュールを提案．CNNの出力と構造表現間において，順列不変なk個のSlotを生成．反復的注意メカニズムでSlotのグループ化戦略を学習．点群やグラフのグループ化も可能 https://youtu.be/DYBmD88vpiA\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.15055',\n",
       "   'https://youtu.be/DYBmD88vpiA']},\n",
       " {'conversation_id': '1277799017053859842',\n",
       "  'text': ['An Analysis of SVD for Deep Rotation Estimation\\nPaper:  https://arxiv.org/abs/2006.14616\\xa0',\n",
       "   '深層学習における性質の良い回転表現を提案．\\n回転行列を一度9パラメータで表現し，SVDによる特殊直交化によりSO(3)空間へマップする．\\n深層学習タスクにおいてクォータニオンやangle-axisベクトルなどの他の回転表現より高精度に姿勢を求めることが可能．\\n https://arxiv.org/abs/2006.14616\\xa0 pic.twitter.com/WVhQcNGBad'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.14616',\n",
       "   'https://arxiv.org/abs/2006.14616']},\n",
       " {'conversation_id': '1277436627485290496',\n",
       "  'text': ['Grid-GCN for Fast and Scalable Point Cloud Learning (CVPR2020)\\nPaper:  https://arxiv.org/abs/1912.02984\\xa0\\nSupp:  https://xharlie.github.io/papers/GGCN_supCamReady.pdf\\xa0…\\nCode: https://github.com/Xharlie/Grid-GCN\\xa0…',\n",
       "   '高速でスケーラブルな点群データの処理機構を提案．Voxelを用いることで高速かつカバー率の高いサンプリングを行い，Voxel内部でローカルにグラフを構築して畳み込む．点群の分類とセグメンテーションで従来手法より高速かつ高精度を達成．\\n https://arxiv.org/abs/1912.02984\\xa0 pic.twitter.com/nlIsA1sC2v'],\n",
       "  'urls': ['https://arxiv.org/abs/1912.02984',\n",
       "   'https://xharlie.github.io/papers/GGCN_supCamReady.pdf',\n",
       "   'https://github.com/Xharlie/Grid-GCN',\n",
       "   'https://arxiv.org/abs/1912.02984']},\n",
       " {'conversation_id': '1277074304786546689',\n",
       "  'text': ['PointTriNet: Learned Triangulation of 3D Point Sets (arXiv)\\nPaper:  https://arxiv.org/abs/2005.02138\\xa0',\n",
       "   'PointNetに類似したネットワークを用い点群から三角形メッシュを生成．入力点群から三角形群を出力するネットと，入力三角形群の中から3Dモデルとして妥当な三角形を判定するネットを交互に適用し，メッシュモデルを復元する．\\n https://arxiv.org/abs/2005.02138\\xa0 pic.twitter.com/8JeOljgxXg'],\n",
       "  'urls': ['https://arxiv.org/abs/2005.02138',\n",
       "   'https://arxiv.org/abs/2005.02138']},\n",
       " {'conversation_id': '1276742451806404608',\n",
       "  'text': ['Meshlet Priors for 3D Mesh Reconstruction (CVPR2020)\\nPaper:  https://arxiv.org/abs/2001.01744\\xa0\\nCode:  https://github.com/NVlabs/meshlets\\xa0 pic.twitter.com/9pOqOHCVFd',\n",
       "   'スパースまたノイジーな点群からメッシュ生成のため、ローカル幾何形状を表現するMeshletを提案。VAEでMeshletをポーズ不変な潜在空間にエンコード、点群と近いMeshlet(補助メッシュから取出)をデコード、変形の補助メッシュを利用、Meshlet間のグローバルな整合性を強める。 https://youtu.be/glZyJ66ktog\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2001.01744',\n",
       "   'https://github.com/NVlabs/meshlets',\n",
       "   'https://youtu.be/glZyJ66ktog']},\n",
       " {'conversation_id': '1276361064104341506',\n",
       "  'text': ['NRMVS: Non-Rigid Multi-View Stereo (WACV2020)\\nVideo:  https://www.youtube.com/watch?v=B4YBWFuYBdE\\xa0…\\nPaper:  http://openaccess.thecvf.com/content_WACV_2020/papers/Innmann_NRMVS_Non-Rigid_Multi-view_Stereo_WACV_2020_paper.pdf\\xa0… pic.twitter.com/5uHD5WJ9gC',\n",
       "   '非剛体変形下での多視点ステレオの提案．まず変形の程度の小さい画像ペアを一組決定し，対象の基本的な3次元構造を復元．さらに，その他の変形を伴う画像群に対してもDeformation graphを利用したJoint optimizationにより，DeformationとDepthの推定を同時に行っている． https://youtu.be/B4YBWFuYBdE\\xa0'],\n",
       "  'urls': ['https://www.youtube.com/watch?v=B4YBWFuYBdE',\n",
       "   'http://openaccess.thecvf.com/content_WACV_2020/papers/Innmann_NRMVS_Non-Rigid_Multi-view_Stereo_WACV_2020_paper.pdf',\n",
       "   'https://youtu.be/B4YBWFuYBdE']},\n",
       " {'conversation_id': '1276006512670830593',\n",
       "  'text': ['関連研究 https://twitter.com/slam_hub/status/1272001420745535489\\xa0…',\n",
       "   'Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping (ICRA 2020)\\nVideo:  https://youtu.be/-5XxXRABXJs\\xa0\\nCode:  https://github.com/MIT-SPARK/Kimera\\xa0…\\nPaper:  https://arxiv.org/abs/1910.02490\\xa0',\n",
       "   'Visual-Inertial SLAM の Kimera。IMU の on-manifold preintegration と画像の Shi-Tomasi コーナー特徴点で VIO。DBoW2 でループ検出、GTSAM（iSAM2）でグラフ最適化。メッシュ生成と TSDF での復元。画像でセマンティックラベリングして逆投影し、ボクセルをベイズで更新。\\n https://arxiv.org/abs/1910.02490\\xa0 pic.twitter.com/RSnDbyYV23'],\n",
       "  'urls': ['https://twitter.com/slam_hub/status/1272001420745535489',\n",
       "   'https://youtu.be/-5XxXRABXJs',\n",
       "   'https://github.com/MIT-SPARK/Kimera',\n",
       "   'https://arxiv.org/abs/1910.02490',\n",
       "   'https://arxiv.org/abs/1910.02490']},\n",
       " {'conversation_id': '1275630109311950849',\n",
       "  'text': ['Atlas: End-to-End 3D Scene Reconstruction from Posed Images (arXiv)\\nPaper:  https://arxiv.org/abs/2003.10432\\xa0',\n",
       "   '複数視点の2D-CNN出力をVoxelにBack-projetして蓄積し，Voxel mapを3D-CNNに通すことでSemantic情報を含んだMulti-view Stereoを実現．2D-CNNにはResnet50-FPN，3D-CNNはSkip Connectionを持つEncoder-decoderを利用．実時間処理が可能．\\n https://arxiv.org/abs/2003.10432\\xa0 pic.twitter.com/e3e3J3TEua'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.10432',\n",
       "   'https://arxiv.org/abs/2003.10432']},\n",
       " {'conversation_id': '1275262302594293762',\n",
       "  'text': ['Privacy-Preserving Visual Feature Descriptors through Adversarial Affine Subspace Embedding (arXiv)\\nPaper:  https://arxiv.org/abs/2006.06634\\xa0 pic.twitter.com/3BksIPzqMs',\n",
       "   '特徴量を，それ自身を含む部分アフィン空間へ埋め込みことで，識別機能を保ちながらプライバシーアタックへの耐性を大幅に向上．部分空間同士の距離を導入し特徴マッチングを可能とした．元の特徴量と比較して，僅かな識別性能の低下により高いプラバシー保護性能を実現．\\n https://arxiv.org/abs/2006.06634\\xa0 pic.twitter.com/ex4qczr200'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.06634',\n",
       "   'https://arxiv.org/abs/2006.06634']},\n",
       " {'conversation_id': '1274899915655843840',\n",
       "  'text': [' pic.twitter.com/SZBbuTPgIl',\n",
       "   'Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes (CVPR2020 Oral)\\nPaper:  http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\\xa0…\\nCode: https://github.com/lzqsd/TransparentShapeReconstruction\\xa0…',\n",
       "   '複数画像からガラスオブジェクトの3D形状を再構築するネットワークを提案．Visual hullで得た荒い形状を元に，各視点で屈折，反射点の法線を推論．環境マップでレンダリングした再投影誤差と，視点間を統合した点群とGTとの損失で学習．高品質な3D復元が可能なことを実証．\\n http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\\xa0… pic.twitter.com/uXXFhuXt98'],\n",
       "  'urls': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html',\n",
       "   'https://github.com/lzqsd/TransparentShapeReconstruction',\n",
       "   'http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html']},\n",
       " {'conversation_id': '1274537524241797121',\n",
       "  'text': ['VPLNet: Deep Single View Normal Estimation With Vanishing Points and Lines (CVPR2020)\\nPaper:  http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\\xa0…',\n",
       "   '単一画像での法線推定手法の提案.RGB画像とマンハッタン線マップを入力とし,マンハッタン方向に沿う領域を識別するマップと法線マップをネットワークで回帰,融合する.従来手法より優れた結果を示し未見のデータに対しても推定可能なことを示した.\\n http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\\xa0… pic.twitter.com/vRPvFPGMEV'],\n",
       "  'urls': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html',\n",
       "   'http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html']},\n",
       " {'conversation_id': '1274205587597737985',\n",
       "  'text': ['LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World (CVPR2020)\\nPaper:  https://arxiv.org/abs/2006.09348\\xa0\\nSupp:  http://openaccess.thecvf.com/content_CVPR_2020/html/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.html\\xa0…',\n",
       "   'Lidarによる実世界データを用いた，従来のCADモデルによる手法よりも多彩で現実感の高い自動運転用シミュレーションを提案．Lidar点群から動的物体や環境マップなどのアセットを作成後，物理レンダリングとDNNでドメインギャップの小さなセンサシミュレーションを行う． https://arxiv.org/abs/2006.09348\\xa0 pic.twitter.com/nrtVaRw315'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.09348',\n",
       "   'http://openaccess.thecvf.com/content_CVPR_2020/html/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.html',\n",
       "   'https://arxiv.org/abs/2006.09348']},\n",
       " {'conversation_id': '1273816472230363138',\n",
       "  'text': ['Revisiting visual-inertial structure from motion for odometry and SLAM initialization (arXiv)\\nPaper:  https://arxiv.org/abs/2006.06017\\xa0',\n",
       "   'VIO, VI-SLAMにおける状態変数の初期化手法を提案．効率的に不要変数を除去しつつ，3つ以上の3D点の観測を平等に扱う新たな定式化．この線形ソルバはシンプルな構造ながら過去の手法と比較してモーション推定の精度を最大50%向上させ，非線形ソルバの反復回数も削減．\\n https://arxiv.org/abs/2006.06017\\xa0 pic.twitter.com/IYHoycp0k8'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.06017',\n",
       "   'https://arxiv.org/abs/2006.06017']},\n",
       " {'conversation_id': '1273450361563435008',\n",
       "  'text': ['PointAugment: an Auto-Augmentation Framework for Point Cloud Classification (CVPR2020)\\nPaper:  https://arxiv.org/abs/2002.10876\\xa0\\nCode: https://github.com/liruihui/PointAugment/\\xa0…',\n",
       "   'End-to-endに学習可能な点群データのAugmentorを提案．入力点群ごとに全体の変形量と個々の点の変位量を出力し，分類器にとってより難しい変換となるよう敵対的に学習する．複数のモデルでランダムな水増しより良い精度を達成．\\n https://arxiv.org/abs/2002.10876\\xa0 pic.twitter.com/Zso2mevGNk'],\n",
       "  'urls': ['https://arxiv.org/abs/2002.10876',\n",
       "   'https://github.com/liruihui/PointAugment/',\n",
       "   'https://arxiv.org/abs/2002.10876']},\n",
       " {'conversation_id': '1273118847114018816',\n",
       "  'text': ['LiDAR-based vehicle localization on the satellite image via a neural network (Robotics and Autonomous Systems 2020)\\nPaper:  https://www.sciencedirect.com/science/article/pii/S0921889019305202\\xa0…\\nVideo: https://www.sciencedirect.com/science/article/pii/S0921889019305202#mmc1\\xa0…',\n",
       "   'LIDARスキャンを基に衛星画像上での自己位置推定を行う手法を提案．推定にはパーティクルフィルタを用い，各パーティクル位置の衛星画像とLIDARスキャンの一致度を測るネットワークによって評価する．衛星画像上での遮蔽・陰影に頑強な位置推定が可能であることを示した．\\n https://www.sciencedirect.com/science/article/pii/S0921889019305202\\xa0… pic.twitter.com/4PelxvyMge'],\n",
       "  'urls': ['https://www.sciencedirect.com/science/article/pii/S0921889019305202',\n",
       "   'https://www.sciencedirect.com/science/article/pii/S0921889019305202#mmc1',\n",
       "   'https://www.sciencedirect.com/science/article/pii/S0921889019305202']},\n",
       " {'conversation_id': '1272725587359076352',\n",
       "  'text': [' pic.twitter.com/EGCDq1YNgd',\n",
       "   'MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views (IROS2020 submission)\\nPaper:  https://arxiv.org/abs/2006.05518\\xa0\\nProject:  https://research.nvidia.com/publication/2020-06_MVLidarNet\\xa0…',\n",
       "   '三次元点群の透視投影画像でセマンティックセグメンテーションを行い、分割結果が反応したBEV画像から物体を検出する、シンプルかつ高効率な2-stage検出手法を提案。既存手法と精度の差が大きくない上で、組み込みGPUでもマルチクラス物体検出と道路の分割を150 FPSで実現。 https://youtu.be/2ck5_sToayc\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.05518',\n",
       "   'https://research.nvidia.com/publication/2020-06_MVLidarNet',\n",
       "   'https://youtu.be/2ck5_sToayc']},\n",
       " {'conversation_id': '1272363203498930176',\n",
       "  'text': ['Global visual localization in LiDAR-maps through shared 2D-3D embedding space (ICRA2020)\\nVideo:  https://www.facebook.com/iralabdisco/videos/icra2020-submission-global-visual-localization-in-lidar-maps-through-shared-2d-3/371792790436848/\\xa0…\\nPaper:  https://arxiv.org/abs/1910.04871\\xa0',\n",
       "   '広域なLiDAR地図における単眼カメラの大域位置同定．異種のデータに対するShared embedding spaceを獲得するため，2D-CNNと3D-DNNを一緒に学習する枠組みを提案．同種データ内で完結するSame-Modality lossに加え，異種データ間でCross-Modality lossを用いて学習を行った．\\n https://arxiv.org/abs/1910.04871\\xa0 pic.twitter.com/0Ft0zuZoH8'],\n",
       "  'urls': ['https://www.facebook.com/iralabdisco/videos/icra2020-submission-global-visual-localization-in-lidar-maps-through-shared-2d-3/371792790436848/',\n",
       "   'https://arxiv.org/abs/1910.04871',\n",
       "   'https://arxiv.org/abs/1910.04871']},\n",
       " {'conversation_id': '1272001420745535489',\n",
       "  'text': ['関連研究\\nKimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping (ICRA 2020)\\nVideo:  https://youtu.be/-5XxXRABXJs\\xa0\\nCode:  https://github.com/MIT-SPARK/Kimera\\xa0…\\nPaper:  https://arxiv.org/abs/1910.02490\\xa0',\n",
       "   '3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans (RSS 2020)\\nVideo:  https://youtu.be/SWbofjhyPzI\\xa0\\nPaper:  https://arxiv.org/abs/2002.06289\\xa0',\n",
       "   '建物、部屋、物体などの関係を表すシーングラフを動的環境に拡張し、3D Dynamic Scene Graphs を提案。Visual-Inertial SLAM の Kimera を用いてセマンティックマッピング。さらに移動物体（人のメッシュモデル）をトラッキングして、時空間の物体モデル構造を階層的に表現。\\n https://arxiv.org/abs/2002.06289\\xa0 pic.twitter.com/F5yOdQlh7F'],\n",
       "  'urls': ['https://youtu.be/-5XxXRABXJs',\n",
       "   'https://github.com/MIT-SPARK/Kimera',\n",
       "   'https://arxiv.org/abs/1910.02490',\n",
       "   'https://youtu.be/SWbofjhyPzI',\n",
       "   'https://arxiv.org/abs/2002.06289',\n",
       "   'https://arxiv.org/abs/2002.06289']},\n",
       " {'conversation_id': '1271651964275683329',\n",
       "  'text': ['OccuSeg: Occupancy-aware 3D Instance Segmentation (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.06537\\xa0\\nyoutube:  https://youtu.be/co7y6LQ7Kqc\\xa0 pic.twitter.com/PyloRJb7Z8',\n",
       "   'Voxel ベースの U-Net でsemanticsを推定し，Super-voxel間の類似度を計算して3D instance segmentationを実現．U-Netで各Instanceに対するVoxelの占有数(＝体積)を推定し，適切にSuper-voxelをクラスタリングして Instance を生成．ScanNet Benchmark の現在１位． https://youtu.be/co7y6LQ7Kqc\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.06537',\n",
       "   'https://youtu.be/co7y6LQ7Kqc',\n",
       "   'https://youtu.be/co7y6LQ7Kqc']},\n",
       " {'conversation_id': '1271276037117218816',\n",
       "  'text': ['3D Photography using Context-aware Layered Depth Inpainting (CVPR2020)\\nPaper:  https://arxiv.org/abs/2004.04727\\xa0\\nProject:  https://shihmengli.github.io/3D-Photo-Inpainting/\\xa0…\\nCode: https://github.com/vt-vl-lab/3d-photo-inpainting\\xa0…',\n",
       "   'RGB-D単眼画像を入力とし，視点を変えると発生する空白領域をインペインティングするモデルの提案．Depthの断層と層状のDepth表現という着想をベースに，各層で背景を外側へ補完するようにRGB-Dを推定．Mesh表現に変換することで，エッジデバイスでも軽快に動作可能． https://www.youtube.com/watch?v=D0JObXCfxv0\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.04727',\n",
       "   'https://shihmengli.github.io/3D-Photo-Inpainting/',\n",
       "   'https://github.com/vt-vl-lab/3d-photo-inpainting',\n",
       "   'https://www.youtube.com/watch?v=D0JObXCfxv0']},\n",
       " {'conversation_id': '1270913650086178817',\n",
       "  'text': ['Point2Mesh: A Self-Prior for Deformable Meshes (SIGGRAPH2020)\\nProject:\\n https://ranahanocka.github.io/point2mesh/\\xa0\\nCode:\\n https://github.com/ranahanocka/Point2Mesh/\\xa0…\\nYoutube: https://www.youtube.com/watch?v=AySwwJuPqOk&feature=emb_title\\xa0…',\n",
       "   'ノイズや欠損を含む点群から高精細な水密メッシュモデルを生成する手法を提案．coarse-to-fineで初期メッシュのエッジの移動量を推定し，入力点群自身と損失を計算して反復的に誤差逆伝搬することでself priorを学習．平滑化仮定では生成できない微細なメッシュも生成可能．\\n https://arxiv.org/abs/2005.11084\\xa0 pic.twitter.com/QLCq7LsaEX'],\n",
       "  'urls': ['https://ranahanocka.github.io/point2mesh/',\n",
       "   'https://github.com/ranahanocka/Point2Mesh/',\n",
       "   'https://www.youtube.com/watch?v=AySwwJuPqOk&feature=emb_title',\n",
       "   'https://arxiv.org/abs/2005.11084']},\n",
       " {'conversation_id': '1270551260056387585',\n",
       "  'text': [' pic.twitter.com/LECto0vyTT',\n",
       "   ' pic.twitter.com/UAT3HsEmrY',\n",
       "   'Novel Object Viewpoint Estimation through Reconstruction Alignment (CVPR 2020)\\nProject:  https://mbanani.github.io/novelviewpoints/\\xa0…\\nCode: https://github.com/mbanani/novelviewpoints\\xa0…',\n",
       "   '未知物体を撮影した画像間の相対姿勢の推定. 学習に用いられた物体以外の視点の推定は困難であったが２枚の画像を3D特徴グリッドにマッピングする学習を行い位置を合わせることで相対的な位置を推定する.学習時と大きく異なる物体で推論する際に従来手法より良い精度を示した.\\n https://arxiv.org/abs/2006.03586\\xa0 pic.twitter.com/SktdplZMb1'],\n",
       "  'urls': ['https://mbanani.github.io/novelviewpoints/',\n",
       "   'https://github.com/mbanani/novelviewpoints',\n",
       "   'https://arxiv.org/abs/2006.03586']},\n",
       " {'conversation_id': '1270188872232660993',\n",
       "  'text': ['4D Visualization of Dynamic Events from Unconstrained Multi-View Videos (CVPR2020)\\nProject:  http://www.cs.cmu.edu/~aayushb/Open4D/\\xa0…\\nCode:  https://github.com/aayushbansal/Open4D\\xa0…\\nPaper:  https://arxiv.org/abs/2005.13532\\xa0',\n",
       "   '複数カメラで撮影された動的イベントに対して，視点と時間を移動可能な4次元時空間可視化を行うシステム．シーン特化のself-supervisedなCNNを用いて静的・動的部分の抽出を行う．SfMによる既存手法で困難であった非ランバート面や，テクスチャレスな領域もキャプチャ可能に． https://youtu.be/sq2hhkHgtb0\\xa0'],\n",
       "  'urls': ['http://www.cs.cmu.edu/~aayushb/Open4D/',\n",
       "   'https://github.com/aayushbansal/Open4D',\n",
       "   'https://arxiv.org/abs/2005.13532',\n",
       "   'https://youtu.be/sq2hhkHgtb0']},\n",
       " {'conversation_id': '1269826483599618049',\n",
       "  'text': ['An Efficient Planar Bundle Adjustment Algorithm\\nPaper:  https://arxiv.org/abs/2006.00187\\xa0',\n",
       "   '点群が平面状に分布する制約を加えたPlaner Bundle Adjustmentを提案．ヤコビアン行列のコンパクトな表現を含む新たな定式化によって精度向上と計算量の削減を両立．評価実験で同問題設定のSOTAと比較して高速，高精度に，そして初期値にロバストなことが示された．\\n https://arxiv.org/abs/2006.00187\\xa0 pic.twitter.com/rVKDwkK3LH'],\n",
       "  'urls': ['https://arxiv.org/abs/2006.00187',\n",
       "   'https://arxiv.org/abs/2006.00187']},\n",
       " {'conversation_id': '1269519361787613184',\n",
       "  'text': ['Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.01251\\xa0\\nCode: https://github.com/WeijingShi/Point-GNN\\xa0…',\n",
       "   'GNNを用いた三次元点群からの物体検出手法を提案．近傍点を結んだグラフからGNNで特徴抽出し，点ごとに所属する物体クラスとBBOXを推定．最後に重複したBBOXを中央値で統合する．KITTTIデータセットで従来手法を上回る精度を達成．\\n https://arxiv.org/abs/2003.01251\\xa0 pic.twitter.com/Qu1jOIW3xo'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.01251',\n",
       "   'https://github.com/WeijingShi/Point-GNN',\n",
       "   'https://arxiv.org/abs/2003.01251']},\n",
       " {'conversation_id': '1269104347004022785',\n",
       "  'text': ['PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking (RSS2019)\\nPaper:  https://arxiv.org/abs/1905.09304\\xa0\\nPresentation:  https://youtu.be/pknL_nyirZ4?t=295\\xa0… pic.twitter.com/c5IaCqhBli',\n",
       "   '6DoF物体姿勢推定のためのRBPFを提案．姿勢分布を分解し，平行移動はサンプリング，回転は物体の各回転に対するEmbeddingを予め計算しておき，パーティクルのEmbeddingをこれと比較することで評価．6DoFを200パーティクル程度でロバストに推定しSOTA精度． https://youtu.be/lE5gjzRKWuA\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/1905.09304',\n",
       "   'https://youtu.be/pknL_nyirZ4?t=295',\n",
       "   'https://youtu.be/lE5gjzRKWuA']},\n",
       " {'conversation_id': '1268739321369989121',\n",
       "  'text': ['Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds (ICLR2020)\\nProject:  https://ge.in.tum.de/publications/2020-iclr-prantl/\\xa0…\\nCode:  https://gitlab.com/Prantl/NeuralParticles\\xa0…\\nPaper:  https://openreview.net/forum?id=BJeKh3VYDH\\xa0… pic.twitter.com/ZcMRH1HQqc',\n",
       "   '安定的にフレキシブルな時系列点群の生成手法を提案。既存手法の安定性と多様性の両立し難い問題を改善するため、新たなTemporal Lossを導入、点群から時間的一貫性がある特徴を学習し、変形可能な数が多い点群にたしても有効性を示す。 https://youtu.be/6OoRZrqfSJ4\\xa0'],\n",
       "  'urls': ['https://ge.in.tum.de/publications/2020-iclr-prantl/',\n",
       "   'https://gitlab.com/Prantl/NeuralParticles',\n",
       "   'https://openreview.net/forum?id=BJeKh3VYDH',\n",
       "   'https://youtu.be/6OoRZrqfSJ4']},\n",
       " {'conversation_id': '1268387507969785856',\n",
       "  'text': ['FroDO: From Detections to 3D Objects (CVPR2020)\\nProject:  https://research.fb.com/publications/frodo-from-detections-to-3d-objects/\\xa0…\\nPaper:  https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\\xa0…',\n",
       "   '多視点のRGB画像列に基づく物体の3次元復元手法を提案．Point / Surface による相補的な形状デコードにより形状表現の効率性と記述力を両立させており，より高速な形状復元を実現している．\\n https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\\xa0… pic.twitter.com/Y0iFz7omo2'],\n",
       "  'urls': ['https://research.fb.com/publications/frodo-from-detections-to-3d-objects/',\n",
       "   'https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf',\n",
       "   'https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf']},\n",
       " {'conversation_id': '1268013854698557440',\n",
       "  'text': ['関連研究 https://twitter.com/slam_hub/status/1256059179724271616\\xa0…',\n",
       "   'OverlapNet: Loop Closing for LiDAR-based SLAM (RSS 2020)\\nProject:  https://www.ipb.uni-bonn.de/people/xieyuanli-chen/\\xa0…\\nCode (coming soon):  https://github.com/PRBonn/OverlapNet\\xa0…\\nPaper (pdf):  https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf\\xa0… pic.twitter.com/8Yy5coRclW',\n",
       "   'SLAM で重要なループ検出を、位置合わせなしで end-to-end に実現。3D-LIDAR の距離画像、法線、受光強度、セマンティクスを入力。2つのスキャンの重複率とヨー角を推定。ループ拘束は SLAM 側で求める。重なりが小さくても適切にループ検出し、SuMa より高精度な地図を構築。 https://youtu.be/YTfliBco6aw\\xa0'],\n",
       "  'urls': ['https://twitter.com/slam_hub/status/1256059179724271616',\n",
       "   'https://www.ipb.uni-bonn.de/people/xieyuanli-chen/',\n",
       "   'https://github.com/PRBonn/OverlapNet',\n",
       "   'https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf',\n",
       "   'https://youtu.be/YTfliBco6aw']},\n",
       " {'conversation_id': '1267727467101315072',\n",
       "  'text': ['A Modular Optimization Framework for Localization and Mapping (RSS2019)\\nCode:  https://github.com/MOLAorg/mola\\xa0\\nPaper:  http://www.roboticsproceedings.org/rss15/p43.pdf\\xa0\\nPresentation:  https://youtu.be/qwh8hGEJSlA\\xa0 pic.twitter.com/KLDOehbecC',\n",
       "   'SLAMを構成要素(入出力, フロント/バックエンド, マップストレージ)に分割し，センサ種類・個数，マッピング方式(global map vs local submaps)，状態空間(SE2/SE3/SE3+vel)などの違いを包括的に扱えるミドルウェア寄りのライブラリを提案． http://youtu.be/Bb92aMBJR44\\xa0'],\n",
       "  'urls': ['https://github.com/MOLAorg/mola',\n",
       "   'http://www.roboticsproceedings.org/rss15/p43.pdf',\n",
       "   'https://youtu.be/qwh8hGEJSlA',\n",
       "   'http://youtu.be/Bb92aMBJR44']},\n",
       " {'conversation_id': '1267289768544460800',\n",
       "  'text': ['Towards Better Generalization: Joint Depth-Pose Learning without PoseNet (CVPR2020)\\nGitHub https://github.com/B1ueber2y/TrianFlow\\xa0…',\n",
       "   'オプティカルフローを経由して8点法により直接推定した相対姿勢，さらにそこから計算した３次元点を自己教師とすることで，スケールの推定をネットワークから分離し，高い汎化性能とスケールの一貫性を実現．屋内外のデータセットでORB-SLAMや学習ベースの手法を凌駕．\\n https://arxiv.org/abs/2004.01314\\xa0 pic.twitter.com/XU3nSsMtX4'],\n",
       "  'urls': ['https://github.com/B1ueber2y/TrianFlow',\n",
       "   'https://arxiv.org/abs/2004.01314']},\n",
       " {'conversation_id': '1266927379089248256',\n",
       "  'text': ['DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-motion from Monocular Videos (IROS 2020)',\n",
       "   'Unsupervised単眼Depth推定で精度を改善する2つの機構の提案．特定のDepth誤りを，フォトメトリックエラーをもとにした外れ値Maskを導入し対処．また，重み付きマルチスケール機構でアーティファクトを除去．簡単に追加できる機構で，他手法よりも高い精度を達成．\\n https://arxiv.org/abs/2003.01360\\xa0 pic.twitter.com/I0JA9KV4Lo'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.01360']},\n",
       " {'conversation_id': '1266564992805076997',\n",
       "  'text': [' pic.twitter.com/Fs98y7CS2Z',\n",
       "   ' pic.twitter.com/96fNlTaANF',\n",
       "   'Deep Implicit Volume Compression (CVPR 2020 Oral)\\nProject  https://augmentedperception.github.io/deep_volume_compression/\\xa0…\\nPaper\\n https://arxiv.org/abs/2005.08877\\xa0 pic.twitter.com/AujHzInHUf',\n",
       "   'Volumetricな表現で使用されるTSDFとそれに対応したテクスチャを圧縮する新しい方法を提案．End-to-Endで訓練されたニューラルネットを用い，トポロジカルなエラーを防ぐためにTSDFの符号を失わずに圧縮する．従来手法より優れた圧縮率と歪みのトレードオフを得た． https://youtu.be/GuLzjnFGDKs\\xa0'],\n",
       "  'urls': ['https://augmentedperception.github.io/deep_volume_compression/',\n",
       "   'https://arxiv.org/abs/2005.08877',\n",
       "   'https://youtu.be/GuLzjnFGDKs']},\n",
       " {'conversation_id': '1266202604868075520',\n",
       "  'text': ['VDO-SLAM: A Visual Dynamic Object-aware SLAM System (submitted to International Journal of Robotics Research)\\nCode: https://github.com/halajun/vdo_slam\\xa0…',\n",
       "   'モーションセグメンテーション，動的物体追跡，カメラ姿勢，シーン剛体の姿勢変化や速度の計算を全て行い，実世界の屋外シナリオで実証可能な世界初の動的SLAMシステムを提案．ロバスト性の向上の為，カメラと物体の動きの推定はOptical Flowの改良と合わせて因子グラフ最適化\\n https://arxiv.org/abs/2005.11052\\xa0 pic.twitter.com/esR5t04wPi'],\n",
       "  'urls': ['https://github.com/halajun/vdo_slam',\n",
       "   'https://arxiv.org/abs/2005.11052']},\n",
       " {'conversation_id': '1265849825678913537',\n",
       "  'text': ['関連研究\\nSuperPoint: Self-Supervised Interest Point Detection and Description\\nPaper:  https://arxiv.org/abs/1712.07629\\xa0\\nNeural-Guided RANSAC: Learning Where to Sample Model Hypotheses\\nPaper:  https://arxiv.org/abs/1905.04132\\xa0',\n",
       "   'Reinforced Feature Points:Optimizing Feature Detection and Description for a High-Level Task (CVPR2020 Oral)\\nPaper:  https://arxiv.org/abs/1912.00623\\xa0 pic.twitter.com/bGcxPKEvcA',\n",
       "   '画像の特徴点検出と記述子表現をend-to-endに学習する手法を提案．特徴点マッチングで誤差伝搬できないため，画像間の相対姿勢誤差を負の報酬ににした強化学習で特徴点検出CNNと記述子推定CNNをトレーニングする．学習ベースの局所特徴量抽出器としてSOTAを達成． https://youtu.be/Zttl3eDjNyc\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/1712.07629',\n",
       "   'https://arxiv.org/abs/1905.04132',\n",
       "   'https://arxiv.org/abs/1912.00623',\n",
       "   'https://youtu.be/Zttl3eDjNyc']},\n",
       " {'conversation_id': '1265477828973416448',\n",
       "  'text': ['SampleNet: Differentiable Point Cloud Sampling (CVPR2020)\\nCode:  https://github.com/itailang/SampleNet\\xa0…\\nPaper:  https://arxiv.org/abs/1912.03663\\xa0\\nRelated:  https://arxiv.org/abs/1812.01659\\xa0 pic.twitter.com/grZNpshhiI',\n",
       "   '微分可能な三次元点群のサンプリング手法を提案．NNによって入力点群を簡素化するDovratらの手法を拡張．最近傍サンプリングをk近傍の重み付き和で近似することで，簡素化した点群を基に入力点群をサンプリングするステップを微分可能にした． https://www.youtube.com/watch?v=JHz_ImeI8HE\\xa0…'],\n",
       "  'urls': ['https://github.com/itailang/SampleNet',\n",
       "   'https://arxiv.org/abs/1912.03663',\n",
       "   'https://arxiv.org/abs/1812.01659',\n",
       "   'https://www.youtube.com/watch?v=JHz_ImeI8HE']},\n",
       " {'conversation_id': '1265120071728283648',\n",
       "  'text': ['G2L-Net: Global to Local Network for Real-time 6D Pose Estimation withEmbedding Vector Features (CVPR2020)\\nVideo:  https://youtu.be/a5JWe6mOAEs\\xa0\\nCode:  https://github.com/DC1991/G2L_Net\\xa0\\nPaper:  https://arxiv.org/abs/2003.11089\\xa0 pic.twitter.com/lghuSEkzDC',\n",
       "   'RGB-D画像から特定の物体の6DoF姿勢を3段階で推定．1.2D物体認識で対象物を含む点群を抽出．2.PointNetを利用し詳細な物体抽出と並進量を推定．3.並進後，回転量をPointNetで推定．回転量推定の学習には，各点に付加した方向ベクトルが真値の方向になるように学習． https://youtu.be/a5JWe6mOAEs\\xa0'],\n",
       "  'urls': ['https://youtu.be/a5JWe6mOAEs',\n",
       "   'https://github.com/DC1991/G2L_Net',\n",
       "   'https://arxiv.org/abs/2003.11089',\n",
       "   'https://youtu.be/a5JWe6mOAEs']},\n",
       " {'conversation_id': '1264753052122308615',\n",
       "  'text': ['ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes (CVPR2020)\\nPaper:  https://arxiv.org/abs/2001.10692\\xa0',\n",
       "   '画像からgeometric、semantic、textureのVote特徴を抽出、三次元点群のVote特徴と融合し、3D物体検出の手法を提案。Multi-modalデータ融合を改善するにmulti-towerとgradient blendingの構造を使用し、SUNRGB-Dで既存SOTAより5.7mAPの精度を向上させ、SLAMようなSparse点群に対する有効性も確認。 pic.twitter.com/6zfURbMEPw'],\n",
       "  'urls': ['https://arxiv.org/abs/2001.10692']},\n",
       " {'conversation_id': '1264390814454607872',\n",
       "  'text': ['PrimiTect: Fast Continuous Hough Voting for Primitive Detection (ICRA2020)\\nCode:  https://github.com/c-sommer/primitect\\xa0…\\nPaper:  https://arxiv.org/abs/2005.07457\\xa0',\n",
       "   'RGB-Dカメラ等で計測された3次元点群における幾何プリミティブ(円柱，円錐，球)の検出．Point Pair Feature に各幾何プリミティブの形状に応じた拘束を導入，またLinear interpolation votingを幾何プリミティブ用に改良し，ハフ変換による低計算量での頑健な検出を実現．\\n https://arxiv.org/abs/2005.07457\\xa0 pic.twitter.com/AylPuLfTAw'],\n",
       "  'urls': ['https://github.com/c-sommer/primitect',\n",
       "   'https://arxiv.org/abs/2005.07457',\n",
       "   'https://arxiv.org/abs/2005.07457']},\n",
       " {'conversation_id': '1264027851038154753',\n",
       "  'text': ['Visual Odometry Revisited: What Should Be Learnt? (ICRA 2020)\\nVideo:  https://youtu.be/Nl8mFU4SJKY\\xa0\\nCode:  https://github.com/Huangying-Zhan/DF-VO\\xa0…\\nPaper:  https://arxiv.org/abs/1909.09803\\xa0',\n",
       "   'End-to-End 深層学習の Visual Odometry (VO) の性能は、まだ幾何ベースの手法に及ばない。そこで VO の基礎を再検討し、エピポーラ幾何や PnP と深層学習を組み合わせる手法を提案。スケール整合性のある単眼深度推定とオプティカルフローの2つの CNN を利用。KITTI dataset の評価で従来手法を凌駕。 pic.twitter.com/9C4yJ9i1gv'],\n",
       "  'urls': ['https://youtu.be/Nl8mFU4SJKY',\n",
       "   'https://github.com/Huangying-Zhan/DF-VO',\n",
       "   'https://arxiv.org/abs/1909.09803']},\n",
       " {'conversation_id': '1263670128400404480',\n",
       "  'text': ['Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences (CVPR2020)\\nPaper\\n https://arxiv.org/abs/2005.01014\\xa0\\nCode (中身はまだ未公開？) https://github.com/XiaoshuiHuang/fmr\\xa0…',\n",
       "   '二つの点群について，特徴空間上で並進・回転に対するヤコビアンを数値微分で求めLucus-Kanade法で位置合わせを実行．また点群に対するEncoder-Decorderを構築し特徴をUn-supervisedまたはSemi-supervisedで学習を可能にした．\\n https://arxiv.org/abs/2005.01014\\xa0 pic.twitter.com/JZeCcd11hS'],\n",
       "  'urls': ['https://arxiv.org/abs/2005.01014',\n",
       "   'https://github.com/XiaoshuiHuang/fmr',\n",
       "   'https://arxiv.org/abs/2005.01014']},\n",
       " {'conversation_id': '1263303500836069376',\n",
       "  'text': ['Self-Supervised Scene De-occlusion (CVPR2020 Oral)\\nParper:  https://arxiv.org/abs/2004.02788\\xa0\\nProject:  https://xiaohangzhan.github.io/projects/deocclusion/\\xa0…',\n",
       "   'self-supervisedに学習可能な，画像のオクルージョン領域を復元するモデルの提案．物体ごとの被オクルージョン領域推定で，増加範囲からオクルージョン関係のグラフを構築．その後推定した領域MaskからRGBを復元．教師あり学習に匹敵するパフォーマンスを達成． https://www.youtube.com/watch?v=xIHCyyaB5gU\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.02788',\n",
       "   'https://xiaohangzhan.github.io/projects/deocclusion/',\n",
       "   'https://www.youtube.com/watch?v=xIHCyyaB5gU']},\n",
       " {'conversation_id': '1262941112664117248',\n",
       "  'text': ['Self-Supervised Deep Visual Odometry with Online Adaptation (CVPR 2020 Oral)',\n",
       "   '自己教師Visual Odometry手法を提案．畳み込みLSTMを利用することで過去の経験を活かして推定をし，未知のシーンにオンラインで適応することが可能．さらにオープンワールドでの環境の変化に対応するために特徴量を揃える手法を提案．既存の手法を大きく上回ることを実験で確認 https://arxiv.org/abs/2005.06136\\xa0 pic.twitter.com/YMw3oYqEK3'],\n",
       "  'urls': ['https://arxiv.org/abs/2005.06136']},\n",
       " {'conversation_id': '1262595837172973569',\n",
       "  'text': ['End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection (CVPR2020)\\nGitHub https://github.com/mileyan/pseudo-LiDAR_e2e\\xa0…',\n",
       "   '画像ベースの深度マップを擬似LiDARに変換し3次元物体検出するというパイプライン全体を，End-to-Endで学習するフレームワーク．既存手法では深度推定と物体検出で別学習していたが，間の表現変化をプーリングと量子化の工夫で微分可能にし実現．PointRCNNと組み合わせでSOTA.\\n https://arxiv.org/abs/2004.03080\\xa0 pic.twitter.com/7TfZbZ079s'],\n",
       "  'urls': ['https://github.com/mileyan/pseudo-LiDAR_e2e',\n",
       "   'https://arxiv.org/abs/2004.03080']},\n",
       " {'conversation_id': '1262216336865976320',\n",
       "  'text': ['SuperGlue: Learning Feature Matching with Graph Neural Networks (CVPR2020 Oral)\\nProject:\\n https://psarlin.com/superglue/\\xa0\\nGitHub: https://github.com/magicleap/SuperGluePretrainedNetwork\\xa0…',\n",
       "   '画像間の局所特徴量をマッチングするGNNを用いたアルゴリズムを提案．視点の大幅な違いにも適用可能．2種類のアテンション機構により画像内，画像間でユニークな特徴量を活用する．GPUでリアルタイム動作し，既存手法と比べ屋内外のシーンで大幅に性能向上． https://arxiv.org/abs/1911.11763\\xa0 pic.twitter.com/4yuITowWG7'],\n",
       "  'urls': ['https://psarlin.com/superglue/',\n",
       "   'https://github.com/magicleap/SuperGluePretrainedNetwork',\n",
       "   'https://arxiv.org/abs/1911.11763']},\n",
       " {'conversation_id': '1261853950791135233',\n",
       "  'text': ['PointRend: Image Segmentation as Rendering (arXiv)\\nCode:  https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend\\xa0…',\n",
       "   'CGレンダリングにヒントを得たセグメンテーションの精緻化モジュールを提案．不確かな点をサンプリングし，MLPで推定し直すことで適応的に物体境界を精緻化．Mask-RCNNやFCNに取り付けることで，少ない計算コストで精度向上．\\n https://arxiv.org/abs/1912.08193\\xa0 pic.twitter.com/KwQ6ASEwyf'],\n",
       "  'urls': ['https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend',\n",
       "   'https://arxiv.org/abs/1912.08193']},\n",
       " {'conversation_id': '1261492927231975425',\n",
       "  'text': ['To Learn or Not to Learn: Visual Localization from Essential Matrices (ICRA2020)\\nPaper:  https://arxiv.org/abs/1908.01293\\xa0\\nCode: https://github.com/GrumpyZhou/visloc-relapose\\xa0…',\n",
       "   '画像による自己位置推定の精度評価．Data-drivenな手法は，精度面でIndirect法等の従来手法に劣ることが通説となっている．本論文では，特徴量抽出や基礎行列計算等の各フェーズをハンドクラフトからData-drivenまで程度を変え，各組み合わせにおける精度を検証している．\\n https://arxiv.org/abs/1908.01293\\xa0 pic.twitter.com/Rt4IfVGCGK'],\n",
       "  'urls': ['https://arxiv.org/abs/1908.01293',\n",
       "   'https://github.com/GrumpyZhou/visloc-relapose',\n",
       "   'https://arxiv.org/abs/1908.01293']},\n",
       " {'conversation_id': '1261129175722799106',\n",
       "  'text': ['RPM-Net: Robust Point Matching using Learned Features (CVPR2020)\\nPaper:  https://arxiv.org/abs/2003.13479\\xa0\\nProject:  https://github.com/yewzijian/RPMNet\\xa0… pic.twitter.com/P627OysyT8',\n",
       "   '微分可能なSinkhornレイヤーを使い、hybrid特徴から点と点のソフトな対応を取り、 誤対応やoverlapが少ない点群ペアでも対処できる学習ベースRobust Point Matching点群位置合わせ手法を提案。ModelNet40での実験結果で(rule-&learning-based)既存手法より優れた性能を示す。 https://youtu.be/7hxGmMk4MZ0\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.13479',\n",
       "   'https://github.com/yewzijian/RPMNet',\n",
       "   'https://youtu.be/7hxGmMk4MZ0']},\n",
       " {'conversation_id': '1260767145731821568',\n",
       "  'text': ['DOOR-SLAM: Distributed, Online, and Outlier Resilient SLAM for Robotic Teams (RA-L)\\npaper:  https://arxiv.org/abs/1909.12198\\xa0\\nyoutube:  https://www.youtube.com/watch?v=h0bqURQlZGA\\xa0… pic.twitter.com/TTS8dMExZq',\n",
       "   '複数ロボット上で走るStereoVOを統合する分散グラフSLAMを提案．ロボット間ループ制約の中から一貫性を保つ最大集合を探す最大クリーク問題を解いて誤検出を除去し，積極的にループ追加を行う方針を採用．大量のループ誤検出を除去し，一貫した地図を生成できることを示した． https://www.youtube.com/watch?v=h0bqURQlZGA\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/1909.12198',\n",
       "   'https://www.youtube.com/watch?v=h0bqURQlZGA',\n",
       "   'https://www.youtube.com/watch?v=h0bqURQlZGA']},\n",
       " {'conversation_id': '1260418715004416009',\n",
       "  'text': ['Voxgraph: Globally Consistent, Volumetric Mapping using Signed Distance Function Submaps (RA-L 2020)\\nCode:  https://github.com/ethz-asl/voxgraph\\xa0…\\nPaper:  https://ieeexplore.ieee.org/document/8903279\\xa0… pic.twitter.com/kudUjBy2vy',\n",
       "   'Volumetric な地図表現のグラフベース SLAM 手法．SDF サブマップの集合で環境形状を表現．SDF を利用した位置合わせで隣接拘束を生成してポーズグラフ最適化．対応付け不要なので計算コストが低い．ループ閉じ込みは，外部から DBoW などを利用してループ拘束を与える． https://youtu.be/N9p1_Fkxxro\\xa0'],\n",
       "  'urls': ['https://github.com/ethz-asl/voxgraph',\n",
       "   'https://ieeexplore.ieee.org/document/8903279',\n",
       "   'https://youtu.be/N9p1_Fkxxro']},\n",
       " {'conversation_id': '1260043290415489025',\n",
       "  'text': ['OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems (ICRA2020)\\nPaper\\n https://arxiv.org/abs/2003.08056\\xa0\\nYoutube\\n https://youtu.be/RFhH4j0gzsI\\xa0\\nRelated work: OmniMVS, ROVO\\n https://arxiv.org/abs/1908.06257\\xa0\\n https://arxiv.org/abs/1902.11154\\xa0',\n",
       "   '4つの魚眼カメラを用いてロバストなオドメトリ、全方位のデプス画像生成、密な環境復元を実現．全方位のデプス画像生成にはEnd-to-Endの学習ベースによるOmniMVSを用いて生成．推定した全方位デプス画像をTSDFで統合して密な環境復元を行う．\\n https://youtu.be/RFhH4j0gzsI\\xa0 pic.twitter.com/JfqoF0LKsA'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.08056',\n",
       "   'https://youtu.be/RFhH4j0gzsI',\n",
       "   'https://arxiv.org/abs/1908.06257',\n",
       "   'https://arxiv.org/abs/1902.11154',\n",
       "   'https://youtu.be/RFhH4j0gzsI']},\n",
       " {'conversation_id': '1259679622674870272',\n",
       "  'text': ['Learning to Explore using Active Neural SLAM (ICLR2020)\\nPaper\\n https://openreview.net/pdf?id=HklXn1BKDH\\xa0…\\nProject\\n https://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html\\xa0…\\nCode\\n https://github.com/devendrachaplot/Neural-SLAM\\xa0…\\nYoutube https://youtu.be/yl9eQkVdZco\\xa0',\n",
       "   '画像列から占有格子地図の推定と未知領域探索の行動選択を学習するための強化学習フレームワークを提案．地図と姿勢を推定する\"Neraul SLAM module\"，長期的ゴールを決定する\"Global policy\"，行動を決定する\"Local policy\"の3つの要素で構成．CVPR 2019 Habitat PointGoal Navigation Challenで優勝． pic.twitter.com/onV54zPZDC'],\n",
       "  'urls': ['https://openreview.net/pdf?id=HklXn1BKDH',\n",
       "   'https://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html',\n",
       "   'https://github.com/devendrachaplot/Neural-SLAM',\n",
       "   'https://youtu.be/yl9eQkVdZco']},\n",
       " {'conversation_id': '1259387630472409088',\n",
       "  'text': ['Weakly Supervised Semantic Segmentation in 3D Graph-Structured Point Clouds of Wild Scenes',\n",
       "   '3D点群のクラス推定を2Dのセグメンテーションマップのみを教師として学習するモデルの提案．各点のクラスとvisibilityを推論し，これらで合成した2Dセグメンテーションマップと教師との損失をもとに学習する．複数の物体を含む大規模なシーンでも高い性能を達成． https://arxiv.org/abs/2004.12498v1\\xa0… pic.twitter.com/C4Gefp6dFI'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.12498v1']},\n",
       " {'conversation_id': '1258592458109321220',\n",
       "  'text': ['GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization (ICRA2020) pic.twitter.com/isbHbJFRDF',\n",
       "   '単眼SLAMの初期化手法を提案.Homography推定後,Global Plane Optimization (GPO)で最適化しカメラ姿勢と平面の法線を取得.複数フレームの平面情報を組み合わせることで三角測量やHomography分解の計算負荷を減らすことが可能で,精度とリアルタイム性で優れていることを示した.\\n https://arxiv.org/abs/2004.12051\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.12051']},\n",
       " {'conversation_id': '1258230071900475393',\n",
       "  'text': ['Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences (submitted to IROS2020)\\nPaper\\n https://arxiv.org/abs/2004.00740\\xa0\\nGithub\\n https://github.com/levenberg/2D-3D-pose-tracking\\xa0…',\n",
       "   '事前のLiDARマップを用いた単眼カメラ定位手法．マップからofflineで3D線を，AFMでビデオからonlineで2D線を検出．VINS-Monoからのカメラ動き予測により，2D-3D線の対応を取得．その2D-3D対応を用いたポーズ最適化により，ループクローズなしで、VIOのドリフトを低減させた． https://youtu.be/H80Bnxm8IPE\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.00740',\n",
       "   'https://github.com/levenberg/2D-3D-pose-tracking',\n",
       "   'https://youtu.be/H80Bnxm8IPE']},\n",
       " {'conversation_id': '1257867682533330947',\n",
       "  'text': ['Learning Feature Descriptors using Camera Pose Supervision\\nProject\\n https://qianqianwang68.github.io/DescfromPose/\\xa0',\n",
       "   '画像間の相対姿勢のみを用いた弱教師あり学習による特徴量抽出手法を提案．中間特徴の相関に基づく微分可能なマッチング層やcourse-to-fine構造のネットワークを用い，エピポーラ幾何の拘束を組み込んだ損失関数で学習を行う．教師あり学習による既存手法の精度を上回った．\\n https://arxiv.org/abs/2004.13324\\xa0 pic.twitter.com/CDPfuoZCOm'],\n",
       "  'urls': ['https://qianqianwang68.github.io/DescfromPose/',\n",
       "   'https://arxiv.org/abs/2004.13324']},\n",
       " {'conversation_id': '1257506731942273024',\n",
       "  'text': ['PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation (CVPR2020 Oral)\\n https://arxiv.org/abs/2004.01658v1\\xa0… pic.twitter.com/VZFyGePMms',\n",
       "   '三次元点群のインスタンスセグメンテーション手法の研究．VoxelベースのU-Netで各点のクラスラベルと物体中心へのオフセットを推定．推定された座標について点群をクラスタリングすることで物体候補を生成し，後段のNNでスコアを出力する． https://www.youtube.com/watch?v=HMetye3gmAs\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2004.01658v1',\n",
       "   'https://www.youtube.com/watch?v=HMetye3gmAs']},\n",
       " {'conversation_id': '1257146424707280896',\n",
       "  'text': ['Real-Time Global Registration for Globally Consistent RGB-D SLAM (IEEE Transactions on Robotics)\\nPaper:  https://ieeexplore.ieee.org/document/8606275\\xa0…',\n",
       "   'リアルタイムでGlobally ConsistentなRGB-D SLAMのため，最適化空間を線形な特徴量部分と非線形な姿勢部分に分解．線形部分を特徴量の二次統計量で表すことで，効率的に計算を行う．処理時間をフレーム数に対してほぼリニアな増加に抑えつつ，SOTA精度を達成．\\nPaper:  https://ieeexplore.ieee.org/document/8606275\\xa0… pic.twitter.com/2VsW10Ca6T'],\n",
       "  'urls': ['https://ieeexplore.ieee.org/document/8606275',\n",
       "   'https://ieeexplore.ieee.org/document/8606275']},\n",
       " {'conversation_id': '1256726778132828162',\n",
       "  'text': ['複数視点の画像を利用しているのは学習時のみで，推定は常に単眼です．（各視点の画像を独立に推定しているのに，動画として整合性が取れているのがポイントです）'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1256780519343140864',\n",
       "  'text': ['三次元点群のOne-Stage物体検出Hybrid Voxel Network (HVNet)手法を提案。Pointwiseでのmulti-scale特徴をHybrid Voxel Feature Extraction(HVFE)で抽出、Voxelwise attention featureにエンコード、Pseudo-Image Featureへデカップル、リアルタイムの31HzでSOTAを達成。\\n https://arxiv.org/abs/2003.00186\\xa0 pic.twitter.com/0yyB3iaOkU'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.00186']},\n",
       " {'conversation_id': '1256466307647602693',\n",
       "  'text': [' pic.twitter.com/YVJknvUhHF',\n",
       "   ' pic.twitter.com/fDFNfyDBvc',\n",
       "   'Consistent Video Depth Estimation (SIGGRAPH 2020)\\n@XuanLuo14 \\nProject\\n https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\\xa0…\\nPaper\\n https://arxiv.org/abs/2004.15021\\xa0',\n",
       "   '単眼デプス推定モデルに対して，SfMで離れた画像ペアを選択し，MVSとOptical Flowの結果から奥行と画像座標の距離を損失としてfine-tuneすることで，動画に対し一貫性のある推定を実現．学習の前処理でMVSの結果からスケールを調整．動物体による誤差の影響でSOTAに近い精度． https://youtu.be/5Tia2oblJAg\\xa0'],\n",
       "  'urls': ['https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/',\n",
       "   'https://arxiv.org/abs/2004.15021',\n",
       "   'https://youtu.be/5Tia2oblJAg']},\n",
       " {'conversation_id': '1256416173610041344',\n",
       "  'text': ['Footprints and Free Space from a Single Color Image (CVPR2020)\\nCode:  https://github.com/nianticlabs/footprints\\xa0…\\nPaper:  https://arxiv.org/abs/2004.06376\\xa0',\n",
       "   '単眼画像からの自由空間の推定．多視点のステレオ画像から得られた対象環境の幾何情報を単眼画像に集約し学習することで，単一視点からでは不可視な領域に対してもTraversabilityやDepthの評価が可能に．\\n https://arxiv.org/abs/2004.06376\\xa0 pic.twitter.com/7xkhPWGGu8'],\n",
       "  'urls': ['https://github.com/nianticlabs/footprints',\n",
       "   'https://arxiv.org/abs/2004.06376',\n",
       "   'https://arxiv.org/abs/2004.06376']},\n",
       " {'conversation_id': '1256059179724271616',\n",
       "  'text': ['Video: https://youtu.be/wuokg7MFZyU\\xa0',\n",
       "   '関連研究2（RangeNet++）\\nRangeNet++: Fast and Accurate LiDAR Semantic Segmentation (IROS 2019)\\nCode:  https://github.com/PRBonn/lidar-bonnetal\\xa0…\\nPaper:  https://ieeexplore.ieee.org/document/8967762\\xa0…',\n",
       "   'Video: https://youtu.be/-AEX203rXkE\\xa0',\n",
       "   '関連研究1（SuMa）\\nEfficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments (RSS 2018)\\nProject:  http://jbehley.github.io/projects/surfel_mapping/\\xa0…\\nCode:  https://github.com/jbehley/SuMa\\xa0\\nPaper:  http://www.roboticsproceedings.org/rss14/p16.html\\xa0',\n",
       "   'Video: https://youtu.be/uo3ZuLuFAzk\\xa0',\n",
       "   'SuMa++: Efficient LiDAR-based Semantic SLAM (IROS 2019)\\nCode:  https://github.com/PRBonn/semantic_suma\\xa0…\\nPaper:  https://ieeexplore.ieee.org/document/8967704\\xa0…',\n",
       "   '地図を surfel で表現するグラフベース SLAM 手法の SuMa を拡張。3D LIDAR 点群を距離画像に変換し、FCN でセマンティックセグメンテーション。セマンティクスの整合性を重みとする。静止している車は位置合わせに利用される。移動している車が多い KITTI dataset の高速道路でも高精度な推定を実現。 pic.twitter.com/M286GNv7WB'],\n",
       "  'urls': ['https://youtu.be/wuokg7MFZyU',\n",
       "   'https://github.com/PRBonn/lidar-bonnetal',\n",
       "   'https://ieeexplore.ieee.org/document/8967762',\n",
       "   'https://youtu.be/-AEX203rXkE',\n",
       "   'http://jbehley.github.io/projects/surfel_mapping/',\n",
       "   'https://github.com/jbehley/SuMa',\n",
       "   'http://www.roboticsproceedings.org/rss14/p16.html',\n",
       "   'https://youtu.be/uo3ZuLuFAzk',\n",
       "   'https://github.com/PRBonn/semantic_suma',\n",
       "   'https://ieeexplore.ieee.org/document/8967704']},\n",
       " {'conversation_id': '1255704202178752513',\n",
       "  'text': ['Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction',\n",
       "   'Kinect Fusionに利用されているTSDFを学習器に置き換えたDeepSDFを局所適用し，詳細な形状表現を可能にした．DeepSDFは全体を関数近似するのに対し，提案手法はVoxel単位で関数近似．DeepSDFが8日かかった形状復元が，提案手法では1分と大幅に短縮．\\n https://arxiv.org/pdf/2003.10983.pdf\\xa0… pic.twitter.com/Y89OJ9FDXh'],\n",
       "  'urls': ['https://arxiv.org/pdf/2003.10983.pdf']},\n",
       " {'conversation_id': '1255331095656239105',\n",
       "  'text': ['第一著者自身の引用論文\\n\\nHigh-dimensional Convolutional Networks for Geometric Pattern Recognition (CVPR2020)\\nPaper:\\n http://vladlen.info/papers/HDConvNets.pdf\\xa0… …\\n\\nFully Convolutional Geometric Features (ICCV2019)\\nPaper:\\n https://node1.chrischoy.org/data/publications/fcgf/fcgf.pdf\\xa0… …\\nGithub: https://github.com/chrischoy/FCGF\\xa0',\n",
       "   'Deep Global Registration (CVPR2020 Oral)\\nProject https://chrischoy.github.io/publication/dgr/\\xa0…',\n",
       "   '点群位置合わせの微分可能なフレームワークを提案．6D ConvNetで推定した対応点のInlier確率を重みとし，その重みで微分した勾配をガイドとするProcrustes法により密な対応点を利用した高精度な位置合せが可能．finetuneを追加しEnd-to-endで学習．精度，頑健性，速度でSOTA．\\n https://arxiv.org/abs/2004.11540\\xa0 pic.twitter.com/33W0tiDzCe'],\n",
       "  'urls': ['http://vladlen.info/papers/HDConvNets.pdf',\n",
       "   'https://node1.chrischoy.org/data/publications/fcgf/fcgf.pdf',\n",
       "   'https://github.com/chrischoy/FCGF',\n",
       "   'https://chrischoy.github.io/publication/dgr/',\n",
       "   'https://arxiv.org/abs/2004.11540']},\n",
       " {'conversation_id': '1254968580136820736',\n",
       "  'text': ['SynSin: End-to-end View Synthesis from a Single Image (CVPR2020 Oral)\\nProject\\n http://www.robots.ox.ac.uk/~ow/synsin.html\\xa0\\nGithub https://github.com/facebookresearch/synsin\\xa0…',\n",
       "   '任意解像度の単一画像から任意のビューを合成するend-to-endなネットワークの提案．推論したFeatureとDepthを用い点群を構築，微分可能な点群レンダラーとリファインメントネットワークを通すことで欠損のないビューを合成．\\n https://arxiv.org/abs/1912.08804\\xa0 pic.twitter.com/cp2TcgoURr'],\n",
       "  'urls': ['http://www.robots.ox.ac.uk/~ow/synsin.html',\n",
       "   'https://github.com/facebookresearch/synsin',\n",
       "   'https://arxiv.org/abs/1912.08804']},\n",
       " {'conversation_id': '1254608188801875968',\n",
       "  'text': ['Online LiDAR-SLAM for Legged Robots with Deep-Learned Loop Closure (ICRA2020)\\nProject\\n https://ori.ox.ac.uk/lidar-slam/\\xa0\\nPaper\\n https://arxiv.org/abs/2001.10249\\xa0',\n",
       "   '深層学習を用いた特徴量ベースのループ検出器を組み込んだグラフLiDAR-SLAMシステムを提案．四脚ロボットでも動作するように浅いネットワークを用いておりCPUで推論可能．kd-treeを用いた点群繋ぎ合わせの高速な検証方法を提案．屋内外の産業環境でロバスト性を実証． https://youtu.be/djf7vGtf7CA\\xa0'],\n",
       "  'urls': ['https://ori.ox.ac.uk/lidar-slam/',\n",
       "   'https://arxiv.org/abs/2001.10249',\n",
       "   'https://youtu.be/djf7vGtf7CA']},\n",
       " {'conversation_id': '1254607258375213056',\n",
       "  'text': ['SLAM-Hubに千葉工大 fuRoの原先生 @ystk_hara が加入されました．千葉周辺にお住まいのSLAM-Hubにご興味のある方は原先生へ気軽にご相談ください．#slamhub'],\n",
       "  'urls': []},\n",
       " {'conversation_id': '1254246604090961920',\n",
       "  'text': ['YOLOv4: Optimal Speed and Accuracy of Object Detection\\nGithub https://github.com/AlexeyAB/darknet\\xa0…',\n",
       "   'CNNによる高速な物体検出器YOLOの最新版YOLOv4を提案．検出器の学習における，Bag of freebiesやBag of specialsによる効果を検証．バッチ正規化や残差スキップ接続など，モデルやデータセットに関して普遍的で効果のよい手法を用いることで精度を向上させた．\\n https://arxiv.org/abs/2004.10934\\xa0 pic.twitter.com/G3cdiZTGMZ'],\n",
       "  'urls': ['https://github.com/AlexeyAB/darknet',\n",
       "   'https://arxiv.org/abs/2004.10934']},\n",
       " {'conversation_id': '1253881416191897600',\n",
       "  'text': ['Least Squares Optimization: from Theory to Practice\\nGithub https://github.com/srrg-sapienza/srrg2_solver\\xa0…',\n",
       "   '反復法による最小自乗を解く新たな最適化システムを提案．既存の問題を統一的に解けるようにソルバを設計することで，疎/密，動的/静的な要素にシームレスに対応した．様々な観点で比較評価を行い，提案手法が既存システムに対し同等以上の速度，精度性能を達成した．\\n https://arxiv.org/abs/2002.11051\\xa0 pic.twitter.com/FUqWsYd55A'],\n",
       "  'urls': ['https://github.com/srrg-sapienza/srrg2_solver',\n",
       "   'https://arxiv.org/abs/2002.11051']},\n",
       " {'conversation_id': '1253519029291094020',\n",
       "  'text': ['グラフ描画のアルゴリズムを用いて3D点群を2D画像に投影する手法を提案。投影した画像にU-Netを適用し、3D点群のセグメンテーションでSOTAを達成。階層的クラスタリングで得られた部分点群ごとに投影することで計算コストを削減。\\n http://arxiv.org/abs/2003.05593\\xa0 pic.twitter.com/4Mt0buWYu9'],\n",
       "  'urls': ['http://arxiv.org/abs/2003.05593']},\n",
       " {'conversation_id': '1253182486252670978',\n",
       "  'text': ['Unsupervised Geometry-Aware Deep LiDAR Odometry (ICRA2020)\\nproject: https://sites.google.com/view/deeplo\\xa0',\n",
       "   '2つのLIDARスキャン間の相対姿勢推定をUnsupervisedに学習．全周画像上で特徴量抽出や最終的なICP誤差のロスを定義することで対応付けを避けて学習に適した枠組みとしている．\\nvideo: https://www.youtube.com/watch?v=-imRJXq6ZuE\\xa0… pic.twitter.com/WV2OHJKXzA'],\n",
       "  'urls': ['https://sites.google.com/view/deeplo',\n",
       "   'https://www.youtube.com/watch?v=-imRJXq6ZuE']},\n",
       " {'conversation_id': '1252794252284891136',\n",
       "  'text': ['SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans (CVPR2020)\\nPaper: \\n https://arxiv.org/abs/1912.00036\\xa0\\nProject:  https://www.3dunderstanding.org/papers/2020/dai2020sgnn/\\xa0… pic.twitter.com/uPOIFDJRom',\n",
       "   'ターゲット点群の一部スキャンを削除、更に不完全な点群を入力として、自己教師ありで点群補完を学習させるSG-NNを提案。点群はスパースTSDFで表現、encoder-decoderを使って、見たこともない三次元形状にデコードでき、ターゲット点群より高い分解能点群を補完することが可能 https://youtu.be/rN6D3QmMNuU\\xa0'],\n",
       "  'urls': ['https://arxiv.org/abs/1912.00036',\n",
       "   'https://www.3dunderstanding.org/papers/2020/dai2020sgnn/',\n",
       "   'https://youtu.be/rN6D3QmMNuU']},\n",
       " {'conversation_id': '1252430913985642496',\n",
       "  'text': ['Efficient Exploration in Constrained Environments with Goal-Oriented Reference Path (arXiv)\\n@ohtake_i @kanejaki\\nProject: https://keiohta.github.io/publications/2020-03-01_gai_navigation\\xa0…',\n",
       "   '2D地図を入力とした目的地への経路・行動生成手法の提案．GOSELOと呼ばれる地図表現を介しCNNによるWaypointを生成(教師あり学習)，さらにWaypointに沿うような操作量を生成する層(強化学習)を後段に追加することで安全なナビゲーションを実現している．\\n https://arxiv.org/abs/2003.01641\\xa0 pic.twitter.com/yskq5iMDHZ'],\n",
       "  'urls': ['https://keiohta.github.io/publications/2020-03-01_gai_navigation',\n",
       "   'https://arxiv.org/abs/2003.01641']},\n",
       " {'conversation_id': '1252071153817903108',\n",
       "  'text': ['PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points (NeurIPS2019)',\n",
       "   'RGB画像1枚から物体の3D Bounding Box(3DBB)と6DoF姿勢推定．3DBB投影時のコーナー点位置をテンプレートの重み付け和で表現し，その重みを推定する枠組み．同時に3DBBの3次元位置姿勢を推定し投影点上でのLossを定義し学習する．\\n https://arxiv.org/abs/1912.07744\\xa0 pic.twitter.com/lVKH8vErzB'],\n",
       "  'urls': ['https://arxiv.org/abs/1912.07744']},\n",
       " {'conversation_id': '1251707090277548033',\n",
       "  'text': ['NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction\\n@SucarEdgar @wkentaro_ @AjdDavison\\nProject\\n https://edgarsucar.github.io/NodeSLAM/\\xa0\\nPaper\\n https://arxiv.org/abs/2004.04485\\xa0',\n",
       "   'VAEを用いて学習した各物体カテゴリの3D形状特徴量を，トラッキングしながらカメラ姿勢と同時に最適化することで，単一あるいは複数視点のRGB-D画像から，オクルージョンがあっても欠損がない物体の高精度な3D形状を復元する手法を提案． https://www.youtube.com/watch?time_continue=279&v=zPzMtXU-0JE&feature=emb_logo\\xa0…'],\n",
       "  'urls': ['https://edgarsucar.github.io/NodeSLAM/',\n",
       "   'https://arxiv.org/abs/2004.04485',\n",
       "   'https://www.youtube.com/watch?time_continue=279&v=zPzMtXU-0JE&feature=emb_logo']},\n",
       " {'conversation_id': '1251344702227353600',\n",
       "  'text': ['Redesigning SLAM for Arbitrary Multi-Camera Systems (ICRA2020)\\n https://arxiv.org/abs/2003.02014\\xa0 pic.twitter.com/eMBMAKlj8e',\n",
       "   '既存のVisual SLAM手法を任意の複数カメラシステムに拡張する手法を提案．適応的な初期化，センサに依存しないキーフレーム選択，voxelベースのマップ管理法を用いることで，精度を保ちセンサ固有の改良なしでの動作を実現. https://www.youtube.com/watch?v=JGL4H93BiNw\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.02014',\n",
       "   'https://www.youtube.com/watch?v=JGL4H93BiNw']},\n",
       " {'conversation_id': '1250976017079922688',\n",
       "  'text': ['ワイドベースラインカメラで撮影した6枚の画像から高品質な幾何形状とSVBRDFを復元する学習ベースの手法を提案．各画像ごとに拡散・鏡面アルベド，法線，鏡面粗さをネットワークで推定し，推定結果を融合して幾何形状とSVBRDF得る．従来難しかった疎な画像からの復元に成功．\\n https://arxiv.org/abs/2003.12642\\xa0 pic.twitter.com/Qn5OoQkR0s'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.12642']},\n",
       " {'conversation_id': '1250589729210449920',\n",
       "  'text': ['ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings (CVPR2020)\\n https://arxiv.org/abs/2003.12980\\xa0 pic.twitter.com/TD0uMjecFV',\n",
       "   'ステレオカメラを用いて，物体上の特徴点のクラスター化と，自身と物体の動きの推定を同時に行うシステムを提案．クラスター化は，物体検出による特徴点のクラスラベルを用いて，3次元位置も考慮したCRFにより実装．シーンに依存せず，オンラインでの動作を可能にした． https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2003.12980',\n",
       "   'https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be']},\n",
       " {'conversation_id': '1250227339776217088',\n",
       "  'text': ['DeepFactors: Real-Time Probabilistic Dense Monocular SLAM (RA-L)\\n https://arxiv.org/abs/2001.05049\\xa0 pic.twitter.com/raqAYPgSje',\n",
       "   'CodeSLAMをベースにした新たな深層学習ベースのVisual SLAMシステムを提案．既存のコードによるコンパクトなデプスマップ表現に加え，損失関数の改善，ループクロージングと全体最適化の追加により，精度とロバストを向上．さらにリアルタイム動作を実現した． https://www.youtube.com/watch?v=htnRuGKZmZw\\xa0…'],\n",
       "  'urls': ['https://arxiv.org/abs/2001.05049',\n",
       "   'https://www.youtube.com/watch?v=htnRuGKZmZw']},\n",
       " {'conversation_id': '1249864951092506624',\n",
       "  'text': ['Github: https://github.com/QingyongHu/RandLA-Net\\xa0…',\n",
       "   'RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds (CVPR2020)\\nVideo: https://www.youtube.com/watch?v=Ar3eY_lwzMk&feature=youtu.be\\xa0…',\n",
       "   '点数が百万を超える大規模三次元点群のセグメンテーションにおいては、従来の高コストな点群サンプリング手法よりもランダムサンプリングが有効であることを示した．KNNとアテンションを用いて積極的に受容野を拡大することで、サンプリングによる点群の欠損に対処．\\n https://arxiv.org/abs/1911.11236\\xa0 pic.twitter.com/xjpFO7WABl'],\n",
       "  'urls': ['https://github.com/QingyongHu/RandLA-Net',\n",
       "   'https://www.youtube.com/watch?v=Ar3eY_lwzMk&feature=youtu.be',\n",
       "   'https://arxiv.org/abs/1911.11236']},\n",
       " {'conversation_id': '1249517662847283201',\n",
       "  'text': ['R-LINS: A Robocentric Lidar-Inertial State Estimator for Robust and Efficient Navigation (ICRA2020)\\nvideo: https://www.youtube.com/watch?v=Nmr1blC09qw\\xa0…',\n",
       "   'LIDAR-IMUのオドメトリ推定手法．Error State Kalman Filter上でTight-couplingに最適化を行うことで，従来のグラフベース手法と近い精度を維持しつつ大幅に処理速度を向上させた．\\npaper: https://arxiv.org/abs/1907.02233\\xa0 pic.twitter.com/IQ4YAzhL8q'],\n",
       "  'urls': ['https://www.youtube.com/watch?v=Nmr1blC09qw',\n",
       "   'https://arxiv.org/abs/1907.02233']},\n",
       " {'conversation_id': '1249155400122994688',\n",
       "  'text': ['Object Levelより細かく，新たな形状表現Local Implicit Grid (LIG)を提案．AutoencoderでPartの形状をlatent vectorにエンコード，入力点群と復元ロス最小なlatent vectorを最適化，LIGでの内挿により形状にデコード、機械学習で点群からScene Levelの三次元形状復元が可能． https://youtu.be/XCyl1-vxfII\\xa0'],\n",
       "  'urls': ['https://youtu.be/XCyl1-vxfII']},\n",
       " {'conversation_id': '1248796223961624576',\n",
       "  'text': ['EPOS: Estimating 6D Pose of Objects with Symmetries (CVPR2020)\\nProject:  http://cmp.felk.cvut.cz/epos/\\xa0',\n",
       "   'RGB画像における物体の6DoF姿勢推定．Surface fragmentによる3次元モデル表現を介し，物体のPoseを各PixelがどのようなインスタンスやSurface fragmentに対応しうるかを学習．得られた多対多な2D-3D対応をPnP-RANSACによりロバスト化．\\n https://arxiv.org/pdf/2004.00605.pdf\\xa0… pic.twitter.com/Qn2Etv1Nst'],\n",
       "  'urls': ['http://cmp.felk.cvut.cz/epos/',\n",
       "   'https://arxiv.org/pdf/2004.00605.pdf']},\n",
       " {'conversation_id': '1248436573865041920',\n",
       "  'text': ['Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching [ICRA2020]\\n\\nVideo:  https://www.ais.uni-bonn.de/videos/ICRA_2020_Gradient_Dissimilarity/\\xa0…',\n",
       "   '画像の位置合わせに必要なエラーについて，輝度差に代わる新たなメトリックSGFを提案．SGFは勾配画像のコントラストを局所的に正規化し，勾配方向の内積を利用．DSOにSGFを適用したところ精度が改善．\\n https://arxiv.org/pdf/2004.04090.pdf\\xa0… pic.twitter.com/HL3hkOLo61'],\n",
       "  'urls': ['https://www.ais.uni-bonn.de/videos/ICRA_2020_Gradient_Dissimilarity/',\n",
       "   'https://arxiv.org/pdf/2004.04090.pdf']},\n",
       " {'conversation_id': '1248070308080177152',\n",
       "  'text': ['Spatial Hashing (メモリ効率の良いボクセル表現)に関する引用文献\\nReal-time 3D Reconstruction at Scale using Voxel Hashing\\n https://niessnerlab.org/papers/2013/4hashing/niessner2013hashing.pdf\\xa0…\\nOptimized Spatial Hashing for Collision Detection of Deformable Objects\\n https://matthias-research.github.io/pages/publications/tetraederCollision.pdf\\xa0…',\n",
       "   'Visual SLAMで用いられる従来のキーフレーム表現（covisibility graph）では，3D点の遮蔽関係を記述できないため，ボクセルハッシングと視錐台表現を用いたレイキャスティングにより，高速かつ省メモリに幾何的関係性の記述を可能とした．\\n https://arxiv.org/abs/2003.02247\\xa0 pic.twitter.com/MMeABTUKVP'],\n",
       "  'urls': ['https://niessnerlab.org/papers/2013/4hashing/niessner2013hashing.pdf',\n",
       "   'https://matthias-research.github.io/pages/publications/tetraederCollision.pdf',\n",
       "   'https://arxiv.org/abs/2003.02247']},\n",
       " {'conversation_id': '1247356052657455105',\n",
       "  'text': ['R&Dコミュニティ「SLAM-Hub」を立ち上げました！\\n一緒に研究開発してくれるメンバーを募集しています． https://slamhub.xslam.org/?0\\xa0'],\n",
       "  'urls': ['https://slamhub.xslam.org/?0']}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ossyaritoori ご連絡ありがとうございます．とても面白い記事ですね．転載されているのは僅かでしたしご連絡頂いたので問題ございません．今後も更新していくので引き続きご覧頂けますと幸いです．']\n",
      "[]\n",
      "['SLAM-Hub members got one paper accepted by #ECCV2020 !']\n",
      "[]\n",
      "['コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！']\n",
      "[]\n",
      "['複数視点の画像を利用しているのは学習時のみで，推定は常に単眼です．（各視点の画像を独立に推定しているのに，動画として整合性が取れているのがポイントです）']\n",
      "[]\n",
      "['三次元点群のOne-Stage物体検出Hybrid Voxel Network (HVNet)手法を提案。Pointwiseでのmulti-scale特徴をHybrid Voxel Feature Extraction(HVFE)で抽出、Voxelwise attention featureにエンコード、Pseudo-Image Featureへデカップル、リアルタイムの31HzでSOTAを達成。\\n https://arxiv.org/abs/2003.00186\\xa0 pic.twitter.com/0yyB3iaOkU']\n",
      "['https://arxiv.org/abs/2003.00186']\n",
      "['SLAM-Hubに千葉工大 fuRoの原先生 @ystk_hara が加入されました．千葉周辺にお住まいのSLAM-Hubにご興味のある方は原先生へ気軽にご相談ください．#slamhub']\n",
      "[]\n",
      "['グラフ描画のアルゴリズムを用いて3D点群を2D画像に投影する手法を提案。投影した画像にU-Netを適用し、3D点群のセグメンテーションでSOTAを達成。階層的クラスタリングで得られた部分点群ごとに投影することで計算コストを削減。\\n http://arxiv.org/abs/2003.05593\\xa0 pic.twitter.com/4Mt0buWYu9']\n",
      "['http://arxiv.org/abs/2003.05593']\n",
      "['ワイドベースラインカメラで撮影した6枚の画像から高品質な幾何形状とSVBRDFを復元する学習ベースの手法を提案．各画像ごとに拡散・鏡面アルベド，法線，鏡面粗さをネットワークで推定し，推定結果を融合して幾何形状とSVBRDF得る．従来難しかった疎な画像からの復元に成功．\\n https://arxiv.org/abs/2003.12642\\xa0 pic.twitter.com/Qn5OoQkR0s']\n",
      "['https://arxiv.org/abs/2003.12642']\n",
      "['Object Levelより細かく，新たな形状表現Local Implicit Grid (LIG)を提案．AutoencoderでPartの形状をlatent vectorにエンコード，入力点群と復元ロス最小なlatent vectorを最適化，LIGでの内挿により形状にデコード、機械学習で点群からScene Levelの三次元形状復元が可能． https://youtu.be/XCyl1-vxfII\\xa0']\n",
      "['https://youtu.be/XCyl1-vxfII']\n",
      "['R&Dコミュニティ「SLAM-Hub」を立ち上げました！\\n一緒に研究開発してくれるメンバーを募集しています． https://slamhub.xslam.org/?0\\xa0']\n",
      "['https://slamhub.xslam.org/?0']\n"
     ]
    }
   ],
   "source": [
    "# あとからappendしたほうのツイート→新しい方のツイートが要約の可能性が高い。\n",
    "for tweet in tweets:\n",
    "    if len(tweet[\"text\"]) < 2:\n",
    "        print(tweet[\"text\"])\n",
    "        print(tweet[\"urls\"])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 of 124\n",
      "118 of 124\n",
      "119 of 124\n",
      "120 of 124\n",
      "121 of 124\n",
      "122 of 124\n",
      "123 of 124\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import arxiv\n",
    "\n",
    "docdics = []\n",
    "counts = 0\n",
    "\n",
    "for dics in tweets:\n",
    "# 途中からやったときの名残\n",
    "# for i in range(117,len(tweets)):\n",
    "#dics = tweets[i]\n",
    "    print( counts,\"of\",len(tweets),flush=True)\n",
    "    counts += 1\n",
    "    \n",
    "    ddic = {}\n",
    "    title = \"\"\n",
    "    youtubetitle = \"\"\n",
    "    arxivtitle = \"\"\n",
    "    imgurl = \"\"\n",
    "    \n",
    "    # url が空の場合はスキップ\n",
    "    if not dics[\"urls\"]:\n",
    "        continue\n",
    "        \n",
    "    # arxivのリンク情報を埋める\n",
    "    for url in dics[\"urls\"]:\n",
    "        # Youtubeだった場合\n",
    "        if 'youtu' in url: # if it is youtube link\n",
    "            imgurl = subprocess.check_output(\"youtube-dl --get-thumbnail \\\"\"+url +\"\\\"\", shell=True)\n",
    "            youtubetitle = subprocess.check_output(\"youtube-dl --get-title \\\"\"+ url +\"\\\"\", shell=True)\n",
    "            # solve decode problem\n",
    "            if type(imgurl) == bytes:\n",
    "                imgurl = imgurl.decode(\"utf-8\", \"ignore\")\n",
    "            if type(youtubetitle) == bytes:\n",
    "                youtubetitle = youtubetitle.decode(\"utf-8\", \"ignore\")\n",
    "                \n",
    "        # Arxivだった場合\n",
    "        if 'arxiv' in url:\n",
    "            arxivid_ = url.split('/')[-1]\n",
    "            ids = [s for s in arxivid_.split('.') if s[0].isdigit()]\n",
    "\n",
    "            arxivid__=\"\"\n",
    "            for i in range(len(ids)):\n",
    "                arxivid__ += ids[i]+\".\"\n",
    "            arxivid = arxivid__[:-1]\n",
    "            arxivtitle = arxiv.query(id_list=[arxivid])[0]['title']\n",
    "    \n",
    "    # arxivのタイトルがあればそちらを優先\n",
    "    if arxivtitle:\n",
    "        title = arxivtitle\n",
    "    else:\n",
    "        if not youtubetitle:\n",
    "            # youtube のもなければスキップ\n",
    "            continue\n",
    "        else:\n",
    "            title = youtubetitle\n",
    "    \n",
    "    ddic['id'] = dics['conversation_id']\n",
    "    ddic['title'] = title\n",
    "    ddic['imgurl'] = imgurl\n",
    "    ddic['text'] = dics['text']\n",
    "    docdics.append(ddic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Video Demo for RandLA-Net yCVPR'20z\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# どうやらエンコードShift-JISっぽいが面倒なので無視\n",
    "youtubetitle.decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-08-01'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newestdate = sdic[0][\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marp書き込み用のプリアンブル\n",
    "createddate = sdic[0][\"date\"]\n",
    "\n",
    "preamble = \"---\\nmarp: true\\n header: 'from @slam_hub'\\n footer: '@ossyaritoori'\\n size: 16:9\\n paginate: true\\n ---\\n \\n # SLAM_HUB Twitter 情報まとめ \\n\" + \"Created on\" + createddate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdocfile = open(\"survey_twint.md\",\"w+\",encoding='utf-8')\n",
    "\n",
    "#Write Preamble\n",
    "mkdocfile.write(preamble)\n",
    "\n",
    "for mkdics in docdics:\n",
    "    imgurl=mkdics[\"imgurl\"]\n",
    "    # URLが改行で終わっている場合これを解除\n",
    "    if imgurl:\n",
    "        if imgurl[-1] == \"\\n\":\n",
    "            imgurl = imgurl[:-1]\n",
    "    # 要約となるtextはだいたい最初のツイート\n",
    "    abstract = mkdics[\"text\"][-1]\n",
    "    onepagestring = \"\\n---\\n\"+\"## \" + mkdics[\"title\"]\\\n",
    "    + \"\\n\" + abstract\\\n",
    "    + \"\\n ![bg right:40% fit](\"+imgurl+\")\\n\"\n",
    "    mkdocfile.write(onepagestring)\n",
    "    \n",
    "mkdocfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

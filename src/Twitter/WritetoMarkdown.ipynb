{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"slamhub.txt\", 'r')\n",
    "sdic= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 186,\n",
       "  'links': ['https://slamhub.xslam.org/?0'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 66,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'R&Dコミュニティ「SLAM-Hub」を立ち上げました！\\n一緒に研究開発してくれるメンバーを募集しています．https://slamhub.xslam.org/?0\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">R&amp;Dコミュニティ「SLAM-Hub」を立ち上げました！\\n一緒に研究開発してくれるメンバーを募集しています．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://slamhub.xslam.org/?0\" dir=\"ltr\" href=\"https://t.co/UYLpjPAwn0\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://slamhub.xslam.org/?0\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">slamhub.xslam.org/?0</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-04-07T02:50:32',\n",
       "  'timestamp_epochs': 1586227832,\n",
       "  'tweet_id': '1247356052657455105',\n",
       "  'tweet_url': '/slam_hub/status/1247356052657455105',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Edva-6wU8AUplEB.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 19,\n",
       "  'links': ['https://arxiv.org/abs/2003.12735'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '多視点物体認識のための特徴量抽出を自己教師あり学習する手法を提案．代理タスクとして，オブジェクトクラス分類を通し距離学習を行う．これにより視点に因らず同一オブジェクトならば埋め込み表現上でクラスターを形成．ダウンストリームタスクで他手法より高い精度を達成．\\nhttps://arxiv.org/abs/2003.12735\\xa0pic.twitter.com/YyvMehYHKd',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">多視点物体認識のための特徴量抽出を自己教師あり学習する手法を提案．代理タスクとして，オブジェクトクラス分類を通し距離学習を行う．これにより視点に因らず同一オブジェクトならば埋め込み表現上でクラスターを形成．ダウンストリームタスクで他手法より高い精度を達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.12735\" dir=\"ltr\" href=\"https://t.co/CTIHihYYkW\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.12735\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.12735</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/YyvMehYHKd\">pic.twitter.com/YyvMehYHKd</a></p>',\n",
       "  'timestamp': '2020-07-25T03:16:40',\n",
       "  'timestamp_epochs': 1595647000,\n",
       "  'tweet_id': '1286862906143854593',\n",
       "  'tweet_url': '/slam_hub/status/1286862906143854593',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 22,\n",
       "  'links': ['https://youtu.be/AYjgeaQR8uQ'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'ステレオカメラの情報から動的な障害を検出，追跡するシステムの提案．ロボット周囲の物体について動的・静的の2クラスに分類．さらに動的な物体については人とそれ以外のクラスに分類する．ノイズの多いデータから高い精度の動的物体の検出,追跡が可能なことを実験で確認．https://youtu.be/AYjgeaQR8uQ\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ステレオカメラの情報から動的な障害を検出，追跡するシステムの提案．ロボット周囲の物体について動的・静的の2クラスに分類．さらに動的な物体については人とそれ以外のクラスに分類する．ノイズの多いデータから高い精度の動的物体の検出,追跡が可能なことを実験で確認．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/AYjgeaQR8uQ\" dir=\"ltr\" href=\"https://t.co/kna4XiPhiJ\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/AYjgeaQR8uQ\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/AYjgeaQR8uQ</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-24T04:34:05',\n",
       "  'timestamp_epochs': 1595565245,\n",
       "  'tweet_id': '1286520000627372032',\n",
       "  'tweet_url': '/slam_hub/status/1286520000627372032',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': True,\n",
       "  'likes': 14,\n",
       "  'links': ['https://arxiv.org/abs/1910.14139'],\n",
       "  'parent_tweet_id': '1286133936716787713',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [{'screen_name': 'slam_hub',\n",
       "    'user_id': '1244129482132209664'}],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'AjdDavison',\n",
       "  'text': 'ありがとうございます。 FutureMapping 2 もご覧ください！https://arxiv.org/abs/1910.14139\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ありがとうございます。 FutureMapping 2 もご覧ください！<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1910.14139\" dir=\"ltr\" href=\"https://t.co/m29kPkdv9u\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1910.14139\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1910.14139</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-23T08:42:11',\n",
       "  'timestamp_epochs': 1595493731,\n",
       "  'tweet_id': '1286220052547723264',\n",
       "  'tweet_url': '/AjdDavison/status/1286220052547723264',\n",
       "  'user_id': '1446792746',\n",
       "  'username': 'Andrew Davison',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': ['SpatialAI', 'SpatialAI'],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EdhgCeRU0AYBeua.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 122,\n",
       "  'links': ['https://arxiv.org/abs/1803.11288'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 2,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 37,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'SLAM （自己位置推定と地図構築）を発展させ，シーンやオブジェクトの関係性を理解する #SpatialAI の開発が進められている ．#SpatialAI を実際のアプリケーションに応用する上で必要なアルゴリズムやプロセッサ，センサの連携などについて提唱された最初の論文．\\nhttps://arxiv.org/abs/1803.11288\\xa0pic.twitter.com/bm9sSOj5qN',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">SLAM （自己位置推定と地図構築）を発展させ，シーンやオブジェクトの関係性を理解する <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/SpatialAI?src=hash\"><s>#</s><b>SpatialAI</b></a> の開発が進められている ．<a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/SpatialAI?src=hash\"><s>#</s><b>SpatialAI</b></a> を実際のアプリケーションに応用する上で必要なアルゴリズムやプロセッサ，センサの連携などについて提唱された最初の論文．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1803.11288\" dir=\"ltr\" href=\"https://t.co/AeXUjMQAzB\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1803.11288\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1803.11288</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/bm9sSOj5qN\">pic.twitter.com/bm9sSOj5qN</a></p>',\n",
       "  'timestamp': '2020-07-23T03:00:00',\n",
       "  'timestamp_epochs': 1595473200,\n",
       "  'tweet_id': '1286133936716787713',\n",
       "  'tweet_url': '/slam_hub/status/1286133936716787713',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 25,\n",
       "  'links': ['https://www.youtube.com/watch?v=b62iDkLgGSI'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '自己教師あり学習で単眼画像のデプスを推定するPackNetを提案．ピクセルをチャンネル方向に並び替えるSpace2Depthを含むPackNetにより，重要な空間情報を保持した明瞭なデプスが推定可能．既存の教師あり学習と同程度の精度を達成．https://www.youtube.com/watch?v=b62iDkLgGSI\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">自己教師あり学習で単眼画像のデプスを推定するPackNetを提案．ピクセルをチャンネル方向に並び替えるSpace2Depthを含むPackNetにより，重要な空間情報を保持した明瞭なデプスが推定可能．既存の教師あり学習と同程度の精度を達成．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=b62iDkLgGSI\" dir=\"ltr\" href=\"https://t.co/hhI96mXwQI\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=b62iDkLgGSI\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=b62iDk</span><span class=\"invisible\">LgGSI</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-07-22T03:00:00',\n",
       "  'timestamp_epochs': 1595386800,\n",
       "  'tweet_id': '1285771549090308096',\n",
       "  'tweet_url': '/slam_hub/status/1285771549090308096',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EdYEOW7VAAAj2sM.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 28,\n",
       "  'links': ['https://arxiv.org/abs/2007.06888'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '3次元点群に対してセグメンテーションとクラス境界の推定を同時に行う手法を提案．相互に関連する両タスクを同時に解くだけでなく，双方の推定結果を用いてさらに精緻化を行うNNモジュールを提案．屋内データ（S3DIS）でSOTAを達成．\\nhttps://arxiv.org/abs/2007.06888\\xa0pic.twitter.com/O52VDKG07v',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">3次元点群に対してセグメンテーションとクラス境界の推定を同時に行う手法を提案．相互に関連する両タスクを同時に解くだけでなく，双方の推定結果を用いてさらに精緻化を行うNNモジュールを提案．屋内データ（S3DIS）でSOTAを達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2007.06888\" dir=\"ltr\" href=\"https://t.co/op2eqjLf6E\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2007.06888\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2007.06888</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/O52VDKG07v\">pic.twitter.com/O52VDKG07v</a></p>',\n",
       "  'timestamp': '2020-07-21T03:00:00',\n",
       "  'timestamp_epochs': 1595300400,\n",
       "  'tweet_id': '1285409161451380736',\n",
       "  'tweet_url': '/slam_hub/status/1285409161451380736',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EdVp-HCUYAIGpyK.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 54,\n",
       "  'links': ['https://arxiv.org/abs/1908.01863'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '2D SLAMにおける地図表現にSDFを導入し，計測点が存在しない空間の情報を使った局所特徴(free-space features)を提案．曲率ベースの特徴点検出と方向付き勾配ヒストグラムを使った記述子を使い，従来手法より大域位置認識が高精度に行えることを示した．\\nhttps://arxiv.org/abs/1908.01863\\xa0pic.twitter.com/GJSmADnz12',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">2D SLAMにおける地図表現にSDFを導入し，計測点が存在しない空間の情報を使った局所特徴(free-space features)を提案．曲率ベースの特徴点検出と方向付き勾配ヒストグラムを使った記述子を使い，従来手法より大域位置認識が高精度に行えることを示した．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1908.01863\" dir=\"ltr\" href=\"https://t.co/eu5Rh4GKJ2\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1908.01863\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1908.01863</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/GJSmADnz12\">pic.twitter.com/GJSmADnz12</a></p>',\n",
       "  'timestamp': '2020-07-20T03:12:05',\n",
       "  'timestamp_epochs': 1595214725,\n",
       "  'tweet_id': '1285049816234422272',\n",
       "  'tweet_url': '/slam_hub/status/1285049816234422272',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EdVp-HCUYAIGpyK.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 54,\n",
       "  'links': ['https://arxiv.org/abs/1908.01863'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '2D SLAMにおける地図表現にSDFを導入し，計測点が存在しない空間の情報を使った局所特徴(free-space features)を提案．曲率ベースの特徴点検出と方向付き勾配ヒストグラムを使った記述子を使い，従来手法より大域位置認識が高精度に行えることを示した．\\nhttps://arxiv.org/abs/1908.01863\\xa0pic.twitter.com/GJSmADnz12',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">2D SLAMにおける地図表現にSDFを導入し，計測点が存在しない空間の情報を使った局所特徴(free-space features)を提案．曲率ベースの特徴点検出と方向付き勾配ヒストグラムを使った記述子を使い，従来手法より大域位置認識が高精度に行えることを示した．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1908.01863\" dir=\"ltr\" href=\"https://t.co/eu5Rh4GKJ2\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1908.01863\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1908.01863</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/GJSmADnz12\">pic.twitter.com/GJSmADnz12</a></p>',\n",
       "  'timestamp': '2020-07-20T03:12:05',\n",
       "  'timestamp_epochs': 1595214725,\n",
       "  'tweet_id': '1285049816234422272',\n",
       "  'tweet_url': '/slam_hub/status/1285049816234422272',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 16,\n",
       "  'links': ['https://youtu.be/ifL8yTbRFDk'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '三次元点群の意味的な特徴とCenter VotesからCenter Proposalを生成、GCNでProposalの特徴をリファイン、Proposalの合体によるInstance SegmentationのMulti Proposal Aggregation Network(MPA)を提案。既存手法のNon-Maximum-Suppression(NMS)と比べて、MPAの優位性を確認。https://youtu.be/ifL8yTbRFDk\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">三次元点群の意味的な特徴とCenter VotesからCenter Proposalを生成、GCNでProposalの特徴をリファイン、Proposalの合体によるInstance SegmentationのMulti Proposal Aggregation Network(MPA)を提案。既存手法のNon-Maximum-Suppression(NMS)と比べて、MPAの優位性を確認。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/ifL8yTbRFDk\" dir=\"ltr\" href=\"https://t.co/pvjEGQ5fac\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/ifL8yTbRFDk\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/ifL8yTbRFDk</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-19T03:00:00',\n",
       "  'timestamp_epochs': 1595127600,\n",
       "  'tweet_id': '1284684384776462337',\n",
       "  'tweet_url': '/slam_hub/status/1284684384776462337',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EdLUT0hUMAIVyYt.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 16,\n",
       "  'links': ['https://arxiv.org/abs/2007.08509'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 1,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '大域的な一貫性を保ったvid2vid．直前の数フレームに基づきクエリ(セマンティクス画像)に対応する画像生成を行う従来法では，同じ位置に立ち戻る場合に一貫性が保証されない．提案手法では，SfMを利用して環境を逐次的に3次元復元し，その幾何をガイドとした画像生成を行う．\\nhttps://arxiv.org/abs/2007.08509\\xa0pic.twitter.com/uaQO8g5ofF',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">大域的な一貫性を保ったvid2vid．直前の数フレームに基づきクエリ(セマンティクス画像)に対応する画像生成を行う従来法では，同じ位置に立ち戻る場合に一貫性が保証されない．提案手法では，SfMを利用して環境を逐次的に3次元復元し，その幾何をガイドとした画像生成を行う．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2007.08509\" dir=\"ltr\" href=\"https://t.co/Bvu29BBiEk\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2007.08509\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2007.08509</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/uaQO8g5ofF\">pic.twitter.com/uaQO8g5ofF</a></p>',\n",
       "  'timestamp': '2020-07-18T03:01:22',\n",
       "  'timestamp_epochs': 1595041282,\n",
       "  'tweet_id': '1284322339836985344',\n",
       "  'tweet_url': '/slam_hub/status/1284322339836985344',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EdBEUIHUcAE5_bz.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 20,\n",
       "  'links': ['https://arxiv.org/abs/2003.04159'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Pre-Integration を用いた Tight-coupled な Visual-Inertial Odometry (VIO) に、GPS等によるグローバル座標拘束を導入する手法を初めて提案．従来法はVIOの後段にカルマンフィルタ等を用いて分割して対処．提案手法はコスト関数に拘束を組込み一括で最適化．\\nhttps://arxiv.org/abs/2003.04159\\xa0pic.twitter.com/Im1bHSvCyO',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Pre-Integration を用いた Tight-coupled な Visual-Inertial Odometry (VIO) に、GPS等によるグローバル座標拘束を導入する手法を初めて提案．従来法はVIOの後段にカルマンフィルタ等を用いて分割して対処．提案手法はコスト関数に拘束を組込み一括で最適化．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.04159\" dir=\"ltr\" href=\"https://t.co/esbWd3Fr05\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.04159\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.04159</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Im1bHSvCyO\">pic.twitter.com/Im1bHSvCyO</a></p>',\n",
       "  'timestamp': '2020-07-16T03:15:23',\n",
       "  'timestamp_epochs': 1594869323,\n",
       "  'tweet_id': '1283601093612548099',\n",
       "  'tweet_url': '/slam_hub/status/1283601093612548099',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 29,\n",
       "  'links': ['https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画素毎に並列計算可能なFocal-plane Sensor-processor (FPSP) を用いたVisual OdometryアルゴリズムBIT-VOを提案．FPSP上で，アナログ信号の領域で2値のエッジ検出を行い，そのエッジからバイナリ特徴を計算．それらをホストデバイスに転送することで300fpsの6DOF VOを実現．https://www.youtube.com/watch?v=tnPfbJaPrSQ&feature=emb_title\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画素毎に並列計算可能なFocal-plane Sensor-processor (FPSP) を用いたVisual OdometryアルゴリズムBIT-VOを提案．FPSP上で，アナログ信号の領域で2値のエッジ検出を行い，そのエッジからバイナリ特徴を計算．それらをホストデバイスに転送することで300fpsの6DOF VOを実現．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=tnPfbJaPrSQ&amp;feature=emb_title\" dir=\"ltr\" href=\"https://t.co/TELHHAMBQ8\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=tnPfbJaPrSQ&amp;feature=emb_title\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=tnPfbJ</span><span class=\"invisible\">aPrSQ&amp;feature=emb_title</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-07-15T03:00:00',\n",
       "  'timestamp_epochs': 1594782000,\n",
       "  'tweet_id': '1283234833829945345',\n",
       "  'tweet_url': '/slam_hub/status/1283234833829945345',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Ec2t_Y-UcAAYEyM.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 27,\n",
       "  'links': ['https://arxiv.org/abs/2004.01793'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像からの物体方向推定を，self-supervisedに学習する枠組みを提案．画像から3次元方向とスタイル特徴量を抽出し，それらの潜在変数を元に幾何学的変換を行うGenerator(GAN)を用いて学習．損失には一貫性と水平対称性を利用し，教師あり学習に匹敵する性能を達成．\\nhttps://arxiv.org/abs/2004.01793\\xa0pic.twitter.com/Qr4LM3uTxA',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像からの物体方向推定を，self-supervisedに学習する枠組みを提案．画像から3次元方向とスタイル特徴量を抽出し，それらの潜在変数を元に幾何学的変換を行うGenerator(GAN)を用いて学習．損失には一貫性と水平対称性を利用し，教師あり学習に匹敵する性能を達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.01793\" dir=\"ltr\" href=\"https://t.co/rEGIDLI41f\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.01793\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.01793</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Qr4LM3uTxA\">pic.twitter.com/Qr4LM3uTxA</a></p>',\n",
       "  'timestamp': '2020-07-14T03:02:09',\n",
       "  'timestamp_epochs': 1594695729,\n",
       "  'tweet_id': '1282872986467397633',\n",
       "  'tweet_url': '/slam_hub/status/1282872986467397633',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EcuxFqyU4AAiQBT.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 50,\n",
       "  'links': ['https://arxiv.org/abs/2007.01475'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 12,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Kinect等のPerspectiveなdepthセンサーと全方位画像を用いて全方位のdepthを得る手法の提案.Encoderの最後の層でPerspective座標に変換し特徴量の学習難度を下げ、Decoderでequirectangular座標に戻す.他のSoTAな手法より優れていることを示した.\\nhttps://arxiv.org/abs/2007.01475\\xa0pic.twitter.com/ZYPB7ysvaY',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Kinect等のPerspectiveなdepthセンサーと全方位画像を用いて全方位のdepthを得る手法の提案.Encoderの最後の層でPerspective座標に変換し特徴量の学習難度を下げ、Decoderでequirectangular座標に戻す.他のSoTAな手法より優れていることを示した.\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2007.01475\" dir=\"ltr\" href=\"https://t.co/2bYwmzjJ8d\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2007.01475\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2007.01475</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/ZYPB7ysvaY\">pic.twitter.com/ZYPB7ysvaY</a></p>',\n",
       "  'timestamp': '2020-07-13T03:00:00',\n",
       "  'timestamp_epochs': 1594609200,\n",
       "  'tweet_id': '1282510058660737024',\n",
       "  'tweet_url': '/slam_hub/status/1282510058660737024',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EctNZZjVcAA-IRH.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 77,\n",
       "  'links': ['https://arxiv.org/abs/1910.01712'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 28,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '360度屋内画像における物体検出とクラス認識に関するデータセットを提示．Equirectangular形式における極領域の歪みに対応するため，Sphere Netをはじめとする球状CNNを用いたモデルで評価したところ，透視投影画像によるデータセットでの学習よりも大きな改善が見られた．https://arxiv.org/abs/1910.01712\\xa0pic.twitter.com/buiy0FHupl',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">360度屋内画像における物体検出とクラス認識に関するデータセットを提示．Equirectangular形式における極領域の歪みに対応するため，Sphere Netをはじめとする球状CNNを用いたモデルで評価したところ，透視投影画像によるデータセットでの学習よりも大きな改善が見られた．<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1910.01712\" dir=\"ltr\" href=\"https://t.co/3ZkwYquWBt\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1910.01712\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1910.01712</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/buiy0FHupl\">pic.twitter.com/buiy0FHupl</a></p>',\n",
       "  'timestamp': '2020-07-12T06:43:02',\n",
       "  'timestamp_epochs': 1594536182,\n",
       "  'tweet_id': '1282203799256350721',\n",
       "  'tweet_url': '/slam_hub/status/1282203799256350721',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EcnZCpAUEAEpnSG.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 55,\n",
       "  'links': ['https://arxiv.org/abs/2004.10681'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 12,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'CNNで推定した擬似的なデプスマップを用いてRGB-D SLAMを行う．単眼デプス推定の欠点であるスケールの不整合性を，特徴点ベースのSLAMで作成された三次元点を用いてリファインする．これにより，両者の欠点を補った高精度な姿勢推定が可能となった．\\nhttps://arxiv.org/abs/2004.10681\\xa0pic.twitter.com/OXOuG5onDG',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">CNNで推定した擬似的なデプスマップを用いてRGB-D SLAMを行う．単眼デプス推定の欠点であるスケールの不整合性を，特徴点ベースのSLAMで作成された三次元点を用いてリファインする．これにより，両者の欠点を補った高精度な姿勢推定が可能となった．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.10681\" dir=\"ltr\" href=\"https://t.co/QEz7bskILV\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.10681\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.10681</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/OXOuG5onDG\">pic.twitter.com/OXOuG5onDG</a></p>',\n",
       "  'timestamp': '2020-07-11T03:36:30',\n",
       "  'timestamp_epochs': 1594438590,\n",
       "  'tweet_id': '1281794468920713217',\n",
       "  'tweet_url': '/slam_hub/status/1281794468920713217',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EceUhQ5VAAEEjKk.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 26,\n",
       "  'links': ['https://arxiv.org/abs/2002.10701'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '平面への投影を用いた点群畳み込みを提案．明示的に接平面を推定するTangentConvとは異なり，点群の投影と内挿を単一の重み行列で表現し，MLPを用いて学習ベースで推定する．Volmetricな畳み込みとの組み合わせでSoTA達成．\\nhttps://arxiv.org/abs/2002.10701\\xa0pic.twitter.com/ZmJ3UuwIg1',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">平面への投影を用いた点群畳み込みを提案．明示的に接平面を推定するTangentConvとは異なり，点群の投影と内挿を単一の重み行列で表現し，MLPを用いて学習ベースで推定する．Volmetricな畳み込みとの組み合わせでSoTA達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2002.10701\" dir=\"ltr\" href=\"https://t.co/SPhaaYO4RI\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2002.10701\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2002.10701</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/ZmJ3UuwIg1\">pic.twitter.com/ZmJ3UuwIg1</a></p>',\n",
       "  'timestamp': '2020-07-10T03:00:00',\n",
       "  'timestamp_epochs': 1594350000,\n",
       "  'tweet_id': '1281422893826494464',\n",
       "  'tweet_url': '/slam_hub/status/1281422893826494464',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 21,\n",
       "  'links': ['https://youtu.be/apmmduXTnaE'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'LIDARを使った位置推定手法．極座標で作られた高さマップ(Scan Context)を入力とし，CNNで地図上での位置をクラスとして推定する．複数の実データセット上で，一日の学習データで一年通した長期位置推定が高精度に可能であることを示した．https://youtu.be/apmmduXTnaE\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">LIDARを使った位置推定手法．極座標で作られた高さマップ(Scan Context)を入力とし，CNNで地図上での位置をクラスとして推定する．複数の実データセット上で，一日の学習データで一年通した長期位置推定が高精度に可能であることを示した．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/apmmduXTnaE\" dir=\"ltr\" href=\"https://t.co/yuv7HnKiob\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/apmmduXTnaE\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/apmmduXTnaE</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-09T03:56:41',\n",
       "  'timestamp_epochs': 1594267001,\n",
       "  'tweet_id': '1281074772625874946',\n",
       "  'tweet_url': '/slam_hub/status/1281074772625874946',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 19,\n",
       "  'links': ['https://youtu.be/9-ixexpjN-8'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '教師なしでコンパクトかつウォータータイトな三次元メッシュ生成手法を提案。Binary Space Partitioning (BSP) で再帰的に入力形状を超平面に分解し、Constructive Solid Geometry (CSG) のブーリアン演算で、分解した超平面から複雑な表面やオブジェクトの生成が可能となる。https://youtu.be/9-ixexpjN-8\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">教師なしでコンパクトかつウォータータイトな三次元メッシュ生成手法を提案。Binary Space Partitioning (BSP) で再帰的に入力形状を超平面に分解し、Constructive Solid Geometry (CSG) のブーリアン演算で、分解した超平面から複雑な表面やオブジェクトの生成が可能となる。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/9-ixexpjN-8\" dir=\"ltr\" href=\"https://t.co/UjEAIv7g8D\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/9-ixexpjN-8\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/9-ixexpjN-8</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-08T04:43:03',\n",
       "  'timestamp_epochs': 1594183383,\n",
       "  'tweet_id': '1280724052026392576',\n",
       "  'tweet_url': '/slam_hub/status/1280724052026392576',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 16,\n",
       "  'links': ['https://youtu.be/Jvl42VJmYxg'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'インスタンスを意識したRGB-Dセンサによる地図生成．幾何的なセグメンテーション結果をMask R-CNNから補正し，Over-segmentationを抑制した個別の物体形状を獲得．各物体の幾何を大域地図上で関連付けていくことで，セマンティクス＆インスタンス情報も付与した地図を生成．https://youtu.be/Jvl42VJmYxg\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">インスタンスを意識したRGB-Dセンサによる地図生成．幾何的なセグメンテーション結果をMask R-CNNから補正し，Over-segmentationを抑制した個別の物体形状を獲得．各物体の幾何を大域地図上で関連付けていくことで，セマンティクス＆インスタンス情報も付与した地図を生成．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/Jvl42VJmYxg\" dir=\"ltr\" href=\"https://t.co/HTQur14iqD\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/Jvl42VJmYxg\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/Jvl42VJmYxg</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-07T03:37:04',\n",
       "  'timestamp_epochs': 1594093024,\n",
       "  'tweet_id': '1280345060442288129',\n",
       "  'tweet_url': '/slam_hub/status/1280345060442288129',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EcNbQNwVcAEqhUL.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 31,\n",
       "  'links': ['https://arxiv.org/abs/1904.06504'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Visual-Inertial SLAM の Basalt。IMU の preintegration ではなく、非線形因子復元を行って大域的に最適化。IMU の積分は誤差が大きい問題に対処。VIO の相対位置拘束とロール・ピッチ拘束、バンドル調整のループ拘束を統合。小さな最適化問題として定式化でき、精度も向上。\\nhttps://arxiv.org/abs/1904.06504\\xa0pic.twitter.com/mQYfoQ4yjd',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Visual-Inertial SLAM の Basalt。IMU の preintegration ではなく、非線形因子復元を行って大域的に最適化。IMU の積分は誤差が大きい問題に対処。VIO の相対位置拘束とロール・ピッチ拘束、バンドル調整のループ拘束を統合。小さな最適化問題として定式化でき、精度も向上。\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1904.06504\" dir=\"ltr\" href=\"https://t.co/WlTS55aEGX\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1904.06504\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1904.06504</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/mQYfoQ4yjd\">pic.twitter.com/mQYfoQ4yjd</a></p>',\n",
       "  'timestamp': '2020-07-06T03:00:00',\n",
       "  'timestamp_epochs': 1594004400,\n",
       "  'tweet_id': '1279973343249281025',\n",
       "  'tweet_url': '/slam_hub/status/1279973343249281025',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EcIUVnyUwAARQnd.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 23,\n",
       "  'links': ['https://ieeexplore.ieee.org/document/9099049'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '任意の環境形状を表せる陰関数表現を用い2D SLAMを定式化．陰関数に対する分散の導出や陰関数境界内外での最適化の安定化などを行い，楕円・直線モデルを用いた評価実験では従来のモデルフィッティングベース手法より良い精度を示した．\\nhttps://ieeexplore.ieee.org/document/9099049\\xa0…pic.twitter.com/HnpTxH7bL6',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">任意の環境形状を表せる陰関数表現を用い2D SLAMを定式化．陰関数に対する分散の導出や陰関数境界内外での最適化の安定化などを行い，楕円・直線モデルを用いた評価実験では従来のモデルフィッティングベース手法より良い精度を示した．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://ieeexplore.ieee.org/document/9099049\" dir=\"ltr\" href=\"https://t.co/5aqBkPQC8t\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://ieeexplore.ieee.org/document/9099049\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">ieeexplore.ieee.org/document/90990</span><span class=\"invisible\">49</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/HnpTxH7bL6\">pic.twitter.com/HnpTxH7bL6</a></p>',\n",
       "  'timestamp': '2020-07-05T03:00:00',\n",
       "  'timestamp_epochs': 1593918000,\n",
       "  'tweet_id': '1279610956172017666',\n",
       "  'tweet_url': '/slam_hub/status/1279610956172017666',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EcDXJCJUcAEXBxP.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 23,\n",
       "  'links': ['https://arxiv.org/abs/2002.02638'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '複数マップ間のループ検出における外れ値検出手法を提案．各ノード間の回転から閉ループの幾何的整合性をチェックすることで確率的に外れ値を検出．さらにEMアルゴリズムを用いてパラメータをfine-tune．確率伝搬法より高い精度を達成し，収束性も保証．\\nhttps://arxiv.org/abs/2002.02638\\xa0pic.twitter.com/zugcqk2JzE',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">複数マップ間のループ検出における外れ値検出手法を提案．各ノード間の回転から閉ループの幾何的整合性をチェックすることで確率的に外れ値を検出．さらにEMアルゴリズムを用いてパラメータをfine-tune．確率伝搬法より高い精度を達成し，収束性も保証．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2002.02638\" dir=\"ltr\" href=\"https://t.co/bjCvP6ykCV\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2002.02638\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2002.02638</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/zugcqk2JzE\">pic.twitter.com/zugcqk2JzE</a></p>',\n",
       "  'timestamp': '2020-07-04T03:41:05',\n",
       "  'timestamp_epochs': 1593834065,\n",
       "  'tweet_id': '1279258907534213121',\n",
       "  'tweet_url': '/slam_hub/status/1279258907534213121',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': ['ECCV2020'],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 9,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 1,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'SLAM-Hub members got one paper accepted by #ECCV2020 !',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"en\">SLAM-Hub members got one paper accepted by <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/ECCV2020?src=hash\"><s>#</s><b>ECCV2020</b></a> !</p>',\n",
       "  'timestamp': '2020-07-03T04:13:33',\n",
       "  'timestamp_epochs': 1593749613,\n",
       "  'tweet_id': '1278904690546098176',\n",
       "  'tweet_url': '/slam_hub/status/1278904690546098176',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 22,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！</p>',\n",
       "  'timestamp': '2020-07-03T04:12:31',\n",
       "  'timestamp_epochs': 1593749551,\n",
       "  'tweet_id': '1278904428246888448',\n",
       "  'tweet_url': '/slam_hub/status/1278904428246888448',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 16,\n",
       "  'links': ['https://youtu.be/dI2FZG_txN0'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 2,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'ワイヤーフレームや電線など細い物体に対する形状復元の手法を提案．視点の追加ごとに，点群表現のカーブとカメラポーズを新しいマッチング手法に基づき交互に最適化。また，オクルージョンを検知し誤対応を防止．カーブの各部で太さを推定することで高品質な復元が可能に．https://youtu.be/dI2FZG_txN0\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ワイヤーフレームや電線など細い物体に対する形状復元の手法を提案．視点の追加ごとに，点群表現のカーブとカメラポーズを新しいマッチング手法に基づき交互に最適化。また，オクルージョンを検知し誤対応を防止．カーブの各部で太さを推定することで高品質な復元が可能に．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/dI2FZG_txN0\" dir=\"ltr\" href=\"https://t.co/GzWbOIII9D\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/dI2FZG_txN0\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/dI2FZG_txN0</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-03T03:00:00',\n",
       "  'timestamp_epochs': 1593745200,\n",
       "  'tweet_id': '1278886178670796801',\n",
       "  'tweet_url': '/slam_hub/status/1278886178670796801',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Eb42BPFUcAAAe1E.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 52,\n",
       "  'links': ['https://arxiv.org/abs/2006.05734'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 14,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '一枚の画像から人体の3D Meshを推定する手法.画像ピクセルと表面間の密な対応を推定し,その対応により画像空間からUV空間へ局所的な特徴が移され,位置マップに回帰される.最後にマッピング関数により3D Meshを再構成する.3D Meshベースの従来手法より優れていることを示した.\\nhttps://arxiv.org/abs/2006.05734\\xa0pic.twitter.com/Ku422DMyAp',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">一枚の画像から人体の3D Meshを推定する手法.画像ピクセルと表面間の密な対応を推定し,その対応により画像空間からUV空間へ局所的な特徴が移され,位置マップに回帰される.最後にマッピング関数により3D Meshを再構成する.3D Meshベースの従来手法より優れていることを示した.\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.05734\" dir=\"ltr\" href=\"https://t.co/jAGmz0Jxqe\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.05734\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.05734</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Ku422DMyAp\">pic.twitter.com/Ku422DMyAp</a></p>',\n",
       "  'timestamp': '2020-07-02T03:00:00',\n",
       "  'timestamp_epochs': 1593658800,\n",
       "  'tweet_id': '1278523792042496000',\n",
       "  'tweet_url': '/slam_hub/status/1278523792042496000',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 9,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 2,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 2,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'SLAM-Hub members got 4 papers accepted by IROS2020!',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"en\">SLAM-Hub members got 4 papers accepted by IROS2020!</p>',\n",
       "  'timestamp': '2020-07-01T08:27:20',\n",
       "  'timestamp_epochs': 1593592040,\n",
       "  'tweet_id': '1278243781104463872',\n",
       "  'tweet_url': '/slam_hub/status/1278243781104463872',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 50,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'ロボティクス分野の国際会議IROS2020にSLAM-Hubのメンバーから4本の論文が採択されました！',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ロボティクス分野の国際会議IROS2020にSLAM-Hubのメンバーから4本の論文が採択されました！</p>',\n",
       "  'timestamp': '2020-07-01T08:23:39',\n",
       "  'timestamp_epochs': 1593591819,\n",
       "  'tweet_id': '1278242853508898816',\n",
       "  'tweet_url': '/slam_hub/status/1278242853508898816',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 6,\n",
       "  'links': ['https://youtu.be/DYBmD88vpiA'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 1,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'シーン分解や集合予測のアーキテクチャに統合可能な，オブジェクト中心の抽象表現を学習するSlot Attentionモジュールを提案．CNNの出力と構造表現間において，順列不変なk個のSlotを生成．反復的注意メカニズムでSlotのグループ化戦略を学習．点群やグラフのグループ化も可能https://youtu.be/DYBmD88vpiA\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">シーン分解や集合予測のアーキテクチャに統合可能な，オブジェクト中心の抽象表現を学習するSlot Attentionモジュールを提案．CNNの出力と構造表現間において，順列不変なk個のSlotを生成．反復的注意メカニズムでSlotのグループ化戦略を学習．点群やグラフのグループ化も可能<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/DYBmD88vpiA\" dir=\"ltr\" href=\"https://t.co/k6Q3hFeYyq\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/DYBmD88vpiA\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/DYBmD88vpiA</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-07-01T04:46:12',\n",
       "  'timestamp_epochs': 1593578772,\n",
       "  'tweet_id': '1278188128444940288',\n",
       "  'tweet_url': '/slam_hub/status/1278188128444940288',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbuUgMbU8AEbJUJ.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 33,\n",
       "  'links': ['https://arxiv.org/abs/2006.14616'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '深層学習における性質の良い回転表現を提案．\\n回転行列を一度9パラメータで表現し，SVDによる特殊直交化によりSO(3)空間へマップする．\\n深層学習タスクにおいてクォータニオンやangle-axisベクトルなどの他の回転表現より高精度に姿勢を求めることが可能．\\nhttps://arxiv.org/abs/2006.14616\\xa0pic.twitter.com/WVhQcNGBad',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">深層学習における性質の良い回転表現を提案．\\n回転行列を一度9パラメータで表現し，SVDによる特殊直交化によりSO(3)空間へマップする．\\n深層学習タスクにおいてクォータニオンやangle-axisベクトルなどの他の回転表現より高精度に姿勢を求めることが可能．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.14616\" dir=\"ltr\" href=\"https://t.co/eXV1lEhjwb\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.14616\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.14616</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/WVhQcNGBad\">pic.twitter.com/WVhQcNGBad</a></p>',\n",
       "  'timestamp': '2020-06-30T03:00:00',\n",
       "  'timestamp_epochs': 1593486000,\n",
       "  'tweet_id': '1277799017053859842',\n",
       "  'tweet_url': '/slam_hub/status/1277799017053859842',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbpbVrhVcAM26gU.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 34,\n",
       "  'links': ['https://arxiv.org/abs/1912.02984'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '高速でスケーラブルな点群データの処理機構を提案．Voxelを用いることで高速かつカバー率の高いサンプリングを行い，Voxel内部でローカルにグラフを構築して畳み込む．点群の分類とセグメンテーションで従来手法より高速かつ高精度を達成．\\nhttps://arxiv.org/abs/1912.02984\\xa0pic.twitter.com/nlIsA1sC2v',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">高速でスケーラブルな点群データの処理機構を提案．Voxelを用いることで高速かつカバー率の高いサンプリングを行い，Voxel内部でローカルにグラフを構築して畳み込む．点群の分類とセグメンテーションで従来手法より高速かつ高精度を達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1912.02984\" dir=\"ltr\" href=\"https://t.co/gONjLXoQu9\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1912.02984\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1912.02984</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/nlIsA1sC2v\">pic.twitter.com/nlIsA1sC2v</a></p>',\n",
       "  'timestamp': '2020-06-29T03:00:00',\n",
       "  'timestamp_epochs': 1593399600,\n",
       "  'tweet_id': '1277436627485290496',\n",
       "  'tweet_url': '/slam_hub/status/1277436627485290496',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbkUPnoUEAElg-3.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 34,\n",
       "  'links': ['https://arxiv.org/abs/2005.02138'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'PointNetに類似したネットワークを用い点群から三角形メッシュを生成．入力点群から三角形群を出力するネットと，入力三角形群の中から3Dモデルとして妥当な三角形を判定するネットを交互に適用し，メッシュモデルを復元する．\\nhttps://arxiv.org/abs/2005.02138\\xa0pic.twitter.com/8JeOljgxXg',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">PointNetに類似したネットワークを用い点群から三角形メッシュを生成．入力点群から三角形群を出力するネットと，入力三角形群の中から3Dモデルとして妥当な三角形を判定するネットを交互に適用し，メッシュモデルを復元する．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2005.02138\" dir=\"ltr\" href=\"https://t.co/QVv7vmysPt\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2005.02138\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2005.02138</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/8JeOljgxXg\">pic.twitter.com/8JeOljgxXg</a></p>',\n",
       "  'timestamp': '2020-06-28T03:00:15',\n",
       "  'timestamp_epochs': 1593313215,\n",
       "  'tweet_id': '1277074304786546689',\n",
       "  'tweet_url': '/slam_hub/status/1277074304786546689',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 27,\n",
       "  'links': ['https://youtu.be/glZyJ66ktog'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'スパースまたノイジーな点群からメッシュ生成のため、ローカル幾何形状を表現するMeshletを提案。VAEでMeshletをポーズ不変な潜在空間にエンコード、点群と近いMeshlet(補助メッシュから取出)をデコード、変形の補助メッシュを利用、Meshlet間のグローバルな整合性を強める。https://youtu.be/glZyJ66ktog\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">スパースまたノイジーな点群からメッシュ生成のため、ローカル幾何形状を表現するMeshletを提案。VAEでMeshletをポーズ不変な潜在空間にエンコード、点群と近いMeshlet(補助メッシュから取出)をデコード、変形の補助メッシュを利用、Meshlet間のグローバルな整合性を強める。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/glZyJ66ktog\" dir=\"ltr\" href=\"https://t.co/IG1dDhPu7J\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/glZyJ66ktog\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/glZyJ66ktog</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-27T05:01:35',\n",
       "  'timestamp_epochs': 1593234095,\n",
       "  'tweet_id': '1276742451806404608',\n",
       "  'tweet_url': '/slam_hub/status/1276742451806404608',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 24,\n",
       "  'links': ['https://youtu.be/B4YBWFuYBdE'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '非剛体変形下での多視点ステレオの提案．まず変形の程度の小さい画像ペアを一組決定し，対象の基本的な3次元構造を復元．さらに，その他の変形を伴う画像群に対してもDeformation graphを利用したJoint optimizationにより，DeformationとDepthの推定を同時に行っている．https://youtu.be/B4YBWFuYBdE\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">非剛体変形下での多視点ステレオの提案．まず変形の程度の小さい画像ペアを一組決定し，対象の基本的な3次元構造を復元．さらに，その他の変形を伴う画像群に対してもDeformation graphを利用したJoint optimizationにより，DeformationとDepthの推定を同時に行っている．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/B4YBWFuYBdE\" dir=\"ltr\" href=\"https://t.co/IXDz5mMfPS\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/B4YBWFuYBdE\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/B4YBWFuYBdE</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-26T03:46:05',\n",
       "  'timestamp_epochs': 1593143165,\n",
       "  'tweet_id': '1276361064104341506',\n",
       "  'tweet_url': '/slam_hub/status/1276361064104341506',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbVJJlXUwAEYf24.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 31,\n",
       "  'links': ['https://arxiv.org/abs/1910.02490'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 14,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Visual-Inertial SLAM の Kimera。IMU の on-manifold preintegration と画像の Shi-Tomasi コーナー特徴点で VIO。DBoW2 でループ検出、GTSAM（iSAM2）でグラフ最適化。メッシュ生成と TSDF での復元。画像でセマンティックラベリングして逆投影し、ボクセルをベイズで更新。\\nhttps://arxiv.org/abs/1910.02490\\xa0pic.twitter.com/RSnDbyYV23',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Visual-Inertial SLAM の Kimera。IMU の on-manifold preintegration と画像の Shi-Tomasi コーナー特徴点で VIO。DBoW2 でループ検出、GTSAM（iSAM2）でグラフ最適化。メッシュ生成と TSDF での復元。画像でセマンティックラベリングして逆投影し、ボクセルをベイズで更新。\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1910.02490\" dir=\"ltr\" href=\"https://t.co/S937LjNHtN\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1910.02490\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1910.02490</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/RSnDbyYV23\">pic.twitter.com/RSnDbyYV23</a></p>',\n",
       "  'timestamp': '2020-06-25T04:17:14',\n",
       "  'timestamp_epochs': 1593058634,\n",
       "  'tweet_id': '1276006512670830593',\n",
       "  'tweet_url': '/slam_hub/status/1276006512670830593',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbPyxA_UYAYaXp2.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 39,\n",
       "  'links': ['https://arxiv.org/abs/2003.10432'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '複数視点の2D-CNN出力をVoxelにBack-projetして蓄積し，Voxel mapを3D-CNNに通すことでSemantic情報を含んだMulti-view Stereoを実現．2D-CNNにはResnet50-FPN，3D-CNNはSkip Connectionを持つEncoder-decoderを利用．実時間処理が可能．\\nhttps://arxiv.org/abs/2003.10432\\xa0pic.twitter.com/e3e3J3TEua',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">複数視点の2D-CNN出力をVoxelにBack-projetして蓄積し，Voxel mapを3D-CNNに通すことでSemantic情報を含んだMulti-view Stereoを実現．2D-CNNにはResnet50-FPN，3D-CNNはSkip Connectionを持つEncoder-decoderを利用．実時間処理が可能．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.10432\" dir=\"ltr\" href=\"https://t.co/fqH17Dlr50\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.10432\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.10432</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/e3e3J3TEua\">pic.twitter.com/e3e3J3TEua</a></p>',\n",
       "  'timestamp': '2020-06-24T03:21:32',\n",
       "  'timestamp_epochs': 1592968892,\n",
       "  'tweet_id': '1275630109311950849',\n",
       "  'tweet_url': '/slam_hub/status/1275630109311950849',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbH-5qqUMAU4zcQ.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 21,\n",
       "  'links': ['https://arxiv.org/abs/2006.06634'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '特徴量を，それ自身を含む部分アフィン空間へ埋め込みことで，識別機能を保ちながらプライバシーアタックへの耐性を大幅に向上．部分空間同士の距離を導入し特徴マッチングを可能とした．元の特徴量と比較して，僅かな識別性能の低下により高いプラバシー保護性能を実現．\\nhttps://arxiv.org/abs/2006.06634\\xa0pic.twitter.com/ex4qczr200',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">特徴量を，それ自身を含む部分アフィン空間へ埋め込みことで，識別機能を保ちながらプライバシーアタックへの耐性を大幅に向上．部分空間同士の距離を導入し特徴マッチングを可能とした．元の特徴量と比較して，僅かな識別性能の低下により高いプラバシー保護性能を実現．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.06634\" dir=\"ltr\" href=\"https://t.co/rHNaYW5XzW\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.06634\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.06634</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/ex4qczr200\">pic.twitter.com/ex4qczr200</a></p>',\n",
       "  'timestamp': '2020-06-23T03:00:00',\n",
       "  'timestamp_epochs': 1592881200,\n",
       "  'tweet_id': '1275262302594293762',\n",
       "  'tweet_url': '/slam_hub/status/1275262302594293762',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EbFP0N1U4AIpIdb.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 44,\n",
       "  'links': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 18,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '複数画像からガラスオブジェクトの3D形状を再構築するネットワークを提案．Visual hullで得た荒い形状を元に，各視点で屈折，反射点の法線を推論．環境マップでレンダリングした再投影誤差と，視点間を統合した点群とGTとの損失で学習．高品質な3D復元が可能なことを実証．\\nhttp://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\\xa0…pic.twitter.com/uXXFhuXt98',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">複数画像からガラスオブジェクトの3D形状を再構築するネットワークを提案．Visual hullで得た荒い形状を元に，各視点で屈折，反射点の法線を推論．環境マップでレンダリングした再投影誤差と，視点間を統合した点群とGTとの損失で学習．高品質な3D復元が可能なことを実証．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\" dir=\"ltr\" href=\"https://t.co/hf3b7VdFGD\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://openaccess.thecvf.com/content_CVPR_2020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">http://</span><span class=\"js-display-url\">openaccess.thecvf.com/content_CVPR_2</span><span class=\"invisible\">020/html/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.html</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/uXXFhuXt98\">pic.twitter.com/uXXFhuXt98</a></p>',\n",
       "  'timestamp': '2020-06-22T03:00:01',\n",
       "  'timestamp_epochs': 1592794801,\n",
       "  'tweet_id': '1274899915655843840',\n",
       "  'tweet_url': '/slam_hub/status/1274899915655843840',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Ea82XPmUMAUDsh2.png',\n",
       "   'https://pbs.twimg.com/media/Ea82Y6-UYAAo1YY.jpg',\n",
       "   'https://pbs.twimg.com/media/Ea82ZqnUwAAdcoT.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 23,\n",
       "  'links': ['http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 10,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '単一画像での法線推定手法の提案.RGB画像とマンハッタン線マップを入力とし,マンハッタン方向に沿う領域を識別するマップと法線マップをネットワークで回帰,融合する.従来手法より優れた結果を示し未見のデータに対しても推定可能なことを示した.\\nhttp://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\\xa0…pic.twitter.com/vRPvFPGMEV',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">単一画像での法線推定手法の提案.RGB画像とマンハッタン線マップを入力とし,マンハッタン方向に沿う領域を識別するマップと法線マップをネットワークで回帰,融合する.従来手法より優れた結果を示し未見のデータに対しても推定可能なことを示した.\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\" dir=\"ltr\" href=\"https://t.co/ukreWQnh1L\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">http://</span><span class=\"js-display-url\">openaccess.thecvf.com/content_CVPR_2</span><span class=\"invisible\">020/html/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.html</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/vRPvFPGMEV\">pic.twitter.com/vRPvFPGMEV</a></p>',\n",
       "  'timestamp': '2020-06-21T03:00:00',\n",
       "  'timestamp_epochs': 1592708400,\n",
       "  'tweet_id': '1274537524241797121',\n",
       "  'tweet_url': '/slam_hub/status/1274537524241797121',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Ea7i_wTU4AAQXtM.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 42,\n",
       "  'links': ['https://arxiv.org/abs/2006.09348'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 12,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Lidarによる実世界データを用いた，従来のCADモデルによる手法よりも多彩で現実感の高い自動運転用シミュレーションを提案．Lidar点群から動的物体や環境マップなどのアセットを作成後，物理レンダリングとDNNでドメインギャップの小さなセンサシミュレーションを行う．https://arxiv.org/abs/2006.09348\\xa0pic.twitter.com/nrtVaRw315',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Lidarによる実世界データを用いた，従来のCADモデルによる手法よりも多彩で現実感の高い自動運転用シミュレーションを提案．Lidar点群から動的物体や環境マップなどのアセットを作成後，物理レンダリングとDNNでドメインギャップの小さなセンサシミュレーションを行う．<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.09348\" dir=\"ltr\" href=\"https://t.co/HgMKRrvvqi\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.09348\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.09348</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/nrtVaRw315\">pic.twitter.com/nrtVaRw315</a></p>',\n",
       "  'timestamp': '2020-06-20T05:01:00',\n",
       "  'timestamp_epochs': 1592629260,\n",
       "  'tweet_id': '1274205587597737985',\n",
       "  'tweet_url': '/slam_hub/status/1274205587597737985',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Ea2BLHfU0AA7NHO.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 36,\n",
       "  'links': ['https://arxiv.org/abs/2006.06017'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'VIO, VI-SLAMにおける状態変数の初期化手法を提案．効率的に不要変数を除去しつつ，3つ以上の3D点の観測を平等に扱う新たな定式化．この線形ソルバはシンプルな構造ながら過去の手法と比較してモーション推定の精度を最大50%向上させ，非線形ソルバの反復回数も削減．\\nhttps://arxiv.org/abs/2006.06017\\xa0pic.twitter.com/IYHoycp0k8',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">VIO, VI-SLAMにおける状態変数の初期化手法を提案．効率的に不要変数を除去しつつ，3つ以上の3D点の観測を平等に扱う新たな定式化．この線形ソルバはシンプルな構造ながら過去の手法と比較してモーション推定の精度を最大50%向上させ，非線形ソルバの反復回数も削減．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.06017\" dir=\"ltr\" href=\"https://t.co/yBoso8QqrK\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.06017\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.06017</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/IYHoycp0k8\">pic.twitter.com/IYHoycp0k8</a></p>',\n",
       "  'timestamp': '2020-06-19T03:14:48',\n",
       "  'timestamp_epochs': 1592536488,\n",
       "  'tweet_id': '1273816472230363138',\n",
       "  'tweet_url': '/slam_hub/status/1273816472230363138',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/Eawp7NpUEAAGRP1.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 25,\n",
       "  'links': ['https://arxiv.org/abs/2002.10876'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 12,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'End-to-endに学習可能な点群データのAugmentorを提案．入力点群ごとに全体の変形量と個々の点の変位量を出力し，分類器にとってより難しい変換となるよう敵対的に学習する．複数のモデルでランダムな水増しより良い精度を達成．\\nhttps://arxiv.org/abs/2002.10876\\xa0pic.twitter.com/Zso2mevGNk',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">End-to-endに学習可能な点群データのAugmentorを提案．入力点群ごとに全体の変形量と個々の点の変位量を出力し，分類器にとってより難しい変換となるよう敵対的に学習する．複数のモデルでランダムな水増しより良い精度を達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2002.10876\" dir=\"ltr\" href=\"https://t.co/hZ11z2y07W\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2002.10876\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2002.10876</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Zso2mevGNk\">pic.twitter.com/Zso2mevGNk</a></p>',\n",
       "  'timestamp': '2020-06-18T03:00:00',\n",
       "  'timestamp_epochs': 1592449200,\n",
       "  'tweet_id': '1273450361563435008',\n",
       "  'tweet_url': '/slam_hub/status/1273450361563435008',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EasGz34WsAAWoBV.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 59,\n",
       "  'links': ['https://www.sciencedirect.com/science/article/pii/S0921889019305202'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 24,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'LIDARスキャンを基に衛星画像上での自己位置推定を行う手法を提案．推定にはパーティクルフィルタを用い，各パーティクル位置の衛星画像とLIDARスキャンの一致度を測るネットワークによって評価する．衛星画像上での遮蔽・陰影に頑強な位置推定が可能であることを示した．\\nhttps://www.sciencedirect.com/science/article/pii/S0921889019305202\\xa0…pic.twitter.com/4PelxvyMge',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">LIDARスキャンを基に衛星画像上での自己位置推定を行う手法を提案．推定にはパーティクルフィルタを用い，各パーティクル位置の衛星画像とLIDARスキャンの一致度を測るネットワークによって評価する．衛星画像上での遮蔽・陰影に頑強な位置推定が可能であることを示した．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://www.sciencedirect.com/science/article/pii/S0921889019305202\" dir=\"ltr\" href=\"https://t.co/eGNRSt8eYd\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.sciencedirect.com/science/article/pii/S0921889019305202\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">sciencedirect.com/science/articl</span><span class=\"invisible\">e/pii/S0921889019305202</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/4PelxvyMge\">pic.twitter.com/4PelxvyMge</a></p>',\n",
       "  'timestamp': '2020-06-17T05:02:41',\n",
       "  'timestamp_epochs': 1592370161,\n",
       "  'tweet_id': '1273118847114018816',\n",
       "  'tweet_url': '/slam_hub/status/1273118847114018816',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 23,\n",
       "  'links': ['https://youtu.be/2ck5_sToayc'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '三次元点群の透視投影画像でセマンティックセグメンテーションを行い、分割結果が反応したBEV画像から物体を検出する、シンプルかつ高効率な2-stage検出手法を提案。既存手法と精度の差が大きくない上で、組み込みGPUでもマルチクラス物体検出と道路の分割を150 FPSで実現。https://youtu.be/2ck5_sToayc\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">三次元点群の透視投影画像でセマンティックセグメンテーションを行い、分割結果が反応したBEV画像から物体を検出する、シンプルかつ高効率な2-stage検出手法を提案。既存手法と精度の差が大きくない上で、組み込みGPUでもマルチクラス物体検出と道路の分割を150 FPSで実現。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/2ck5_sToayc\" dir=\"ltr\" href=\"https://t.co/gBVdHuZ0iD\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/2ck5_sToayc\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/2ck5_sToayc</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-16T03:00:00',\n",
       "  'timestamp_epochs': 1592276400,\n",
       "  'tweet_id': '1272725587359076352',\n",
       "  'tweet_url': '/slam_hub/status/1272725587359076352',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EahWh7MU0AELAKE.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 34,\n",
       "  'links': ['https://arxiv.org/abs/1910.04871'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 10,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '広域なLiDAR地図における単眼カメラの大域位置同定．異種のデータに対するShared embedding spaceを獲得するため，2D-CNNと3D-DNNを一緒に学習する枠組みを提案．同種データ内で完結するSame-Modality lossに加え，異種データ間でCross-Modality lossを用いて学習を行った．\\nhttps://arxiv.org/abs/1910.04871\\xa0pic.twitter.com/0Ft0zuZoH8',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">広域なLiDAR地図における単眼カメラの大域位置同定．異種のデータに対するShared embedding spaceを獲得するため，2D-CNNと3D-DNNを一緒に学習する枠組みを提案．同種データ内で完結するSame-Modality lossに加え，異種データ間でCross-Modality lossを用いて学習を行った．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1910.04871\" dir=\"ltr\" href=\"https://t.co/2hd3I64yez\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1910.04871\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1910.04871</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/0Ft0zuZoH8\">pic.twitter.com/0Ft0zuZoH8</a></p>',\n",
       "  'timestamp': '2020-06-15T03:00:01',\n",
       "  'timestamp_epochs': 1592190001,\n",
       "  'tweet_id': '1272363203498930176',\n",
       "  'tweet_url': '/slam_hub/status/1272363203498930176',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EacObyPUwAAwARv.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 311,\n",
       "  'links': ['https://arxiv.org/abs/2002.06289'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 87,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '建物、部屋、物体などの関係を表すシーングラフを動的環境に拡張し、3D Dynamic Scene Graphs を提案。Visual-Inertial SLAM の Kimera を用いてセマンティックマッピング。さらに移動物体（人のメッシュモデル）をトラッキングして、時空間の物体モデル構造を階層的に表現。\\nhttps://arxiv.org/abs/2002.06289\\xa0pic.twitter.com/F5yOdQlh7F',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">建物、部屋、物体などの関係を表すシーングラフを動的環境に拡張し、3D Dynamic Scene Graphs を提案。Visual-Inertial SLAM の Kimera を用いてセマンティックマッピング。さらに移動物体（人のメッシュモデル）をトラッキングして、時空間の物体モデル構造を階層的に表現。\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2002.06289\" dir=\"ltr\" href=\"https://t.co/lth2ct5vmr\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2002.06289\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2002.06289</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/F5yOdQlh7F\">pic.twitter.com/F5yOdQlh7F</a></p>',\n",
       "  'timestamp': '2020-06-14T03:02:26',\n",
       "  'timestamp_epochs': 1592103746,\n",
       "  'tweet_id': '1272001420745535489',\n",
       "  'tweet_url': '/slam_hub/status/1272001420745535489',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 27,\n",
       "  'links': ['https://youtu.be/co7y6LQ7Kqc'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Voxel ベースの U-Net でsemanticsを推定し，Super-voxel間の類似度を計算して3D instance segmentationを実現．U-Netで各Instanceに対するVoxelの占有数(＝体積)を推定し，適切にSuper-voxelをクラスタリングして Instance を生成．ScanNet Benchmark の現在１位．https://youtu.be/co7y6LQ7Kqc\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Voxel ベースの U-Net でsemanticsを推定し，Super-voxel間の類似度を計算して3D instance segmentationを実現．U-Netで各Instanceに対するVoxelの占有数(＝体積)を推定し，適切にSuper-voxelをクラスタリングして Instance を生成．ScanNet Benchmark の現在１位．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/co7y6LQ7Kqc\" dir=\"ltr\" href=\"https://t.co/spMx7HWkOv\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/co7y6LQ7Kqc\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/co7y6LQ7Kqc</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-13T03:53:49',\n",
       "  'timestamp_epochs': 1592020429,\n",
       "  'tweet_id': '1271651964275683329',\n",
       "  'tweet_url': '/slam_hub/status/1271651964275683329',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 22,\n",
       "  'links': ['https://www.youtube.com/watch?v=D0JObXCfxv0'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'RGB-D単眼画像を入力とし，視点を変えると発生する空白領域をインペインティングするモデルの提案．Depthの断層と層状のDepth表現という着想をベースに，各層で背景を外側へ補完するようにRGB-Dを推定．Mesh表現に変換することで，エッジデバイスでも軽快に動作可能．https://www.youtube.com/watch?v=D0JObXCfxv0\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">RGB-D単眼画像を入力とし，視点を変えると発生する空白領域をインペインティングするモデルの提案．Depthの断層と層状のDepth表現という着想をベースに，各層で背景を外側へ補完するようにRGB-Dを推定．Mesh表現に変換することで，エッジデバイスでも軽快に動作可能．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=D0JObXCfxv0\" dir=\"ltr\" href=\"https://t.co/WraXotGqOh\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=D0JObXCfxv0\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=D0JObX</span><span class=\"invisible\">Cfxv0</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-06-12T03:00:01',\n",
       "  'timestamp_epochs': 1591930801,\n",
       "  'tweet_id': '1271276037117218816',\n",
       "  'tweet_url': '/slam_hub/status/1271276037117218816',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EaE9eV9UwAEPo9z.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 21,\n",
       "  'links': ['https://arxiv.org/abs/2006.03586'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '未知物体を撮影した画像間の相対姿勢の推定. 学習に用いられた物体以外の視点の推定は困難であったが２枚の画像を3D特徴グリッドにマッピングする学習を行い位置を合わせることで相対的な位置を推定する.学習時と大きく異なる物体で推論する際に従来手法より良い精度を示した.\\nhttps://arxiv.org/abs/2006.03586\\xa0pic.twitter.com/SktdplZMb1',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">未知物体を撮影した画像間の相対姿勢の推定. 学習に用いられた物体以外の視点の推定は困難であったが２枚の画像を3D特徴グリッドにマッピングする学習を行い位置を合わせることで相対的な位置を推定する.学習時と大きく異なる物体で推論する際に従来手法より良い精度を示した.\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.03586\" dir=\"ltr\" href=\"https://t.co/5jHRl8RlHM\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.03586\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.03586</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/SktdplZMb1\">pic.twitter.com/SktdplZMb1</a></p>',\n",
       "  'timestamp': '2020-06-10T03:00:00',\n",
       "  'timestamp_epochs': 1591758000,\n",
       "  'tweet_id': '1270551260056387585',\n",
       "  'tweet_url': '/slam_hub/status/1270551260056387585',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 39,\n",
       "  'links': ['https://youtu.be/sq2hhkHgtb0'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 15,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '複数カメラで撮影された動的イベントに対して，視点と時間を移動可能な4次元時空間可視化を行うシステム．シーン特化のself-supervisedなCNNを用いて静的・動的部分の抽出を行う．SfMによる既存手法で困難であった非ランバート面や，テクスチャレスな領域もキャプチャ可能に．https://youtu.be/sq2hhkHgtb0\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">複数カメラで撮影された動的イベントに対して，視点と時間を移動可能な4次元時空間可視化を行うシステム．シーン特化のself-supervisedなCNNを用いて静的・動的部分の抽出を行う．SfMによる既存手法で困難であった非ランバート面や，テクスチャレスな領域もキャプチャ可能に．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/sq2hhkHgtb0\" dir=\"ltr\" href=\"https://t.co/0JndxLnDbB\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/sq2hhkHgtb0\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/sq2hhkHgtb0</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-09T03:00:00',\n",
       "  'timestamp_epochs': 1591671600,\n",
       "  'tweet_id': '1270188872232660993',\n",
       "  'tweet_url': '/slam_hub/status/1270188872232660993',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': ['首里城', 'OUR_Shurijo'],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 4,\n",
       "  'links': ['https://our-shurijo.org'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'our_shurijo',\n",
       "  'text': 'OUR Shurijo reports.\\n2020/06/08 までの合計: 3134 名、44435枚\\nData collected by 2020/06/08: 44435 images from 3134 people.\\n\\nhttps://our-shurijo.org\\xa0\\n#首里城\\n#OUR_Shurijo',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">OUR Shurijo reports.\\n2020/06/08 までの合計: 3134 名、44435枚\\nData collected by 2020/06/08: 44435 images from 3134 people.\\n\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://our-shurijo.org\" dir=\"ltr\" href=\"https://t.co/AVLAQOkAIp\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://our-shurijo.org\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">our-shurijo.org</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a>\\n<a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/%E9%A6%96%E9%87%8C%E5%9F%8E?src=hash\"><s>#</s><b>首里城</b></a>\\n<a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/OUR_Shurijo?src=hash\"><s>#</s><b>OUR_Shurijo</b></a></p>',\n",
       "  'timestamp': '2020-06-08T09:00:02',\n",
       "  'timestamp_epochs': 1591606802,\n",
       "  'tweet_id': '1269917087310336000',\n",
       "  'tweet_url': '/our_shurijo/status/1269917087310336000',\n",
       "  'user_id': '1191719410924285958',\n",
       "  'username': 'OUR Shurijo: みんなの首里城デジタル復元プロジェクト',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EZ6nHJqUwAA9OC7.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 35,\n",
       "  'links': ['https://arxiv.org/abs/2006.00187'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '点群が平面状に分布する制約を加えたPlaner Bundle Adjustmentを提案．ヤコビアン行列のコンパクトな表現を含む新たな定式化によって精度向上と計算量の削減を両立．評価実験で同問題設定のSOTAと比較して高速，高精度に，そして初期値にロバストなことが示された．\\nhttps://arxiv.org/abs/2006.00187\\xa0pic.twitter.com/rVKDwkK3LH',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">点群が平面状に分布する制約を加えたPlaner Bundle Adjustmentを提案．ヤコビアン行列のコンパクトな表現を含む新たな定式化によって精度向上と計算量の削減を両立．評価実験で同問題設定のSOTAと比較して高速，高精度に，そして初期値にロバストなことが示された．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2006.00187\" dir=\"ltr\" href=\"https://t.co/7o2kH77R3Y\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2006.00187\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2006.00187</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/rVKDwkK3LH\">pic.twitter.com/rVKDwkK3LH</a></p>',\n",
       "  'timestamp': '2020-06-08T03:00:00',\n",
       "  'timestamp_epochs': 1591585200,\n",
       "  'tweet_id': '1269826483599618049',\n",
       "  'tweet_url': '/slam_hub/status/1269826483599618049',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EZ483GVU0AEdfeG.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 44,\n",
       "  'links': ['https://arxiv.org/abs/2003.01251'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'GNNを用いた三次元点群からの物体検出手法を提案．近傍点を結んだグラフからGNNで特徴抽出し，点ごとに所属する物体クラスとBBOXを推定．最後に重複したBBOXを中央値で統合する．KITTTIデータセットで従来手法を上回る精度を達成．\\nhttps://arxiv.org/abs/2003.01251\\xa0pic.twitter.com/Qu1jOIW3xo',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">GNNを用いた三次元点群からの物体検出手法を提案．近傍点を結んだグラフからGNNで特徴抽出し，点ごとに所属する物体クラスとBBOXを推定．最後に重複したBBOXを中央値で統合する．KITTTIデータセットで従来手法を上回る精度を達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.01251\" dir=\"ltr\" href=\"https://t.co/syhBOz5Pnb\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.01251\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.01251</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Qu1jOIW3xo\">pic.twitter.com/Qu1jOIW3xo</a></p>',\n",
       "  'timestamp': '2020-06-07T06:39:37',\n",
       "  'timestamp_epochs': 1591511977,\n",
       "  'tweet_id': '1269519361787613184',\n",
       "  'tweet_url': '/slam_hub/status/1269519361787613184',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 25,\n",
       "  'links': ['https://youtu.be/lE5gjzRKWuA'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '6DoF物体姿勢推定のためのRBPFを提案．姿勢分布を分解し，平行移動はサンプリング，回転は物体の各回転に対するEmbeddingを予め計算しておき，パーティクルのEmbeddingをこれと比較することで評価．6DoFを200パーティクル程度でロバストに推定しSOTA精度．https://youtu.be/lE5gjzRKWuA\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">6DoF物体姿勢推定のためのRBPFを提案．姿勢分布を分解し，平行移動はサンプリング，回転は物体の各回転に対するEmbeddingを予め計算しておき，パーティクルのEmbeddingをこれと比較することで評価．6DoFを200パーティクル程度でロバストに推定しSOTA精度．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/lE5gjzRKWuA\" dir=\"ltr\" href=\"https://t.co/JQVTpDo6g2\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/lE5gjzRKWuA\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/lE5gjzRKWuA</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-06T03:10:29',\n",
       "  'timestamp_epochs': 1591413029,\n",
       "  'tweet_id': '1269104347004022785',\n",
       "  'tweet_url': '/slam_hub/status/1269104347004022785',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 5,\n",
       "  'links': ['https://youtu.be/6OoRZrqfSJ4'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '安定的にフレキシブルな時系列点群の生成手法を提案。既存手法の安定性と多様性の両立し難い問題を改善するため、新たなTemporal Lossを導入、点群から時間的一貫性がある特徴を学習し、変形可能な数が多い点群にたしても有効性を示す。https://youtu.be/6OoRZrqfSJ4\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">安定的にフレキシブルな時系列点群の生成手法を提案。既存手法の安定性と多様性の両立し難い問題を改善するため、新たなTemporal Lossを導入、点群から時間的一貫性がある特徴を学習し、変形可能な数が多い点群にたしても有効性を示す。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/6OoRZrqfSJ4\" dir=\"ltr\" href=\"https://t.co/x55Hz7RTv0\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/6OoRZrqfSJ4\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/6OoRZrqfSJ4</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-05T03:00:00',\n",
       "  'timestamp_epochs': 1591326000,\n",
       "  'tweet_id': '1268739321369989121',\n",
       "  'tweet_url': '/slam_hub/status/1268739321369989121',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EZo3gFiUcAAONHP.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 26,\n",
       "  'links': ['https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 11,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '多視点のRGB画像列に基づく物体の3次元復元手法を提案．Point / Surface による相補的な形状デコードにより形状表現の効率性と記述力を両立させており，より高速な形状復元を実現している．\\nhttps://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\\xa0…pic.twitter.com/Y0iFz7omo2',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">多視点のRGB画像列に基づく物体の3次元復元手法を提案．Point / Surface による相補的な形状デコードにより形状表現の効率性と記述力を両立させており，より高速な形状復元を実現している．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\" dir=\"ltr\" href=\"https://t.co/zRWPmf1dxo\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">research.fb.com/wp-content/upl</span><span class=\"invisible\">oads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Y0iFz7omo2\">pic.twitter.com/Y0iFz7omo2</a></p>',\n",
       "  'timestamp': '2020-06-04T03:42:02',\n",
       "  'timestamp_epochs': 1591242122,\n",
       "  'tweet_id': '1268387507969785856',\n",
       "  'tweet_url': '/slam_hub/status/1268387507969785856',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 45,\n",
       "  'links': ['https://youtu.be/YTfliBco6aw'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 11,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'SLAM で重要なループ検出を、位置合わせなしで end-to-end に実現。3D-LIDAR の距離画像、法線、受光強度、セマンティクスを入力。2つのスキャンの重複率とヨー角を推定。ループ拘束は SLAM 側で求める。重なりが小さくても適切にループ検出し、SuMa より高精度な地図を構築。https://youtu.be/YTfliBco6aw\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">SLAM で重要なループ検出を、位置合わせなしで end-to-end に実現。3D-LIDAR の距離画像、法線、受光強度、セマンティクスを入力。2つのスキャンの重複率とヨー角を推定。ループ拘束は SLAM 側で求める。重なりが小さくても適切にループ検出し、SuMa より高精度な地図を構築。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/YTfliBco6aw\" dir=\"ltr\" href=\"https://t.co/Dg45OQuByN\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/YTfliBco6aw\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/YTfliBco6aw</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-03T02:57:16',\n",
       "  'timestamp_epochs': 1591153036,\n",
       "  'tweet_id': '1268013854698557440',\n",
       "  'tweet_url': '/slam_hub/status/1268013854698557440',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 34,\n",
       "  'links': ['http://youtu.be/Bb92aMBJR44'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 18,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'SLAMを構成要素(入出力, フロント/バックエンド, マップストレージ)に分割し，センサ種類・個数，マッピング方式(global map vs local submaps)，状態空間(SE2/SE3/SE3+vel)などの違いを包括的に扱えるミドルウェア寄りのライブラリを提案．http://youtu.be/Bb92aMBJR44\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">SLAMを構成要素(入出力, フロント/バックエンド, マップストレージ)に分割し，センサ種類・個数，マッピング方式(global map vs local submaps)，状態空間(SE2/SE3/SE3+vel)などの違いを包括的に扱えるミドルウェア寄りのライブラリを提案．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"http://youtu.be/Bb92aMBJR44\" dir=\"ltr\" href=\"https://t.co/RviYrazPwz\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://youtu.be/Bb92aMBJR44\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">http://</span><span class=\"js-display-url\">youtu.be/Bb92aMBJR44</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-06-02T07:59:16',\n",
       "  'timestamp_epochs': 1591084756,\n",
       "  'tweet_id': '1267727467101315072',\n",
       "  'tweet_url': '/slam_hub/status/1267727467101315072',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EZVwVrFUMAIJfoT.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 50,\n",
       "  'links': ['https://arxiv.org/abs/2004.01314'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 14,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'オプティカルフローを経由して8点法により直接推定した相対姿勢，さらにそこから計算した３次元点を自己教師とすることで，スケールの推定をネットワークから分離し，高い汎化性能とスケールの一貫性を実現．屋内外のデータセットでORB-SLAMや学習ベースの手法を凌駕．\\nhttps://arxiv.org/abs/2004.01314\\xa0pic.twitter.com/XU3nSsMtX4',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">オプティカルフローを経由して8点法により直接推定した相対姿勢，さらにそこから計算した３次元点を自己教師とすることで，スケールの推定をネットワークから分離し，高い汎化性能とスケールの一貫性を実現．屋内外のデータセットでORB-SLAMや学習ベースの手法を凌駕．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.01314\" dir=\"ltr\" href=\"https://t.co/Sv5f9jPQjC\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.01314\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.01314</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/XU3nSsMtX4\">pic.twitter.com/XU3nSsMtX4</a></p>',\n",
       "  'timestamp': '2020-06-01T03:00:00',\n",
       "  'timestamp_epochs': 1590980400,\n",
       "  'tweet_id': '1267289768544460800',\n",
       "  'tweet_url': '/slam_hub/status/1267289768544460800',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EZRdRrBVcAckBzq.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 36,\n",
       "  'links': ['https://arxiv.org/abs/2003.01360'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 12,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Unsupervised単眼Depth推定で精度を改善する2つの機構の提案．特定のDepth誤りを，フォトメトリックエラーをもとにした外れ値Maskを導入し対処．また，重み付きマルチスケール機構でアーティファクトを除去．簡単に追加できる機構で，他手法よりも高い精度を達成．\\nhttps://arxiv.org/abs/2003.01360\\xa0pic.twitter.com/I0JA9KV4Lo',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Unsupervised単眼Depth推定で精度を改善する2つの機構の提案．特定のDepth誤りを，フォトメトリックエラーをもとにした外れ値Maskを導入し対処．また，重み付きマルチスケール機構でアーティファクトを除去．簡単に追加できる機構で，他手法よりも高い精度を達成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.01360\" dir=\"ltr\" href=\"https://t.co/jSEFUaRTgP\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.01360\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.01360</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/I0JA9KV4Lo\">pic.twitter.com/I0JA9KV4Lo</a></p>',\n",
       "  'timestamp': '2020-05-31T03:00:00',\n",
       "  'timestamp_epochs': 1590894000,\n",
       "  'tweet_id': '1266927379089248256',\n",
       "  'tweet_url': '/slam_hub/status/1266927379089248256',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 22,\n",
       "  'links': ['https://youtu.be/GuLzjnFGDKs'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Volumetricな表現で使用されるTSDFとそれに対応したテクスチャを圧縮する新しい方法を提案．End-to-Endで訓練されたニューラルネットを用い，トポロジカルなエラーを防ぐためにTSDFの符号を失わずに圧縮する．従来手法より優れた圧縮率と歪みのトレードオフを得た．https://youtu.be/GuLzjnFGDKs\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Volumetricな表現で使用されるTSDFとそれに対応したテクスチャを圧縮する新しい方法を提案．End-to-Endで訓練されたニューラルネットを用い，トポロジカルなエラーを防ぐためにTSDFの符号を失わずに圧縮する．従来手法より優れた圧縮率と歪みのトレードオフを得た．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/GuLzjnFGDKs\" dir=\"ltr\" href=\"https://t.co/8LghjPLFgB\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/GuLzjnFGDKs\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/GuLzjnFGDKs</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-30T03:00:00',\n",
       "  'timestamp_epochs': 1590807600,\n",
       "  'tweet_id': '1266564992805076997',\n",
       "  'tweet_url': '/slam_hub/status/1266564992805076997',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EZJizyPUYAE03aP.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 47,\n",
       "  'links': ['https://arxiv.org/abs/2005.11052'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 2,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 11,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'モーションセグメンテーション，動的物体追跡，カメラ姿勢，シーン剛体の姿勢変化や速度の計算を全て行い，実世界の屋外シナリオで実証可能な世界初の動的SLAMシステムを提案．ロバスト性の向上の為，カメラと物体の動きの推定はOptical Flowの改良と合わせて因子グラフ最適化\\nhttps://arxiv.org/abs/2005.11052\\xa0pic.twitter.com/esR5t04wPi',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">モーションセグメンテーション，動的物体追跡，カメラ姿勢，シーン剛体の姿勢変化や速度の計算を全て行い，実世界の屋外シナリオで実証可能な世界初の動的SLAMシステムを提案．ロバスト性の向上の為，カメラと物体の動きの推定はOptical Flowの改良と合わせて因子グラフ最適化\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2005.11052\" dir=\"ltr\" href=\"https://t.co/gLqDF3c19Y\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2005.11052\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2005.11052</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/esR5t04wPi\">pic.twitter.com/esR5t04wPi</a></p>',\n",
       "  'timestamp': '2020-05-29T03:00:00',\n",
       "  'timestamp_epochs': 1590721200,\n",
       "  'tweet_id': '1266202604868075520',\n",
       "  'tweet_url': '/slam_hub/status/1266202604868075520',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 26,\n",
       "  'links': ['https://youtu.be/Zttl3eDjNyc'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像の特徴点検出と記述子表現をend-to-endに学習する手法を提案．特徴点マッチングで誤差伝搬できないため，画像間の相対姿勢誤差を負の報酬ににした強化学習で特徴点検出CNNと記述子推定CNNをトレーニングする．学習ベースの局所特徴量抽出器としてSOTAを達成．https://youtu.be/Zttl3eDjNyc\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像の特徴点検出と記述子表現をend-to-endに学習する手法を提案．特徴点マッチングで誤差伝搬できないため，画像間の相対姿勢誤差を負の報酬ににした強化学習で特徴点検出CNNと記述子推定CNNをトレーニングする．学習ベースの局所特徴量抽出器としてSOTAを達成．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/Zttl3eDjNyc\" dir=\"ltr\" href=\"https://t.co/HWkH0lPWpt\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/Zttl3eDjNyc\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/Zttl3eDjNyc</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-28T03:38:11',\n",
       "  'timestamp_epochs': 1590637091,\n",
       "  'tweet_id': '1265849825678913537',\n",
       "  'tweet_url': '/slam_hub/status/1265849825678913537',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 27,\n",
       "  'links': ['https://www.youtube.com/watch?v=JHz_ImeI8HE'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 1,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '微分可能な三次元点群のサンプリング手法を提案．NNによって入力点群を簡素化するDovratらの手法を拡張．最近傍サンプリングをk近傍の重み付き和で近似することで，簡素化した点群を基に入力点群をサンプリングするステップを微分可能にした．https://www.youtube.com/watch?v=JHz_ImeI8HE\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">微分可能な三次元点群のサンプリング手法を提案．NNによって入力点群を簡素化するDovratらの手法を拡張．最近傍サンプリングをk近傍の重み付き和で近似することで，簡素化した点群を基に入力点群をサンプリングするステップを微分可能にした．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=JHz_ImeI8HE\" dir=\"ltr\" href=\"https://t.co/6fpS3ke082\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=JHz_ImeI8HE\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=JHz_Im</span><span class=\"invisible\">eI8HE</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-05-27T03:00:00',\n",
       "  'timestamp_epochs': 1590548400,\n",
       "  'tweet_id': '1265477828973416448',\n",
       "  'tweet_url': '/slam_hub/status/1265477828973416448',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 40,\n",
       "  'links': ['https://youtu.be/a5JWe6mOAEs'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 14,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'RGB-D画像から特定の物体の6DoF姿勢を3段階で推定．1.2D物体認識で対象物を含む点群を抽出．2.PointNetを利用し詳細な物体抽出と並進量を推定．3.並進後，回転量をPointNetで推定．回転量推定の学習には，各点に付加した方向ベクトルが真値の方向になるように学習．https://youtu.be/a5JWe6mOAEs\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">RGB-D画像から特定の物体の6DoF姿勢を3段階で推定．1.2D物体認識で対象物を含む点群を抽出．2.PointNetを利用し詳細な物体抽出と並進量を推定．3.並進後，回転量をPointNetで推定．回転量推定の学習には，各点に付加した方向ベクトルが真値の方向になるように学習．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/a5JWe6mOAEs\" dir=\"ltr\" href=\"https://t.co/2qzAZcXPf8\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/a5JWe6mOAEs\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/a5JWe6mOAEs</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-26T03:18:24',\n",
       "  'timestamp_epochs': 1590463104,\n",
       "  'tweet_id': '1265120071728283648',\n",
       "  'tweet_url': '/slam_hub/status/1265120071728283648',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': ['openvslam'],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 21,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'sakuDken',\n",
       "  'text': '【お願い】OpenVSLAMに関する使い方や仕様等の質問のみに関しては，他の方々と情報を共有できるGitHubのissueやSlackなどへお願いいたします．（問い合わせが多く個別に対応するのが難しい状況です） #openvslam',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">【お願い】OpenVSLAMに関する使い方や仕様等の質問のみに関しては，他の方々と情報を共有できるGitHubのissueやSlackなどへお願いいたします．（問い合わせが多く個別に対応するのが難しい状況です） <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/openvslam?src=hash\"><s>#</s><b>openvslam</b></a></p>',\n",
       "  'timestamp': '2020-05-25T05:01:27',\n",
       "  'timestamp_epochs': 1590382887,\n",
       "  'tweet_id': '1264783617554673664',\n",
       "  'tweet_url': '/sakuDken/status/1264783617554673664',\n",
       "  'user_id': '425816448',\n",
       "  'username': 'Ken Sakurada',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYw3pD7U4AESaXj.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 50,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像からgeometric、semantic、textureのVote特徴を抽出、三次元点群のVote特徴と融合し、3D物体検出の手法を提案。Multi-modalデータ融合を改善するにmulti-towerとgradient blendingの構造を使用し、SUNRGB-Dで既存SOTAより5.7mAPの精度を向上させ、SLAMようなSparse点群に対する有効性も確認。pic.twitter.com/6zfURbMEPw',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像からgeometric、semantic、textureのVote特徴を抽出、三次元点群のVote特徴と融合し、3D物体検出の手法を提案。Multi-modalデータ融合を改善するにmulti-towerとgradient blendingの構造を使用し、SUNRGB-Dで既存SOTAより5.7mAPの精度を向上させ、SLAMようなSparse点群に対する有効性も確認。<a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/6zfURbMEPw\">pic.twitter.com/6zfURbMEPw</a></p>',\n",
       "  'timestamp': '2020-05-25T03:00:00',\n",
       "  'timestamp_epochs': 1590375600,\n",
       "  'tweet_id': '1264753052122308615',\n",
       "  'tweet_url': '/slam_hub/status/1264753052122308615',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYwEihiU8AAOrk6.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 44,\n",
       "  'links': ['https://arxiv.org/abs/2005.07457'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'RGB-Dカメラ等で計測された3次元点群における幾何プリミティブ(円柱，円錐，球)の検出．Point Pair Feature に各幾何プリミティブの形状に応じた拘束を導入，またLinear interpolation votingを幾何プリミティブ用に改良し，ハフ変換による低計算量での頑健な検出を実現．\\nhttps://arxiv.org/abs/2005.07457\\xa0pic.twitter.com/AylPuLfTAw',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">RGB-Dカメラ等で計測された3次元点群における幾何プリミティブ(円柱，円錐，球)の検出．Point Pair Feature に各幾何プリミティブの形状に応じた拘束を導入，またLinear interpolation votingを幾何プリミティブ用に改良し，ハフ変換による低計算量での頑健な検出を実現．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2005.07457\" dir=\"ltr\" href=\"https://t.co/1Hd5Ycests\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2005.07457\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2005.07457</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/AylPuLfTAw\">pic.twitter.com/AylPuLfTAw</a></p>',\n",
       "  'timestamp': '2020-05-24T03:00:36',\n",
       "  'timestamp_epochs': 1590289236,\n",
       "  'tweet_id': '1264390814454607872',\n",
       "  'tweet_url': '/slam_hub/status/1264390814454607872',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 197,\n",
       "  'links': ['https://www.slideshare.net/KenSakurada/126-rsj2020522'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 71,\n",
       "  'screen_name': 'sakuDken',\n",
       "  'text': '本日のRSJセミナーの講演資料です． Visual SLAMと深層学習を用いた３Dモデリングについて，私たち（@sumicco_cv ，@mikiya85 ）の開発するOpenVSLAMを交えて紹介しています．（OpenVSLAMは先日ROS2に対応し，皆様のおかげでスター数は2,300を超えました！）https://www.slideshare.net/KenSakurada/126-rsj2020522\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">本日のRSJセミナーの講演資料です． Visual SLAMと深層学習を用いた３Dモデリングについて，私たち（<a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"1001107628120883206\" dir=\"ltr\" href=\"/sumicco_cv\"><s>@</s><b>sumicco_cv</b></a> ，<a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"882022760532262912\" dir=\"ltr\" href=\"/mikiya85\"><s>@</s><b>mikiya85</b></a> ）の開発するOpenVSLAMを交えて紹介しています．（OpenVSLAMは先日ROS2に対応し，皆様のおかげでスター数は2,300を超えました！）<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.slideshare.net/KenSakurada/126-rsj2020522\" dir=\"ltr\" href=\"https://t.co/HIWIX47yhE\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.slideshare.net/KenSakurada/126-rsj2020522\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">slideshare.net/KenSakurada/12</span><span class=\"invisible\">6-rsj2020522</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-05-22T09:02:55',\n",
       "  'timestamp_epochs': 1590138175,\n",
       "  'tweet_id': '1263757220807729153',\n",
       "  'tweet_url': '/sakuDken/status/1263757220807729153',\n",
       "  'user_id': '425816448',\n",
       "  'username': 'Ken Sakurada',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYl1RBmU8AIEVoz.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 34,\n",
       "  'links': ['https://arxiv.org/abs/2005.01014'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '二つの点群について，特徴空間上で並進・回転に対するヤコビアンを数値微分で求めLucus-Kanade法で位置合わせを実行．また点群に対するEncoder-Decorderを構築し特徴をUn-supervisedまたはSemi-supervisedで学習を可能にした．\\nhttps://arxiv.org/abs/2005.01014\\xa0pic.twitter.com/JZeCcd11hS',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">二つの点群について，特徴空間上で並進・回転に対するヤコビアンを数値微分で求めLucus-Kanade法で位置合わせを実行．また点群に対するEncoder-Decorderを構築し特徴をUn-supervisedまたはSemi-supervisedで学習を可能にした．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2005.01014\" dir=\"ltr\" href=\"https://t.co/KOQ6i1p0Nx\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2005.01014\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2005.01014</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/JZeCcd11hS\">pic.twitter.com/JZeCcd11hS</a></p>',\n",
       "  'timestamp': '2020-05-22T03:16:51',\n",
       "  'timestamp_epochs': 1590117411,\n",
       "  'tweet_id': '1263670128400404480',\n",
       "  'tweet_url': '/slam_hub/status/1263670128400404480',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 24,\n",
       "  'links': ['https://www.youtube.com/watch?v=xIHCyyaB5gU'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'self-supervisedに学習可能な，画像のオクルージョン領域を復元するモデルの提案．物体ごとの被オクルージョン領域推定で，増加範囲からオクルージョン関係のグラフを構築．その後推定した領域MaskからRGBを復元．教師あり学習に匹敵するパフォーマンスを達成．https://www.youtube.com/watch?v=xIHCyyaB5gU\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">self-supervisedに学習可能な，画像のオクルージョン領域を復元するモデルの提案．物体ごとの被オクルージョン領域推定で，増加範囲からオクルージョン関係のグラフを構築．その後推定した領域MaskからRGBを復元．教師あり学習に匹敵するパフォーマンスを達成．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=xIHCyyaB5gU\" dir=\"ltr\" href=\"https://t.co/kw7zUYlQDH\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=xIHCyyaB5gU\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=xIHCyy</span><span class=\"invisible\">aB5gU</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-05-21T03:00:00',\n",
       "  'timestamp_epochs': 1590030000,\n",
       "  'tweet_id': '1263303500836069376',\n",
       "  'tweet_url': '/slam_hub/status/1263303500836069376',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYX8UGzUcAErmZZ.png',\n",
       "   'https://pbs.twimg.com/media/EYX8UySU0AEph5J.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 30,\n",
       "  'links': ['https://arxiv.org/abs/2005.06136'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '自己教師Visual Odometry手法を提案．畳み込みLSTMを利用することで過去の経験を活かして推定をし，未知のシーンにオンラインで適応することが可能．さらにオープンワールドでの環境の変化に対応するために特徴量を揃える手法を提案．既存の手法を大きく上回ることを実験で確認https://arxiv.org/abs/2005.06136\\xa0pic.twitter.com/YMw3oYqEK3',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">自己教師Visual Odometry手法を提案．畳み込みLSTMを利用することで過去の経験を活かして推定をし，未知のシーンにオンラインで適応することが可能．さらにオープンワールドでの環境の変化に対応するために特徴量を揃える手法を提案．既存の手法を大きく上回ることを実験で確認<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2005.06136\" dir=\"ltr\" href=\"https://t.co/3dXw01lTvt\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2005.06136\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2005.06136</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/YMw3oYqEK3\">pic.twitter.com/YMw3oYqEK3</a></p>',\n",
       "  'timestamp': '2020-05-20T03:00:00',\n",
       "  'timestamp_epochs': 1589943600,\n",
       "  'tweet_id': '1262941112664117248',\n",
       "  'tweet_url': '/slam_hub/status/1262941112664117248',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYWkGGCUcAAMG4l.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 43,\n",
       "  'links': ['https://arxiv.org/abs/2004.03080'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像ベースの深度マップを擬似LiDARに変換し3次元物体検出するというパイプライン全体を，End-to-Endで学習するフレームワーク．既存手法では深度推定と物体検出で別学習していたが，間の表現変化をプーリングと量子化の工夫で微分可能にし実現．PointRCNNと組み合わせでSOTA.\\nhttps://arxiv.org/abs/2004.03080\\xa0pic.twitter.com/7TfZbZ079s',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像ベースの深度マップを擬似LiDARに変換し3次元物体検出するというパイプライン全体を，End-to-Endで学習するフレームワーク．既存手法では深度推定と物体検出で別学習していたが，間の表現変化をプーリングと量子化の工夫で微分可能にし実現．PointRCNNと組み合わせでSOTA.\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.03080\" dir=\"ltr\" href=\"https://t.co/zrhvXOQ8M1\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.03080\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.03080</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/7TfZbZ079s\">pic.twitter.com/7TfZbZ079s</a></p>',\n",
       "  'timestamp': '2020-05-19T04:08:00',\n",
       "  'timestamp_epochs': 1589861280,\n",
       "  'tweet_id': '1262595837172973569',\n",
       "  'tweet_url': '/slam_hub/status/1262595837172973569',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYN-WuXUcAABCpb.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 60,\n",
       "  'links': ['https://arxiv.org/abs/1911.11763'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 10,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像間の局所特徴量をマッチングするGNNを用いたアルゴリズムを提案．視点の大幅な違いにも適用可能．2種類のアテンション機構により画像内，画像間でユニークな特徴量を活用する．GPUでリアルタイム動作し，既存手法と比べ屋内外のシーンで大幅に性能向上．https://arxiv.org/abs/1911.11763\\xa0pic.twitter.com/4yuITowWG7',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像間の局所特徴量をマッチングするGNNを用いたアルゴリズムを提案．視点の大幅な違いにも適用可能．2種類のアテンション機構により画像内，画像間でユニークな特徴量を活用する．GPUでリアルタイム動作し，既存手法と比べ屋内外のシーンで大幅に性能向上．<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1911.11763\" dir=\"ltr\" href=\"https://t.co/b89grxHaIL\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1911.11763\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1911.11763</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/4yuITowWG7\">pic.twitter.com/4yuITowWG7</a></p>',\n",
       "  'timestamp': '2020-05-18T03:00:00',\n",
       "  'timestamp_epochs': 1589770800,\n",
       "  'tweet_id': '1262216336865976320',\n",
       "  'tweet_url': '/slam_hub/status/1262216336865976320',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYL4G6eUEAAfBx6.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 36,\n",
       "  'links': ['https://arxiv.org/abs/1912.08193'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'CGレンダリングにヒントを得たセグメンテーションの精緻化モジュールを提案．不確かな点をサンプリングし，MLPで推定し直すことで適応的に物体境界を精緻化．Mask-RCNNやFCNに取り付けることで，少ない計算コストで精度向上．\\nhttps://arxiv.org/abs/1912.08193\\xa0pic.twitter.com/KwQ6ASEwyf',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">CGレンダリングにヒントを得たセグメンテーションの精緻化モジュールを提案．不確かな点をサンプリングし，MLPで推定し直すことで適応的に物体境界を精緻化．Mask-RCNNやFCNに取り付けることで，少ない計算コストで精度向上．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1912.08193\" dir=\"ltr\" href=\"https://t.co/fMjzUb4Qlw\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1912.08193\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1912.08193</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/KwQ6ASEwyf\">pic.twitter.com/KwQ6ASEwyf</a></p>',\n",
       "  'timestamp': '2020-05-17T03:00:00',\n",
       "  'timestamp_epochs': 1589684400,\n",
       "  'tweet_id': '1261853950791135233',\n",
       "  'tweet_url': '/slam_hub/status/1261853950791135233',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EYG5FPiVcAECQWL.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 50,\n",
       "  'links': ['https://arxiv.org/abs/1908.01293'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 15,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像による自己位置推定の精度評価．Data-drivenな手法は，精度面でIndirect法等の従来手法に劣ることが通説となっている．本論文では，特徴量抽出や基礎行列計算等の各フェーズをハンドクラフトからData-drivenまで程度を変え，各組み合わせにおける精度を検証している．\\nhttps://arxiv.org/abs/1908.01293\\xa0pic.twitter.com/Rt4IfVGCGK',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像による自己位置推定の精度評価．Data-drivenな手法は，精度面でIndirect法等の従来手法に劣ることが通説となっている．本論文では，特徴量抽出や基礎行列計算等の各フェーズをハンドクラフトからData-drivenまで程度を変え，各組み合わせにおける精度を検証している．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1908.01293\" dir=\"ltr\" href=\"https://t.co/hBt6ZY5oii\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1908.01293\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1908.01293</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Rt4IfVGCGK\">pic.twitter.com/Rt4IfVGCGK</a></p>',\n",
       "  'timestamp': '2020-05-16T03:05:25',\n",
       "  'timestamp_epochs': 1589598325,\n",
       "  'tweet_id': '1261492927231975425',\n",
       "  'tweet_url': '/slam_hub/status/1261492927231975425',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 20,\n",
       "  'links': ['https://youtu.be/7hxGmMk4MZ0'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '微分可能なSinkhornレイヤーを使い、hybrid特徴から点と点のソフトな対応を取り、 誤対応やoverlapが少ない点群ペアでも対処できる学習ベースRobust Point Matching点群位置合わせ手法を提案。ModelNet40での実験結果で(rule-&learning-based)既存手法より優れた性能を示す。https://youtu.be/7hxGmMk4MZ0\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">微分可能なSinkhornレイヤーを使い、hybrid特徴から点と点のソフトな対応を取り、 誤対応やoverlapが少ない点群ペアでも対処できる学習ベースRobust Point Matching点群位置合わせ手法を提案。ModelNet40での実験結果で(rule-&amp;learning-based)既存手法より優れた性能を示す。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/7hxGmMk4MZ0\" dir=\"ltr\" href=\"https://t.co/IOXPnRNy6j\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/7hxGmMk4MZ0\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/7hxGmMk4MZ0</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-15T03:00:00',\n",
       "  'timestamp_epochs': 1589511600,\n",
       "  'tweet_id': '1261129175722799106',\n",
       "  'tweet_url': '/slam_hub/status/1261129175722799106',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 22,\n",
       "  'links': ['https://www.youtube.com/watch?v=h0bqURQlZGA'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '複数ロボット上で走るStereoVOを統合する分散グラフSLAMを提案．ロボット間ループ制約の中から一貫性を保つ最大集合を探す最大クリーク問題を解いて誤検出を除去し，積極的にループ追加を行う方針を採用．大量のループ誤検出を除去し，一貫した地図を生成できることを示した．https://www.youtube.com/watch?v=h0bqURQlZGA\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">複数ロボット上で走るStereoVOを統合する分散グラフSLAMを提案．ロボット間ループ制約の中から一貫性を保つ最大集合を探す最大クリーク問題を解いて誤検出を除去し，積極的にループ追加を行う方針を採用．大量のループ誤検出を除去し，一貫した地図を生成できることを示した．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=h0bqURQlZGA\" dir=\"ltr\" href=\"https://t.co/O1MsU1dgWS\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=h0bqURQlZGA\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=h0bqUR</span><span class=\"invisible\">QlZGA</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-05-14T03:01:26',\n",
       "  'timestamp_epochs': 1589425286,\n",
       "  'tweet_id': '1260767145731821568',\n",
       "  'tweet_url': '/slam_hub/status/1260767145731821568',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 25,\n",
       "  'links': ['https://youtu.be/N9p1_Fkxxro'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Volumetric な地図表現のグラフベース SLAM 手法．SDF サブマップの集合で環境形状を表現．SDF を利用した位置合わせで隣接拘束を生成してポーズグラフ最適化．対応付け不要なので計算コストが低い．ループ閉じ込みは，外部から DBoW などを利用してループ拘束を与える．https://youtu.be/N9p1_Fkxxro\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Volumetric な地図表現のグラフベース SLAM 手法．SDF サブマップの集合で環境形状を表現．SDF を利用した位置合わせで隣接拘束を生成してポーズグラフ最適化．対応付け不要なので計算コストが低い．ループ閉じ込みは，外部から DBoW などを利用してループ拘束を与える．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/N9p1_Fkxxro\" dir=\"ltr\" href=\"https://t.co/PugAKvx6QH\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/N9p1_Fkxxro\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/N9p1_Fkxxro</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-13T03:56:53',\n",
       "  'timestamp_epochs': 1589342213,\n",
       "  'tweet_id': '1260418715004416009',\n",
       "  'tweet_url': '/slam_hub/status/1260418715004416009',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EXySm7iUcAASOh8.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 44,\n",
       "  'links': ['https://youtu.be/RFhH4j0gzsI'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 13,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '4つの魚眼カメラを用いてロバストなオドメトリ、全方位のデプス画像生成、密な環境復元を実現．全方位のデプス画像生成にはEnd-to-Endの学習ベースによるOmniMVSを用いて生成．推定した全方位デプス画像をTSDFで統合して密な環境復元を行う．\\nhttps://youtu.be/RFhH4j0gzsI\\xa0pic.twitter.com/JfqoF0LKsA',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">4つの魚眼カメラを用いてロバストなオドメトリ、全方位のデプス画像生成、密な環境復元を実現．全方位のデプス画像生成にはEnd-to-Endの学習ベースによるOmniMVSを用いて生成．推定した全方位デプス画像をTSDFで統合して密な環境復元を行う．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://youtu.be/RFhH4j0gzsI\" dir=\"ltr\" href=\"https://t.co/KaC4YRDOCA\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/RFhH4j0gzsI\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/RFhH4j0gzsI</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/JfqoF0LKsA\">pic.twitter.com/JfqoF0LKsA</a></p>',\n",
       "  'timestamp': '2020-05-12T03:05:05',\n",
       "  'timestamp_epochs': 1589252705,\n",
       "  'tweet_id': '1260043290415489025',\n",
       "  'tweet_url': '/slam_hub/status/1260043290415489025',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EXo-QIDUYAALT0S.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 44,\n",
       "  'links': ['https://arxiv.org/abs/2004.12498v1'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '3D点群のクラス推定を2Dのセグメンテーションマップのみを教師として学習するモデルの提案．各点のクラスとvisibilityを推論し，これらで合成した2Dセグメンテーションマップと教師との損失をもとに学習する．複数の物体を含む大規模なシーンでも高い性能を達成．https://arxiv.org/abs/2004.12498v1\\xa0…pic.twitter.com/C4Gefp6dFI',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">3D点群のクラス推定を2Dのセグメンテーションマップのみを教師として学習するモデルの提案．各点のクラスとvisibilityを推論し，これらで合成した2Dセグメンテーションマップと教師との損失をもとに学習する．複数の物体を含む大規模なシーンでも高い性能を達成．<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.12498v1\" dir=\"ltr\" href=\"https://t.co/nyHL77pRUX\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.12498v1\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.12498</span><span class=\"invisible\">v1</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/C4Gefp6dFI\">pic.twitter.com/C4Gefp6dFI</a></p>',\n",
       "  'timestamp': '2020-05-10T07:39:44',\n",
       "  'timestamp_epochs': 1589096384,\n",
       "  'tweet_id': '1259387630472409088',\n",
       "  'tweet_url': '/slam_hub/status/1259387630472409088',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 33,\n",
       "  'links': ['https://arxiv.org/abs/2004.12051'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '単眼SLAMの初期化手法を提案.Homography推定後,Global Plane Optimization (GPO)で最適化しカメラ姿勢と平面の法線を取得.複数フレームの平面情報を組み合わせることで三角測量やHomography分解の計算負荷を減らすことが可能で,精度とリアルタイム性で優れていることを示した.\\nhttps://arxiv.org/abs/2004.12051\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">単眼SLAMの初期化手法を提案.Homography推定後,Global Plane Optimization (GPO)で最適化しカメラ姿勢と平面の法線を取得.複数フレームの平面情報を組み合わせることで三角測量やHomography分解の計算負荷を減らすことが可能で,精度とリアルタイム性で優れていることを示した.\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.12051\" dir=\"ltr\" href=\"https://t.co/L5dkXajHYg\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.12051\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.12051</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-08T03:00:00',\n",
       "  'timestamp_epochs': 1588906800,\n",
       "  'tweet_id': '1258592458109321220',\n",
       "  'tweet_url': '/slam_hub/status/1258592458109321220',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 38,\n",
       "  'links': ['https://youtu.be/H80Bnxm8IPE'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 10,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '事前のLiDARマップを用いた単眼カメラ定位手法．マップからofflineで3D線を，AFMでビデオからonlineで2D線を検出．VINS-Monoからのカメラ動き予測により，2D-3D線の対応を取得．その2D-3D対応を用いたポーズ最適化により，ループクローズなしで、VIOのドリフトを低減させた．https://youtu.be/H80Bnxm8IPE\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">事前のLiDARマップを用いた単眼カメラ定位手法．マップからofflineで3D線を，AFMでビデオからonlineで2D線を検出．VINS-Monoからのカメラ動き予測により，2D-3D線の対応を取得．その2D-3D対応を用いたポーズ最適化により，ループクローズなしで、VIOのドリフトを低減させた．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/H80Bnxm8IPE\" dir=\"ltr\" href=\"https://t.co/82hakMCfvz\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/H80Bnxm8IPE\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/H80Bnxm8IPE</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-07T03:00:00',\n",
       "  'timestamp_epochs': 1588820400,\n",
       "  'tweet_id': '1258230071900475393',\n",
       "  'tweet_url': '/slam_hub/status/1258230071900475393',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EXQuefGU0AAZ1ZD.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 45,\n",
       "  'links': ['https://arxiv.org/abs/2004.13324'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 13,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像間の相対姿勢のみを用いた弱教師あり学習による特徴量抽出手法を提案．中間特徴の相関に基づく微分可能なマッチング層やcourse-to-fine構造のネットワークを用い，エピポーラ幾何の拘束を組み込んだ損失関数で学習を行う．教師あり学習による既存手法の精度を上回った．\\nhttps://arxiv.org/abs/2004.13324\\xa0pic.twitter.com/CDPfuoZCOm',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像間の相対姿勢のみを用いた弱教師あり学習による特徴量抽出手法を提案．中間特徴の相関に基づく微分可能なマッチング層やcourse-to-fine構造のネットワークを用い，エピポーラ幾何の拘束を組み込んだ損失関数で学習を行う．教師あり学習による既存手法の精度を上回った．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.13324\" dir=\"ltr\" href=\"https://t.co/5iWl6qzVeh\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.13324\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.13324</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/CDPfuoZCOm\">pic.twitter.com/CDPfuoZCOm</a></p>',\n",
       "  'timestamp': '2020-05-06T03:00:00',\n",
       "  'timestamp_epochs': 1588734000,\n",
       "  'tweet_id': '1257867682533330947',\n",
       "  'tweet_url': '/slam_hub/status/1257867682533330947',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 25,\n",
       "  'links': ['https://www.youtube.com/watch?v=HMetye3gmAs'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '三次元点群のインスタンスセグメンテーション手法の研究．VoxelベースのU-Netで各点のクラスラベルと物体中心へのオフセットを推定．推定された座標について点群をクラスタリングすることで物体候補を生成し，後段のNNでスコアを出力する．https://www.youtube.com/watch?v=HMetye3gmAs\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">三次元点群のインスタンスセグメンテーション手法の研究．VoxelベースのU-Netで各点のクラスラベルと物体中心へのオフセットを推定．推定された座標について点群をクラスタリングすることで物体候補を生成し，後段のNNでスコアを出力する．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=HMetye3gmAs\" dir=\"ltr\" href=\"https://t.co/zkc8bu8Xvt\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=HMetye3gmAs\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=HMetye</span><span class=\"invisible\">3gmAs</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-05-05T03:05:42',\n",
       "  'timestamp_epochs': 1588647942,\n",
       "  'tweet_id': '1257506731942273024',\n",
       "  'tweet_url': '/slam_hub/status/1257506731942273024',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': ['SLAM',\n",
       "   'VisualOdometry',\n",
       "   'Keyframes',\n",
       "   'BundleAdjustment',\n",
       "   'FeatureMatching',\n",
       "   'LocalFeatures',\n",
       "   'python'],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EXLcS0BUwAMYvpA.jpg',\n",
       "   'https://pbs.twimg.com/media/EXLcU30UEAIE-97.jpg',\n",
       "   'https://pbs.twimg.com/media/EXLe_qpUEAIB5Dt.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 657,\n",
       "  'links': ['https://github.com/luigifreda/pyslam'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 11,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 193,\n",
       "  'screen_name': 'LuigiFreda',\n",
       "  'text': 'I am excited to release\\xa0pySLAM v2. Now you can play with #SLAM techniques, #VisualOdometry, #Keyframes, #BundleAdjustment, #FeatureMatching, and many modern #LocalFeatures (based on DL).  Everything is accessible from a single #python environment.  https://github.com/luigifreda/pyslam\\xa0…pic.twitter.com/6K5yi5Z2G1',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"en\">I am excited to release\\xa0pySLAM v2. Now you can play with <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/SLAM?src=hash\"><s>#</s><b>SLAM</b></a> techniques, <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/VisualOdometry?src=hash\"><s>#</s><b>VisualOdometry</b></a>, <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/Keyframes?src=hash\"><s>#</s><b>Keyframes</b></a>, <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/BundleAdjustment?src=hash\"><s>#</s><b>BundleAdjustment</b></a>, <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/FeatureMatching?src=hash\"><s>#</s><b>FeatureMatching</b></a>, and many modern <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/LocalFeatures?src=hash\"><s>#</s><b>LocalFeatures</b></a> (based on DL).  Everything is accessible from a single <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/python?src=hash\"><s>#</s><b>python</b></a> environment.  <a class=\"twitter-timeline-link\" data-expanded-url=\"https://github.com/luigifreda/pyslam\" dir=\"ltr\" href=\"https://t.co/XEPiuvnXJG\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/luigifreda/pyslam\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">github.com/luigifreda/pys</span><span class=\"invisible\">lam</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/6K5yi5Z2G1\">pic.twitter.com/6K5yi5Z2G1</a></p>',\n",
       "  'timestamp': '2020-05-04T14:19:49',\n",
       "  'timestamp_epochs': 1588601989,\n",
       "  'tweet_id': '1257313988347432973',\n",
       "  'tweet_url': '/LuigiFreda/status/1257313988347432973',\n",
       "  'user_id': '1037574933864435713',\n",
       "  'username': 'Luigi Freda',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EXJH4VjUcAA_hcR.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 26,\n",
       "  'links': ['https://ieeexplore.ieee.org/document/8606275'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'リアルタイムでGlobally ConsistentなRGB-D SLAMのため，最適化空間を線形な特徴量部分と非線形な姿勢部分に分解．線形部分を特徴量の二次統計量で表すことで，効率的に計算を行う．処理時間をフレーム数に対してほぼリニアな増加に抑えつつ，SOTA精度を達成．\\nPaper: https://ieeexplore.ieee.org/document/8606275\\xa0…pic.twitter.com/2VsW10Ca6T',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">リアルタイムでGlobally ConsistentなRGB-D SLAMのため，最適化空間を線形な特徴量部分と非線形な姿勢部分に分解．線形部分を特徴量の二次統計量で表すことで，効率的に計算を行う．処理時間をフレーム数に対してほぼリニアな増加に抑えつつ，SOTA精度を達成．\\nPaper: <a class=\"twitter-timeline-link\" data-expanded-url=\"https://ieeexplore.ieee.org/document/8606275\" dir=\"ltr\" href=\"https://t.co/0UjFiUEZQo\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://ieeexplore.ieee.org/document/8606275\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">ieeexplore.ieee.org/document/86062</span><span class=\"invisible\">75</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/2VsW10Ca6T\">pic.twitter.com/2VsW10Ca6T</a></p>',\n",
       "  'timestamp': '2020-05-04T03:13:58',\n",
       "  'timestamp_epochs': 1588562038,\n",
       "  'tweet_id': '1257146424707280896',\n",
       "  'tweet_url': '/slam_hub/status/1257146424707280896',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EW_xwgBUwAAJz8C.jpg'],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 48,\n",
       "  'links': ['https://arxiv.org/abs/2003.00186'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 11,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '三次元点群のOne-Stage物体検出Hybrid Voxel Network (HVNet)手法を提案。Pointwiseでのmulti-scale特徴をHybrid Voxel Feature Extraction(HVFE)で抽出、Voxelwise attention featureにエンコード、Pseudo-Image Featureへデカップル、リアルタイムの31HzでSOTAを達成。\\nhttps://arxiv.org/abs/2003.00186\\xa0pic.twitter.com/0yyB3iaOkU',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">三次元点群のOne-Stage物体検出Hybrid Voxel Network (HVNet)手法を提案。Pointwiseでのmulti-scale特徴をHybrid Voxel Feature Extraction(HVFE)で抽出、Voxelwise attention featureにエンコード、Pseudo-Image Featureへデカップル、リアルタイムの31HzでSOTAを達成。\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.00186\" dir=\"ltr\" href=\"https://t.co/IJKAJ2FJo9\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.00186\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.00186</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/0yyB3iaOkU\">pic.twitter.com/0yyB3iaOkU</a></p>',\n",
       "  'timestamp': '2020-05-03T03:00:00',\n",
       "  'timestamp_epochs': 1588474800,\n",
       "  'tweet_id': '1256780519343140864',\n",
       "  'tweet_url': '/slam_hub/status/1256780519343140864',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 45,\n",
       "  'links': ['https://youtu.be/5Tia2oblJAg'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 10,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '単眼デプス推定モデルに対して，SfMで離れた画像ペアを選択し，MVSとOptical Flowの結果から奥行と画像座標の距離を損失としてfine-tuneすることで，動画に対し一貫性のある推定を実現．学習の前処理でMVSの結果からスケールを調整．動物体による誤差の影響でSOTAに近い精度．https://youtu.be/5Tia2oblJAg\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">単眼デプス推定モデルに対して，SfMで離れた画像ペアを選択し，MVSとOptical Flowの結果から奥行と画像座標の距離を損失としてfine-tuneすることで，動画に対し一貫性のある推定を実現．学習の前処理でMVSの結果からスケールを調整．動物体による誤差の影響でSOTAに近い精度．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/5Tia2oblJAg\" dir=\"ltr\" href=\"https://t.co/3tAhEBwqZy\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/5Tia2oblJAg\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/5Tia2oblJAg</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-05-02T06:11:26',\n",
       "  'timestamp_epochs': 1588399886,\n",
       "  'tweet_id': '1256466307647602693',\n",
       "  'tweet_url': '/slam_hub/status/1256466307647602693',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EW-vxpMU4AAsxxf.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 62,\n",
       "  'links': ['https://arxiv.org/abs/2004.06376'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 13,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '単眼画像からの自由空間の推定．多視点のステレオ画像から得られた対象環境の幾何情報を単眼画像に集約し学習することで，単一視点からでは不可視な領域に対してもTraversabilityやDepthの評価が可能に．\\nhttps://arxiv.org/abs/2004.06376\\xa0pic.twitter.com/7xkhPWGGu8',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">単眼画像からの自由空間の推定．多視点のステレオ画像から得られた対象環境の幾何情報を単眼画像に集約し学習することで，単一視点からでは不可視な領域に対してもTraversabilityやDepthの評価が可能に．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.06376\" dir=\"ltr\" href=\"https://t.co/QpFjnZYjj3\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.06376\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.06376</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/7xkhPWGGu8\">pic.twitter.com/7xkhPWGGu8</a></p>',\n",
       "  'timestamp': '2020-05-02T02:52:13',\n",
       "  'timestamp_epochs': 1588387933,\n",
       "  'tweet_id': '1256416173610041344',\n",
       "  'tweet_url': '/slam_hub/status/1256416173610041344',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EW5q4e_VcAATkwL.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 30,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '地図を surfel で表現するグラフベース SLAM 手法の SuMa を拡張。3D LIDAR 点群を距離画像に変換し、FCN でセマンティックセグメンテーション。セマンティクスの整合性を重みとする。静止している車は位置合わせに利用される。移動している車が多い KITTI dataset の高速道路でも高精度な推定を実現。pic.twitter.com/M286GNv7WB',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">地図を surfel で表現するグラフベース SLAM 手法の SuMa を拡張。3D LIDAR 点群を距離画像に変換し、FCN でセマンティックセグメンテーション。セマンティクスの整合性を重みとする。静止している車は位置合わせに利用される。移動している車が多い KITTI dataset の高速道路でも高精度な推定を実現。<a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/M286GNv7WB\">pic.twitter.com/M286GNv7WB</a></p>',\n",
       "  'timestamp': '2020-05-01T03:13:39',\n",
       "  'timestamp_epochs': 1588302819,\n",
       "  'tweet_id': '1256059179724271616',\n",
       "  'tweet_url': '/slam_hub/status/1256059179724271616',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EW0oDztVAAIjhWL.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 50,\n",
       "  'links': ['https://arxiv.org/pdf/2003.10983.pdf'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 14,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Kinect Fusionに利用されているTSDFを学習器に置き換えたDeepSDFを局所適用し，詳細な形状表現を可能にした．DeepSDFは全体を関数近似するのに対し，提案手法はVoxel単位で関数近似．DeepSDFが8日かかった形状復元が，提案手法では1分と大幅に短縮．\\nhttps://arxiv.org/pdf/2003.10983.pdf\\xa0…pic.twitter.com/Y89OJ9FDXh',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Kinect Fusionに利用されているTSDFを学習器に置き換えたDeepSDFを局所適用し，詳細な形状表現を可能にした．DeepSDFは全体を関数近似するのに対し，提案手法はVoxel単位で関数近似．DeepSDFが8日かかった形状復元が，提案手法では1分と大幅に短縮．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/pdf/2003.10983.pdf\" dir=\"ltr\" href=\"https://t.co/m2NeCBySES\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/pdf/2003.10983.pdf\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/pdf/2003.10983</span><span class=\"invisible\">.pdf</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Y89OJ9FDXh\">pic.twitter.com/Y89OJ9FDXh</a></p>',\n",
       "  'timestamp': '2020-04-30T03:43:06',\n",
       "  'timestamp_epochs': 1588218186,\n",
       "  'tweet_id': '1255704202178752513',\n",
       "  'tweet_url': '/slam_hub/status/1255704202178752513',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWRtDnfWAAAaobc.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 195,\n",
       "  'links': ['https://arxiv.org/abs/2004.10566',\n",
       "   'https://www.di.ens.fr/willow/research/sparse-ncnet/'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 3,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 42,\n",
       "  'screen_name': 'relja_work',\n",
       "  'text': 'New paper by my student Ignacio Rocco - a 10x faster and less memory hungry NCNet with equivalent results (or more accurate with a smaller speedup) https://arxiv.org/abs/2004.10566\\xa0 . Code and models available https://www.di.ens.fr/willow/research/sparse-ncnet/\\xa0…pic.twitter.com/FwBNDymJZF',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"en\">New paper by my student Ignacio Rocco - a 10x faster and less memory hungry NCNet with equivalent results (or more accurate with a smaller speedup) <a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.10566\" dir=\"ltr\" href=\"https://t.co/Go4eq3qxnl\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.10566\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.10566</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a> . Code and models available <a class=\"twitter-timeline-link\" data-expanded-url=\"https://www.di.ens.fr/willow/research/sparse-ncnet/\" dir=\"ltr\" href=\"https://t.co/Twk7nSzPbq\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.di.ens.fr/willow/research/sparse-ncnet/\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">di.ens.fr/willow/researc</span><span class=\"invisible\">h/sparse-ncnet/</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/FwBNDymJZF\">pic.twitter.com/FwBNDymJZF</a></p>',\n",
       "  'timestamp': '2020-04-23T09:02:03',\n",
       "  'timestamp_epochs': 1587632523,\n",
       "  'tweet_id': '1253247752848519171',\n",
       "  'tweet_url': '/relja_work/status/1253247752848519171',\n",
       "  'user_id': '841031248839618560',\n",
       "  'username': 'Relja Arandjelović',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 53,\n",
       "  'links': ['https://twitter.com/slam_hub/status/1255331095656239105'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'sakuDken',\n",
       "  'text': 'CVPR2020 Oralに採択されたStanfordのChoyさん( @ChrisChoy208  )の論文，自身がICCV2019で提案した3D ConvNetをさらっと6Dに拡張したり，きれいに定式化して微分可能な点群マッチ提案したり，よく見たらもう1本CVPR2020 Oralに採択された自身の論文引用してるし，流石ですとしか言いようがない．https://twitter.com/slam_hub/status/1255331095656239105\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"4\" lang=\"ja\">CVPR2020 Oralに採択されたStanfordのChoyさん( <a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"3317678802\" dir=\"ltr\" href=\"/ChrisChoy208\"><s>@</s><b>ChrisChoy208</b></a>  )の論文，自身がICCV2019で提案した3D ConvNetをさらっと6Dに拡張したり，きれいに定式化して微分可能な点群マッチ提案したり，よく見たらもう1本CVPR2020 Oralに採択された自身の論文引用してるし，流石ですとしか言いようがない．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://twitter.com/slam_hub/status/1255331095656239105\" dir=\"ltr\" href=\"https://t.co/LSSQ4waZFb\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://twitter.com/slam_hub/status/1255331095656239105\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">twitter.com/slam_hub/statu</span><span class=\"invisible\">s/1255331095656239105</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-29T04:39:57',\n",
       "  'timestamp_epochs': 1588135197,\n",
       "  'tweet_id': '1255356123772788740',\n",
       "  'tweet_url': '/sakuDken/status/1255356123772788740',\n",
       "  'user_id': '425816448',\n",
       "  'username': 'Ken Sakurada',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWm4H8TUMAQ2gk5.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 19,\n",
       "  'links': ['https://arxiv.org/abs/1912.08804'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '任意解像度の単一画像から任意のビューを合成するend-to-endなネットワークの提案．推論したFeatureとDepthを用い点群を構築，微分可能な点群レンダラーとリファインメントネットワークを通すことで欠損のないビューを合成．\\nhttps://arxiv.org/abs/1912.08804\\xa0pic.twitter.com/cp2TcgoURr',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">任意解像度の単一画像から任意のビューを合成するend-to-endなネットワークの提案．推論したFeatureとDepthを用い点群を構築，微分可能な点群レンダラーとリファインメントネットワークを通すことで欠損のないビューを合成．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1912.08804\" dir=\"ltr\" href=\"https://t.co/d8ZfnSs3Bi\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1912.08804\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1912.08804</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/cp2TcgoURr\">pic.twitter.com/cp2TcgoURr</a></p>',\n",
       "  'timestamp': '2020-04-28T03:00:00',\n",
       "  'timestamp_epochs': 1588042800,\n",
       "  'tweet_id': '1254968580136820736',\n",
       "  'tweet_url': '/slam_hub/status/1254968580136820736',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 63,\n",
       "  'links': ['https://slamhub.xslam.org/'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'ystk_hara',\n",
       "  'text': 'SLAM-Hub に参加することになりました！微力ながら貢献できればと思います。https://slamhub.xslam.org/\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">SLAM-Hub に参加することになりました！微力ながら貢献できればと思います。<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://slamhub.xslam.org/\" dir=\"ltr\" href=\"https://t.co/kX0E1hcJ3b\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://slamhub.xslam.org/\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">slamhub.xslam.org</span><span class=\"invisible\">/</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-04-27T03:20:42',\n",
       "  'timestamp_epochs': 1587957642,\n",
       "  'tweet_id': '1254611403148627968',\n",
       "  'tweet_url': '/ystk_hara/status/1254611403148627968',\n",
       "  'user_id': '1113412223001452545',\n",
       "  'username': 'Yoshitaka HARA',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 36,\n",
       "  'links': ['https://youtu.be/djf7vGtf7CA'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 7,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '深層学習を用いた特徴量ベースのループ検出器を組み込んだグラフLiDAR-SLAMシステムを提案．四脚ロボットでも動作するように浅いネットワークを用いておりCPUで推論可能．kd-treeを用いた点群繋ぎ合わせの高速な検証方法を提案．屋内外の産業環境でロバスト性を実証．https://youtu.be/djf7vGtf7CA\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">深層学習を用いた特徴量ベースのループ検出器を組み込んだグラフLiDAR-SLAMシステムを提案．四脚ロボットでも動作するように浅いネットワークを用いておりCPUで推論可能．kd-treeを用いた点群繋ぎ合わせの高速な検証方法を提案．屋内外の産業環境でロバスト性を実証．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/djf7vGtf7CA\" dir=\"ltr\" href=\"https://t.co/93aNP7Z6cq\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/djf7vGtf7CA\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/djf7vGtf7CA</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-04-27T03:07:56',\n",
       "  'timestamp_epochs': 1587956876,\n",
       "  'tweet_id': '1254608188801875968',\n",
       "  'tweet_url': '/slam_hub/status/1254608188801875968',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': ['slamhub'],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 19,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'SLAM-Hubに千葉工大 fuRoの原先生 @ystk_hara が加入されました．千葉周辺にお住まいのSLAM-Hubにご興味のある方は原先生へ気軽にご相談ください．#slamhub',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">SLAM-Hubに千葉工大 fuRoの原先生 <a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"1113412223001452545\" dir=\"ltr\" href=\"/ystk_hara\"><s>@</s><b>ystk_hara</b></a> が加入されました．千葉周辺にお住まいのSLAM-Hubにご興味のある方は原先生へ気軽にご相談ください．<a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/slamhub?src=hash\"><s>#</s><b>slamhub</b></a></p>',\n",
       "  'timestamp': '2020-04-27T03:04:14',\n",
       "  'timestamp_epochs': 1587956654,\n",
       "  'tweet_id': '1254607258375213056',\n",
       "  'tweet_url': '/slam_hub/status/1254607258375213056',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWf6iLgVAAAldSl.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 93,\n",
       "  'links': ['https://arxiv.org/abs/2004.10934'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 28,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'CNNによる高速な物体検出器YOLOの最新版YOLOv4を提案．検出器の学習における，Bag of freebiesやBag of specialsによる効果を検証．バッチ正規化や残差スキップ接続など，モデルやデータセットに関して普遍的で効果のよい手法を用いることで精度を向上させた．\\nhttps://arxiv.org/abs/2004.10934\\xa0pic.twitter.com/G3cdiZTGMZ',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">CNNによる高速な物体検出器YOLOの最新版YOLOv4を提案．検出器の学習における，Bag of freebiesやBag of specialsによる効果を検証．バッチ正規化や残差スキップ接続など，モデルやデータセットに関して普遍的で効果のよい手法を用いることで精度を向上させた．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2004.10934\" dir=\"ltr\" href=\"https://t.co/vW6Mmjo0W3\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2004.10934\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2004.10934</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/G3cdiZTGMZ\">pic.twitter.com/G3cdiZTGMZ</a></p>',\n",
       "  'timestamp': '2020-04-26T03:11:07',\n",
       "  'timestamp_epochs': 1587870667,\n",
       "  'tweet_id': '1254246604090961920',\n",
       "  'tweet_url': '/slam_hub/status/1254246604090961920',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWagb_EUYAEJgO6.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 53,\n",
       "  'links': ['https://arxiv.org/abs/2002.11051'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 16,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '反復法による最小自乗を解く新たな最適化システムを提案．既存の問題を統一的に解けるようにソルバを設計することで，疎/密，動的/静的な要素にシームレスに対応した．様々な観点で比較評価を行い，提案手法が既存システムに対し同等以上の速度，精度性能を達成した．\\nhttps://arxiv.org/abs/2002.11051\\xa0pic.twitter.com/FUqWsYd55A',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">反復法による最小自乗を解く新たな最適化システムを提案．既存の問題を統一的に解けるようにソルバを設計することで，疎/密，動的/静的な要素にシームレスに対応した．様々な観点で比較評価を行い，提案手法が既存システムに対し同等以上の速度，精度性能を達成した．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2002.11051\" dir=\"ltr\" href=\"https://t.co/KkSnMSLTRN\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2002.11051\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2002.11051</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/FUqWsYd55A\">pic.twitter.com/FUqWsYd55A</a></p>',\n",
       "  'timestamp': '2020-04-25T03:00:00',\n",
       "  'timestamp_epochs': 1587783600,\n",
       "  'tweet_id': '1253881416191897600',\n",
       "  'tweet_url': '/slam_hub/status/1253881416191897600',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWRrybDUcAAkDj3.jpg'],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 19,\n",
       "  'links': ['http://arxiv.org/abs/2003.05593'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'グラフ描画のアルゴリズムを用いて3D点群を2D画像に投影する手法を提案。投影した画像にU-Netを適用し、3D点群のセグメンテーションでSOTAを達成。階層的クラスタリングで得られた部分点群ごとに投影することで計算コストを削減。\\nhttp://arxiv.org/abs/2003.05593\\xa0pic.twitter.com/4Mt0buWYu9',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">グラフ描画のアルゴリズムを用いて3D点群を2D画像に投影する手法を提案。投影した画像にU-Netを適用し、3D点群のセグメンテーションでSOTAを達成。階層的クラスタリングで得られた部分点群ごとに投影することで計算コストを削減。\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"http://arxiv.org/abs/2003.05593\" dir=\"ltr\" href=\"https://t.co/m7CAucRzf4\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://arxiv.org/abs/2003.05593\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">http://</span><span class=\"js-display-url\">arxiv.org/abs/2003.05593</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/4Mt0buWYu9\">pic.twitter.com/4Mt0buWYu9</a></p>',\n",
       "  'timestamp': '2020-04-24T03:00:00',\n",
       "  'timestamp_epochs': 1587697200,\n",
       "  'tweet_id': '1253519029291094020',\n",
       "  'tweet_url': '/slam_hub/status/1253519029291094020',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWQy0LjUwAAj205.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 19,\n",
       "  'links': ['https://www.youtube.com/watch?v=-imRJXq6ZuE'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '2つのLIDARスキャン間の相対姿勢推定をUnsupervisedに学習．全周画像上で特徴量抽出や最終的なICP誤差のロスを定義することで対応付けを避けて学習に適した枠組みとしている．\\nvideo:https://www.youtube.com/watch?v=-imRJXq6ZuE\\xa0…pic.twitter.com/WV2OHJKXzA',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">2つのLIDARスキャン間の相対姿勢推定をUnsupervisedに学習．全周画像上で特徴量抽出や最終的なICP誤差のロスを定義することで対応付けを避けて学習に適した枠組みとしている．\\nvideo:<a class=\"twitter-timeline-link\" data-expanded-url=\"https://www.youtube.com/watch?v=-imRJXq6ZuE\" dir=\"ltr\" href=\"https://t.co/pZaRSVllyW\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=-imRJXq6ZuE\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=-imRJX</span><span class=\"invisible\">q6ZuE</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/WV2OHJKXzA\">pic.twitter.com/WV2OHJKXzA</a></p>',\n",
       "  'timestamp': '2020-04-23T04:42:42',\n",
       "  'timestamp_epochs': 1587616962,\n",
       "  'tweet_id': '1253182486252670978',\n",
       "  'tweet_url': '/slam_hub/status/1253182486252670978',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 22,\n",
       "  'links': ['https://youtu.be/rN6D3QmMNuU'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'ターゲット点群の一部スキャンを削除、更に不完全な点群を入力として、自己教師ありで点群補完を学習させるSG-NNを提案。点群はスパースTSDFで表現、encoder-decoderを使って、見たこともない三次元形状にデコードでき、ターゲット点群より高い分解能点群を補完することが可能https://youtu.be/rN6D3QmMNuU\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ターゲット点群の一部スキャンを削除、更に不完全な点群を入力として、自己教師ありで点群補完を学習させるSG-NNを提案。点群はスパースTSDFで表現、encoder-decoderを使って、見たこともない三次元形状にデコードでき、ターゲット点群より高い分解能点群を補完することが可能<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/rN6D3QmMNuU\" dir=\"ltr\" href=\"https://t.co/fcZ2gdRU3t\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/rN6D3QmMNuU\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/rN6D3QmMNuU</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-04-22T03:00:00',\n",
       "  'timestamp_epochs': 1587524400,\n",
       "  'tweet_id': '1252794252284891136',\n",
       "  'tweet_url': '/slam_hub/status/1252794252284891136',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWGHIEwUYAI5H7e.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 80,\n",
       "  'links': ['https://arxiv.org/abs/2003.01641'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 17,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '2D地図を入力とした目的地への経路・行動生成手法の提案．GOSELOと呼ばれる地図表現を介しCNNによるWaypointを生成(教師あり学習)，さらにWaypointに沿うような操作量を生成する層(強化学習)を後段に追加することで安全なナビゲーションを実現している．\\nhttps://arxiv.org/abs/2003.01641\\xa0pic.twitter.com/yskq5iMDHZ',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">2D地図を入力とした目的地への経路・行動生成手法の提案．GOSELOと呼ばれる地図表現を介しCNNによるWaypointを生成(教師あり学習)，さらにWaypointに沿うような操作量を生成する層(強化学習)を後段に追加することで安全なナビゲーションを実現している．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.01641\" dir=\"ltr\" href=\"https://t.co/kDF76mPH0k\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.01641\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.01641</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/yskq5iMDHZ\">pic.twitter.com/yskq5iMDHZ</a></p>',\n",
       "  'timestamp': '2020-04-21T02:56:13',\n",
       "  'timestamp_epochs': 1587437773,\n",
       "  'tweet_id': '1252430913985642496',\n",
       "  'tweet_url': '/slam_hub/status/1252430913985642496',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EWA_xsTU0AAck4E.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 46,\n",
       "  'links': ['https://arxiv.org/abs/1912.07744'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 8,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'RGB画像1枚から物体の3D Bounding Box(3DBB)と6DoF姿勢推定．3DBB投影時のコーナー点位置をテンプレートの重み付け和で表現し，その重みを推定する枠組み．同時に3DBBの3次元位置姿勢を推定し投影点上でのLossを定義し学習する．\\nhttps://arxiv.org/abs/1912.07744\\xa0pic.twitter.com/lVKH8vErzB',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">RGB画像1枚から物体の3D Bounding Box(3DBB)と6DoF姿勢推定．3DBB投影時のコーナー点位置をテンプレートの重み付け和で表現し，その重みを推定する枠組み．同時に3DBBの3次元位置姿勢を推定し投影点上でのLossを定義し学習する．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1912.07744\" dir=\"ltr\" href=\"https://t.co/5icXYHXryK\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1912.07744\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1912.07744</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/lVKH8vErzB\">pic.twitter.com/lVKH8vErzB</a></p>',\n",
       "  'timestamp': '2020-04-20T03:06:40',\n",
       "  'timestamp_epochs': 1587352000,\n",
       "  'tweet_id': '1252071153817903108',\n",
       "  'tweet_url': '/slam_hub/status/1252071153817903108',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 39,\n",
       "  'links': ['https://www.youtube.com/watch?time_continue=279&v=zPzMtXU-0JE&feature=emb_logo'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 14,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'VAEを用いて学習した各物体カテゴリの3D形状特徴量を，トラッキングしながらカメラ姿勢と同時に最適化することで，単一あるいは複数視点のRGB-D画像から，オクルージョンがあっても欠損がない物体の高精度な3D形状を復元する手法を提案．https://www.youtube.com/watch?time_continue=279&v=zPzMtXU-0JE&feature=emb_logo\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">VAEを用いて学習した各物体カテゴリの3D形状特徴量を，トラッキングしながらカメラ姿勢と同時に最適化することで，単一あるいは複数視点のRGB-D画像から，オクルージョンがあっても欠損がない物体の高精度な3D形状を復元する手法を提案．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?time_continue=279&amp;v=zPzMtXU-0JE&amp;feature=emb_logo\" dir=\"ltr\" href=\"https://t.co/VAJUoIcYFm\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?time_continue=279&amp;v=zPzMtXU-0JE&amp;feature=emb_logo\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?time_con</span><span class=\"invisible\">tinue=279&amp;v=zPzMtXU-0JE&amp;feature=emb_logo</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-19T03:00:00',\n",
       "  'timestamp_epochs': 1587265200,\n",
       "  'tweet_id': '1251707090277548033',\n",
       "  'tweet_url': '/slam_hub/status/1251707090277548033',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 49,\n",
       "  'links': ['https://www.youtube.com/watch?v=JGL4H93BiNw'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 13,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '既存のVisual SLAM手法を任意の複数カメラシステムに拡張する手法を提案．適応的な初期化，センサに依存しないキーフレーム選択，voxelベースのマップ管理法を用いることで，精度を保ちセンサ固有の改良なしでの動作を実現.https://www.youtube.com/watch?v=JGL4H93BiNw\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">既存のVisual SLAM手法を任意の複数カメラシステムに拡張する手法を提案．適応的な初期化，センサに依存しないキーフレーム選択，voxelベースのマップ管理法を用いることで，精度を保ちセンサ固有の改良なしでの動作を実現.<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=JGL4H93BiNw\" dir=\"ltr\" href=\"https://t.co/k1ZRC58rHD\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=JGL4H93BiNw\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=JGL4H9</span><span class=\"invisible\">3BiNw</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-18T03:00:00',\n",
       "  'timestamp_epochs': 1587178800,\n",
       "  'tweet_id': '1251344702227353600',\n",
       "  'tweet_url': '/slam_hub/status/1251344702227353600',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVxb1NiU4AEJwkl.jpg'],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 48,\n",
       "  'links': ['https://arxiv.org/abs/2003.12642'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 23,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'ワイドベースラインカメラで撮影した6枚の画像から高品質な幾何形状とSVBRDFを復元する学習ベースの手法を提案．各画像ごとに拡散・鏡面アルベド，法線，鏡面粗さをネットワークで推定し，推定結果を融合して幾何形状とSVBRDF得る．従来難しかった疎な画像からの復元に成功．\\nhttps://arxiv.org/abs/2003.12642\\xa0pic.twitter.com/Qn5OoQkR0s',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ワイドベースラインカメラで撮影した6枚の画像から高品質な幾何形状とSVBRDFを復元する学習ベースの手法を提案．各画像ごとに拡散・鏡面アルベド，法線，鏡面粗さをネットワークで推定し，推定結果を融合して幾何形状とSVBRDF得る．従来難しかった疎な画像からの復元に成功．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.12642\" dir=\"ltr\" href=\"https://t.co/tP9NEqInwl\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.12642\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.12642</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Qn5OoQkR0s\">pic.twitter.com/Qn5OoQkR0s</a></p>',\n",
       "  'timestamp': '2020-04-17T02:34:59',\n",
       "  'timestamp_epochs': 1587090899,\n",
       "  'tweet_id': '1250976017079922688',\n",
       "  'tweet_url': '/slam_hub/status/1250976017079922688',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 3,\n",
       "  'links': [],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 2,\n",
       "  'screen_name': 'sakuDken',\n",
       "  'text': '週一で通勤・通学して良いとなった場合，何曜日にしますか？',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">週一で通勤・通学して良いとなった場合，何曜日にしますか？</p>',\n",
       "  'timestamp': '2020-04-16T02:15:32',\n",
       "  'timestamp_epochs': 1587003332,\n",
       "  'tweet_id': '1250608736646856704',\n",
       "  'tweet_url': '/sakuDken/status/1250608736646856704',\n",
       "  'user_id': '425816448',\n",
       "  'username': 'Ken Sakurada',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 17,\n",
       "  'links': ['https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'ステレオカメラを用いて，物体上の特徴点のクラスター化と，自身と物体の動きの推定を同時に行うシステムを提案．クラスター化は，物体検出による特徴点のクラスラベルを用いて，3次元位置も考慮したCRFにより実装．シーンに依存せず，オンラインでの動作を可能にした．https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">ステレオカメラを用いて，物体上の特徴点のクラスター化と，自身と物体の動きの推定を同時に行うシステムを提案．クラスター化は，物体検出による特徴点のクラスラベルを用いて，3次元位置も考慮したCRFにより実装．シーンに依存せず，オンラインでの動作を可能にした．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=paK-WCQpX-Y&amp;feature=youtu.be\" dir=\"ltr\" href=\"https://t.co/6907L4kl7G\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=paK-WCQpX-Y&amp;feature=youtu.be\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=paK-WC</span><span class=\"invisible\">QpX-Y&amp;feature=youtu.be</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-16T01:00:00',\n",
       "  'timestamp_epochs': 1586998800,\n",
       "  'tweet_id': '1250589729210449920',\n",
       "  'tweet_url': '/slam_hub/status/1250589729210449920',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVpCxOhXQAA6xxk.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 240,\n",
       "  'links': ['https://github.com/nianticlabs/footprints'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 56,\n",
       "  'screen_name': 'dantkz',\n",
       "  'text': 'CVPR 2020 Oral paper from Niantic:  https://github.com/nianticlabs/footprints\\xa0…pic.twitter.com/iaZQcOG70C',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"sl\">CVPR 2020 Oral paper from Niantic:  <a class=\"twitter-timeline-link\" data-expanded-url=\"https://github.com/nianticlabs/footprints\" dir=\"ltr\" href=\"https://t.co/FK1dKGArGW\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/nianticlabs/footprints\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">github.com/nianticlabs/fo</span><span class=\"invisible\">otprints</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/iaZQcOG70C\">pic.twitter.com/iaZQcOG70C</a></p>',\n",
       "  'timestamp': '2020-04-15T11:27:43',\n",
       "  'timestamp_epochs': 1586950063,\n",
       "  'tweet_id': '1250385307553464320',\n",
       "  'tweet_url': '/dantkz/status/1250385307553464320',\n",
       "  'user_id': '363998340',\n",
       "  'username': 'Daniyar Turmukhambetov',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 21,\n",
       "  'links': ['https://www.youtube.com/watch?v=htnRuGKZmZw'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'CodeSLAMをベースにした新たな深層学習ベースのVisual SLAMシステムを提案．既存のコードによるコンパクトなデプスマップ表現に加え，損失関数の改善，ループクロージングと全体最適化の追加により，精度とロバストを向上．さらにリアルタイム動作を実現した．https://www.youtube.com/watch?v=htnRuGKZmZw\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">CodeSLAMをベースにした新たな深層学習ベースのVisual SLAMシステムを提案．既存のコードによるコンパクトなデプスマップ表現に加え，損失関数の改善，ループクロージングと全体最適化の追加により，精度とロバストを向上．さらにリアルタイム動作を実現した．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://www.youtube.com/watch?v=htnRuGKZmZw\" dir=\"ltr\" href=\"https://t.co/xdvQ4i0TDc\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=htnRuGKZmZw\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://www.</span><span class=\"js-display-url\">youtube.com/watch?v=htnRuG</span><span class=\"invisible\">KZmZw</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-15T01:00:00',\n",
       "  'timestamp_epochs': 1586912400,\n",
       "  'tweet_id': '1250227339776217088',\n",
       "  'tweet_url': '/slam_hub/status/1250227339776217088',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVfbCzoU4AEISU9.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 24,\n",
       "  'links': ['https://arxiv.org/abs/1911.11236'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 9,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '点数が百万を超える大規模三次元点群のセグメンテーションにおいては、従来の高コストな点群サンプリング手法よりもランダムサンプリングが有効であることを示した．KNNとアテンションを用いて積極的に受容野を拡大することで、サンプリングによる点群の欠損に対処．\\nhttps://arxiv.org/abs/1911.11236\\xa0pic.twitter.com/xjpFO7WABl',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">点数が百万を超える大規模三次元点群のセグメンテーションにおいては、従来の高コストな点群サンプリング手法よりもランダムサンプリングが有効であることを示した．KNNとアテンションを用いて積極的に受容野を拡大することで、サンプリングによる点群の欠損に対処．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1911.11236\" dir=\"ltr\" href=\"https://t.co/x2oy9Vuvl7\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1911.11236\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1911.11236</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/xjpFO7WABl\">pic.twitter.com/xjpFO7WABl</a></p>',\n",
       "  'timestamp': '2020-04-14T01:00:00',\n",
       "  'timestamp_epochs': 1586826000,\n",
       "  'tweet_id': '1249864951092506624',\n",
       "  'tweet_url': '/slam_hub/status/1249864951092506624',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVaW8PaUMAAm7RO.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 38,\n",
       "  'links': ['https://arxiv.org/abs/1907.02233'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 6,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'LIDAR-IMUのオドメトリ推定手法．Error State Kalman Filter上でTight-couplingに最適化を行うことで，従来のグラフベース手法と近い精度を維持しつつ大幅に処理速度を向上させた．\\npaper:https://arxiv.org/abs/1907.02233\\xa0pic.twitter.com/IQ4YAzhL8q',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">LIDAR-IMUのオドメトリ推定手法．Error State Kalman Filter上でTight-couplingに最適化を行うことで，従来のグラフベース手法と近い精度を維持しつつ大幅に処理速度を向上させた．\\npaper:<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/1907.02233\" dir=\"ltr\" href=\"https://t.co/5eI705xk45\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/1907.02233\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/1907.02233</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/IQ4YAzhL8q\">pic.twitter.com/IQ4YAzhL8q</a></p>',\n",
       "  'timestamp': '2020-04-13T02:00:00',\n",
       "  'timestamp_epochs': 1586743200,\n",
       "  'tweet_id': '1249517662847283201',\n",
       "  'tweet_url': '/slam_hub/status/1249517662847283201',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 11,\n",
       "  'links': ['https://twitter.com/slam_hub/status/1247356052657455105'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 2,\n",
       "  'screen_name': 'mamii0718',\n",
       "  'text': 'SLAMのR&Dコミュニティのメンバーになりました！面白い理論が見つけられるよう精進していきます！(メンバーを募集しています〜)https://twitter.com/slam_hub/status/1247356052657455105\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"4\" lang=\"ja\">SLAMのR&amp;Dコミュニティのメンバーになりました！面白い理論が見つけられるよう精進していきます！(メンバーを募集しています〜)<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://twitter.com/slam_hub/status/1247356052657455105\" dir=\"ltr\" href=\"https://t.co/io0g4bJHkz\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://twitter.com/slam_hub/status/1247356052657455105\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">twitter.com/slam_hub/statu</span><span class=\"invisible\">s/1247356052657455105</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-11T11:55:50',\n",
       "  'timestamp_epochs': 1586606150,\n",
       "  'tweet_id': '1248942834813530112',\n",
       "  'tweet_url': '/mamii0718/status/1248942834813530112',\n",
       "  'user_id': '2540936520',\n",
       "  'username': 'まみー',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 8,\n",
       "  'links': ['https://youtu.be/XCyl1-vxfII'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 2,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Object Levelより細かく，新たな形状表現Local Implicit Grid (LIG)を提案．AutoencoderでPartの形状をlatent vectorにエンコード，入力点群と復元ロス最小なlatent vectorを最適化，LIGでの内挿により形状にデコード、機械学習で点群からScene Levelの三次元形状復元が可能．https://youtu.be/XCyl1-vxfII\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Object Levelより細かく，新たな形状表現Local Implicit Grid (LIG)を提案．AutoencoderでPartの形状をlatent vectorにエンコード，入力点群と復元ロス最小なlatent vectorを最適化，LIGでの内挿により形状にデコード、機械学習で点群からScene Levelの三次元形状復元が可能．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/XCyl1-vxfII\" dir=\"ltr\" href=\"https://t.co/f72zVbgkk0\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/XCyl1-vxfII\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/XCyl1-vxfII</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-04-12T02:00:30',\n",
       "  'timestamp_epochs': 1586656830,\n",
       "  'tweet_id': '1249155400122994688',\n",
       "  'tweet_url': '/slam_hub/status/1249155400122994688',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVSdYxHU0AIFkpg.jpg'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 23,\n",
       "  'links': ['https://arxiv.org/pdf/2004.00605.pdf'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 5,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'RGB画像における物体の6DoF姿勢推定．Surface fragmentによる3次元モデル表現を介し，物体のPoseを各PixelがどのようなインスタンスやSurface fragmentに対応しうるかを学習．得られた多対多な2D-3D対応をPnP-RANSACによりロバスト化．\\nhttps://arxiv.org/pdf/2004.00605.pdf\\xa0…pic.twitter.com/Qn2Etv1Nst',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">RGB画像における物体の6DoF姿勢推定．Surface fragmentによる3次元モデル表現を介し，物体のPoseを各PixelがどのようなインスタンスやSurface fragmentに対応しうるかを学習．得られた多対多な2D-3D対応をPnP-RANSACによりロバスト化．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/pdf/2004.00605.pdf\" dir=\"ltr\" href=\"https://t.co/f0XJojVT4T\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/pdf/2004.00605.pdf\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/pdf/2004.00605</span><span class=\"invisible\">.pdf</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Qn2Etv1Nst\">pic.twitter.com/Qn2Etv1Nst</a></p>',\n",
       "  'timestamp': '2020-04-11T02:13:15',\n",
       "  'timestamp_epochs': 1586571195,\n",
       "  'tweet_id': '1248796223961624576',\n",
       "  'tweet_url': '/slam_hub/status/1248796223961624576',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': ['CVPR2020'],\n",
       "  'img_urls': [],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 41,\n",
       "  'links': ['https://morefusion.wkentaro.com/',\n",
       "   'https://youtu.be/6oLUhuZL4ko'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 11,\n",
       "  'screen_name': 'AjdDavison',\n",
       "  'text': 'MoreFusion (@wkentaro_, @SucarEdgar, @StephenLJames, @DanielLenton1), Dyson Robotics Lab #CVPR2020. Joint volumetric reasoning accurately fits 3D object CAD models in clutter to enable precise pick and place.\\nProject page and paper: https://morefusion.wkentaro.com/\\xa0https://youtu.be/6oLUhuZL4ko\\xa0',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"en\">MoreFusion (<a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"782291244\" dir=\"ltr\" href=\"/wkentaro_\"><s>@</s><b>wkentaro_</b></a>, <a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"858493718952804352\" dir=\"ltr\" href=\"/SucarEdgar\"><s>@</s><b>SucarEdgar</b></a>, <a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"109603566\" dir=\"ltr\" href=\"/StephenLJames\"><s>@</s><b>StephenLJames</b></a>, <a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"1204388962355290112\" dir=\"ltr\" href=\"/DanielLenton1\"><s>@</s><b>DanielLenton1</b></a>), Dyson Robotics Lab <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/CVPR2020?src=hash\"><s>#</s><b>CVPR2020</b></a>. Joint volumetric reasoning accurately fits 3D object CAD models in clutter to enable precise pick and place.\\nProject page and paper: <a class=\"twitter-timeline-link\" data-expanded-url=\"https://morefusion.wkentaro.com/\" dir=\"ltr\" href=\"https://t.co/AZdAhwUbTO\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://morefusion.wkentaro.com/\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">morefusion.wkentaro.com</span><span class=\"invisible\">/</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://youtu.be/6oLUhuZL4ko\" dir=\"ltr\" href=\"https://t.co/LStInnt7XA\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://youtu.be/6oLUhuZL4ko\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">youtu.be/6oLUhuZL4ko</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a></p>',\n",
       "  'timestamp': '2020-04-10T12:45:48',\n",
       "  'timestamp_epochs': 1586522748,\n",
       "  'tweet_id': '1248593022146805760',\n",
       "  'tweet_url': '/AjdDavison/status/1248593022146805760',\n",
       "  'user_id': '1446792746',\n",
       "  'username': 'Andrew Davison',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVNWXyhUUAAYvJo.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 20,\n",
       "  'links': ['https://arxiv.org/pdf/2004.04090.pdf'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 4,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': '画像の位置合わせに必要なエラーについて，輝度差に代わる新たなメトリックSGFを提案．SGFは勾配画像のコントラストを局所的に正規化し，勾配方向の内積を利用．DSOにSGFを適用したところ精度が改善．\\nhttps://arxiv.org/pdf/2004.04090.pdf\\xa0…pic.twitter.com/HL3hkOLo61',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像の位置合わせに必要なエラーについて，輝度差に代わる新たなメトリックSGFを提案．SGFは勾配画像のコントラストを局所的に正規化し，勾配方向の内積を利用．DSOにSGFを適用したところ精度が改善．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/pdf/2004.04090.pdf\" dir=\"ltr\" href=\"https://t.co/WKscROSqPO\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/pdf/2004.04090.pdf\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/pdf/2004.04090</span><span class=\"invisible\">.pdf</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/HL3hkOLo61\">pic.twitter.com/HL3hkOLo61</a></p>',\n",
       "  'timestamp': '2020-04-10T02:24:08',\n",
       "  'timestamp_epochs': 1586485448,\n",
       "  'tweet_id': '1248436573865041920',\n",
       "  'tweet_url': '/slam_hub/status/1248436573865041920',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': [],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EVII8izVAAE-WVi.png'],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 30,\n",
       "  'links': ['https://arxiv.org/abs/2003.02247'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 3,\n",
       "  'screen_name': 'slam_hub',\n",
       "  'text': 'Visual SLAMで用いられる従来のキーフレーム表現（covisibility graph）では，3D点の遮蔽関係を記述できないため，ボクセルハッシングと視錐台表現を用いたレイキャスティングにより，高速かつ省メモリに幾何的関係性の記述を可能とした．\\nhttps://arxiv.org/abs/2003.02247\\xa0pic.twitter.com/MMeABTUKVP',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Visual SLAMで用いられる従来のキーフレーム表現（covisibility graph）では，3D点の遮蔽関係を記述できないため，ボクセルハッシングと視錐台表現を用いたレイキャスティングにより，高速かつ省メモリに幾何的関係性の記述を可能とした．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/abs/2003.02247\" dir=\"ltr\" href=\"https://t.co/Q82OgvSKah\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/abs/2003.02247\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/abs/2003.02247</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/MMeABTUKVP\">pic.twitter.com/MMeABTUKVP</a></p>',\n",
       "  'timestamp': '2020-04-09T02:08:44',\n",
       "  'timestamp_epochs': 1586398124,\n",
       "  'tweet_id': '1248070308080177152',\n",
       "  'tweet_url': '/slam_hub/status/1248070308080177152',\n",
       "  'user_id': '1244129482132209664',\n",
       "  'username': 'SLAM-Hub',\n",
       "  'video_url': ''},\n",
       " {'has_media': True,\n",
       "  'hashtags': ['ICRA2020', 'UZH', 'voxel', 'map', 'SLAMpic'],\n",
       "  'img_urls': ['https://pbs.twimg.com/media/EUDFZfyX0AEP1Q5.jpg'],\n",
       "  'is_replied': False,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 39,\n",
       "  'links': ['http://rpg.ifi.uzh.ch/docs/ICRA20_Muglikar.pdf'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 0,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 11,\n",
       "  'screen_name': 'davsca1',\n",
       "  'text': 'Check out our #ICRA2020 paper: \"Voxel Map for Visual SLAM\". We propose a voxel-map representation that is both scalable and geometry-aware while being as efficient keyframe SLAM techniques.\\nPDF: http://rpg.ifi.uzh.ch/docs/ICRA20_Muglikar.pdf\\xa0…\\n@manasimuglikar #UZH #voxel #map #SLAMpic.twitter.com/JRmP6S9m4W',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"en\">Check out our <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/ICRA2020?src=hash\"><s>#</s><b>ICRA2020</b></a> paper: \"Voxel Map for Visual SLAM\". We propose a voxel-map representation that is both scalable and geometry-aware while being as efficient keyframe SLAM techniques.\\nPDF: <a class=\"twitter-timeline-link\" data-expanded-url=\"http://rpg.ifi.uzh.ch/docs/ICRA20_Muglikar.pdf\" dir=\"ltr\" href=\"https://t.co/Bpg5x6e09D\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://rpg.ifi.uzh.ch/docs/ICRA20_Muglikar.pdf\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">http://</span><span class=\"js-display-url\">rpg.ifi.uzh.ch/docs/ICRA20_Mu</span><span class=\"invisible\">glikar.pdf</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a>\\n<a class=\"twitter-atreply pretty-link js-nav\" data-mentioned-user-id=\"1037623019416576000\" dir=\"ltr\" href=\"/ManasiMuglikar\"><s>@</s><b>manasimuglikar</b></a> <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/UZH?src=hash\"><s>#</s><b>UZH</b></a> <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/voxel?src=hash\"><s>#</s><b>voxel</b></a> <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/map?src=hash\"><s>#</s><b>map</b></a> <a class=\"twitter-hashtag pretty-link js-nav\" data-query-source=\"hashtag_click\" dir=\"ltr\" href=\"/hashtag/SLAM?src=hash\"><s>#</s><b>SLAM</b></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/JRmP6S9m4W\">pic.twitter.com/JRmP6S9m4W</a></p>',\n",
       "  'timestamp': '2020-03-26T16:20:38',\n",
       "  'timestamp_epochs': 1585239638,\n",
       "  'tweet_id': '1243211267281747969',\n",
       "  'tweet_url': '/davsca1/status/1243211267281747969',\n",
       "  'user_id': '1601204664',\n",
       "  'username': 'Davide Scaramuzza',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 13,\n",
       "  'links': ['http://xslam.org',\n",
       "   'https://twitter.com/wayama_ryousuke/status/1247408469562413056'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 2,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 2,\n",
       "  'screen_name': 'sakuDken',\n",
       "  'text': 'そのような将来も見据えてドメインを http://xslam.org\\xa0 としました．https://twitter.com/wayama_ryousuke/status/1247408469562413056\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"4\" lang=\"ja\">そのような将来も見据えてドメインを <a class=\"twitter-timeline-link\" data-expanded-url=\"http://xslam.org\" dir=\"ltr\" href=\"https://t.co/LYtvcDZgIN\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://xslam.org\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">http://</span><span class=\"js-display-url\">xslam.org</span><span class=\"invisible\"></span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span></span></a> としました．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://twitter.com/wayama_ryousuke/status/1247408469562413056\" dir=\"ltr\" href=\"https://t.co/BZjFyeoaTR\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://twitter.com/wayama_ryousuke/status/1247408469562413056\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">twitter.com/wayama_ryousuk</span><span class=\"invisible\">e/status/1247408469562413056</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-07T06:31:32',\n",
       "  'timestamp_epochs': 1586241092,\n",
       "  'tweet_id': '1247411668973281283',\n",
       "  'tweet_url': '/sakuDken/status/1247411668973281283',\n",
       "  'user_id': '425816448',\n",
       "  'username': 'Ken Sakurada',\n",
       "  'video_url': ''},\n",
       " {'has_media': False,\n",
       "  'hashtags': [],\n",
       "  'img_urls': [],\n",
       "  'is_replied': True,\n",
       "  'is_reply_to': False,\n",
       "  'likes': 183,\n",
       "  'links': ['https://twitter.com/slam_hub/status/1247356052657455105'],\n",
       "  'parent_tweet_id': '',\n",
       "  'replies': 1,\n",
       "  'reply_to_users': [],\n",
       "  'retweets': 79,\n",
       "  'screen_name': 'sakuDken',\n",
       "  'text': 'つくば産総研の横塚さんたちとR&Dコミュニティ「SLAM-Hub」を立ち上げました！ SLAMやCV，ロボティクス，ARの研究開発にご興味がある学生さんや研究者の方はぜひご参加をお待ちしております．活動場所はお台場 or つくば or オンラインです．https://twitter.com/slam_hub/status/1247356052657455105\\xa0…',\n",
       "  'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"4\" lang=\"ja\">つくば産総研の横塚さんたちとR&amp;Dコミュニティ「SLAM-Hub」を立ち上げました！ SLAMやCV，ロボティクス，ARの研究開発にご興味がある学生さんや研究者の方はぜひご参加をお待ちしております．活動場所はお台場 or つくば or オンラインです．<a class=\"twitter-timeline-link u-hidden\" data-expanded-url=\"https://twitter.com/slam_hub/status/1247356052657455105\" dir=\"ltr\" href=\"https://t.co/PxZhLsbg3S\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://twitter.com/slam_hub/status/1247356052657455105\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">twitter.com/slam_hub/statu</span><span class=\"invisible\">s/1247356052657455105</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a></p>',\n",
       "  'timestamp': '2020-04-07T03:24:14',\n",
       "  'timestamp_epochs': 1586229854,\n",
       "  'tweet_id': '1247364533024206848',\n",
       "  'tweet_url': '/sakuDken/status/1247364533024206848',\n",
       "  'user_id': '425816448',\n",
       "  'username': 'Ken Sakurada',\n",
       "  'video_url': ''}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLAM-Hub members got one paper accepted by #ECCV2020 !\n",
      "コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！\n",
      "SLAM-Hub members got 4 papers accepted by IROS2020!\n",
      "ロボティクス分野の国際会議IROS2020にSLAM-Hubのメンバーから4本の論文が採択されました！\n",
      "【お願い】OpenVSLAMに関する使い方や仕様等の質問のみに関しては，他の方々と情報を共有できるGitHubのissueやSlackなどへお願いいたします．（問い合わせが多く個別に対応するのが難しい状況です） #openvslam\n",
      "SLAM-Hubに千葉工大 fuRoの原先生 @ystk_hara が加入されました．千葉周辺にお住まいのSLAM-Hubにご興味のある方は原先生へ気軽にご相談ください．#slamhub\n",
      "週一で通勤・通学して良いとなった場合，何曜日にしますか？\n"
     ]
    }
   ],
   "source": [
    "# LinkもURLもないものはまとめない。\n",
    "for dics in sdic:\n",
    "    if dics['links']==[]:\n",
    "        if dics['img_urls']==[]:\n",
    "            print(dics['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVPR2020 Oralに採択されたStanfordのChoyさん( @ChrisChoy208  )の論文，自身がICCV2019で提案した3D ConvNetをさらっと6Dに拡張したり，きれいに定式化して微分可能な点群マッチ提案したり，よく見たらもう1本CVPR2020 Oralに採択された自身の論文引用してるし，流石ですとしか言いようがない．https://twitter.com/slam_hub/status/1255331095656239105 …\n",
      "SLAMのR&Dコミュニティのメンバーになりました！面白い理論が見つけられるよう精進していきます！(メンバーを募集しています〜)https://twitter.com/slam_hub/status/1247356052657455105 …\n",
      "つくば産総研の横塚さんたちとR&Dコミュニティ「SLAM-Hub」を立ち上げました！ SLAMやCV，ロボティクス，ARの研究開発にご興味がある学生さんや研究者の方はぜひご参加をお待ちしております．活動場所はお台場 or つくば or オンラインです．https://twitter.com/slam_hub/status/1247356052657455105 …\n"
     ]
    }
   ],
   "source": [
    "# Twitter と名のつくリンクがある場合も弾く\n",
    "for dics in sdic:\n",
    "    if dics['links']:\n",
    "        if 'twitter' in dics['links'][0]:\n",
    "            print(dics['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 動画があるリンク と名のつくリンクがある場合も弾く\n",
    "video_links = []\n",
    "for dics in sdic:\n",
    "    if dics['links']:\n",
    "        if 'youtu' in dics['links'][0]:\n",
    "            video_links.append(dics['links'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtubeのサムネイルを落とす。たくさん必要だからimagesに\n",
    "import os\n",
    "\n",
    "for line in video_links: \n",
    "    os.system(\"youtube-dl \"+\"--write-thumbnail \"+\"--skip-download \"+\"--output /images\"+line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x81y\\x83{\\x83\\x8b\\x83P\\x81[\\x83m\\x81z\\x94R\\x8f\\xc4\\x8cn\\x82\\xcc\\x8c\\xb8\\x97\\xca\\x90H\\x81I\\x89\\xce\\x93\\xe7\\x95\\x97\\x81I\\n'\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "res = subprocess.check_output(\"youtube-dl --get-title https://www.youtube.com/watch?v=s1d-uY-AT9Q\", shell=True)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'dir images/Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\n' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1f96442ac41b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"youtube-dl --get-title \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mvideo_links\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dir images/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 336\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[1;32m--> 418\u001b[1;33m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'dir images/Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\n' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "res = subprocess.check_output(\"youtube-dl --get-title \"+ video_links[0], shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output(\"dir images/\"+res.decode('utf-8'),shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要素抽出\n",
    "辞書を作る。テキストと画像，リンクがあればいいよね。titleも。\n",
    "\n",
    "Title？\n",
    "Youtubeは取得できる。\n",
    "arxivのときは，，，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arxivのid読み出しテスト\n",
    "'https://arxiv.org/abs/1910.14139'.split('/')[-1]\n",
    "\n",
    "arxivid='https://arxiv.org/abs/1910.14139'.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1910.14139\n"
     ]
    }
   ],
   "source": [
    "# .pdfが混じっている時がある。数字だけを抽出するようにする。\n",
    "# [int(s) for s in str.split() if s.isdigit()]\n",
    "# https://stackoverflow.com/questions/4289331/how-to-extract-numbers-from-a-string-in-python\n",
    "\n",
    "arxivid_ = 'https://arxiv.org/abs/1910.14139.pdf'.split('/')[-1]\n",
    "ids = [s for s in arxivid_.split('.') if s.isdigit()]\n",
    "\n",
    "arxivid__=\"\"\n",
    "for i in range(len(ids)):\n",
    "    arxivid__ += ids[i]+\".\"\n",
    "arxivid = arxivid__[:-1]\n",
    "\n",
    "print(arxivid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FutureMapping 2: Gaussian Belief Propagation for Spatial AI'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "arxiv.query(id_list=[arxivid])[0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has_media': True, 'hashtags': [], 'img_urls': ['https://pbs.twimg.com/media/EYw3pD7U4AESaXj.jpg'], 'is_replied': True, 'is_reply_to': False, 'likes': 50, 'links': [], 'parent_tweet_id': '', 'replies': 1, 'reply_to_users': [], 'retweets': 8, 'screen_name': 'slam_hub', 'text': '画像からgeometric、semantic、textureのVote特徴を抽出、三次元点群のVote特徴と融合し、3D物体検出の手法を提案。Multi-modalデータ融合を改善するにmulti-towerとgradient blendingの構造を使用し、SUNRGB-Dで既存SOTAより5.7mAPの精度を向上させ、SLAMようなSparse点群に対する有効性も確認。pic.twitter.com/6zfURbMEPw', 'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">画像からgeometric、semantic、textureのVote特徴を抽出、三次元点群のVote特徴と融合し、3D物体検出の手法を提案。Multi-modalデータ融合を改善するにmulti-towerとgradient blendingの構造を使用し、SUNRGB-Dで既存SOTAより5.7mAPの精度を向上させ、SLAMようなSparse点群に対する有効性も確認。<a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/6zfURbMEPw\">pic.twitter.com/6zfURbMEPw</a></p>', 'timestamp': '2020-05-25T03:00:00', 'timestamp_epochs': 1590375600, 'tweet_id': '1264753052122308615', 'tweet_url': '/slam_hub/status/1264753052122308615', 'user_id': '1244129482132209664', 'username': 'SLAM-Hub', 'video_url': ''}\n",
      "{'has_media': True, 'hashtags': [], 'img_urls': ['https://pbs.twimg.com/media/EW5q4e_VcAATkwL.jpg'], 'is_replied': True, 'is_reply_to': False, 'likes': 30, 'links': [], 'parent_tweet_id': '', 'replies': 1, 'reply_to_users': [], 'retweets': 9, 'screen_name': 'slam_hub', 'text': '地図を surfel で表現するグラフベース SLAM 手法の SuMa を拡張。3D LIDAR 点群を距離画像に変換し、FCN でセマンティックセグメンテーション。セマンティクスの整合性を重みとする。静止している車は位置合わせに利用される。移動している車が多い KITTI dataset の高速道路でも高精度な推定を実現。pic.twitter.com/M286GNv7WB', 'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">地図を surfel で表現するグラフベース SLAM 手法の SuMa を拡張。3D LIDAR 点群を距離画像に変換し、FCN でセマンティックセグメンテーション。セマンティクスの整合性を重みとする。静止している車は位置合わせに利用される。移動している車が多い KITTI dataset の高速道路でも高精度な推定を実現。<a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/M286GNv7WB\">pic.twitter.com/M286GNv7WB</a></p>', 'timestamp': '2020-05-01T03:13:39', 'timestamp_epochs': 1588302819, 'tweet_id': '1256059179724271616', 'tweet_url': '/slam_hub/status/1256059179724271616', 'user_id': '1244129482132209664', 'username': 'SLAM-Hub', 'video_url': ''}\n"
     ]
    }
   ],
   "source": [
    "for dics in sdic:\n",
    "    if dics['links']:# if has link\n",
    "        1+1\n",
    "    elif dics['img_urls']:\n",
    "        #print(dics['img_urls'])\n",
    "        print(dics)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docdics = []\n",
    "for dics in sdic:\n",
    "    ddic = {}\n",
    "    title = \"\"\n",
    "    imgurl = \"\"    \n",
    "    if dics['links']:# if has link\n",
    "        # linkの最後尾をタイトルにする\n",
    "        title = dics['links'][0].split('/')[-1]\n",
    "        # Youtubeだった場合\n",
    "        if 'youtu' in dics['links'][0]: # if it is youtube link\n",
    "            imgurl = subprocess.check_output(\"youtube-dl --get-thumbnail \\\"\"+dics['links'][0] +\"\\\"\", shell=True)\n",
    "            title = subprocess.check_output(\"youtube-dl --get-title \\\"\"+ dics['links'][0] +\"\\\"\", shell=True)\n",
    "        # Arxivだった場合\n",
    "        if 'arxiv' in dics['links'][0]:\n",
    "            arxivid_ = 'https://arxiv.org/abs/1910.14139.pdf'.split('/')[-1]\n",
    "            ids = [s for s in arxivid_.split('.') if s.isdigit()]\n",
    "\n",
    "            arxivid__=\"\"\n",
    "            for i in range(len(ids)):\n",
    "                arxivid__ += ids[i]+\".\"\n",
    "            arxivid = arxivid__[:-1]\n",
    "            title = arxiv.query(id_list=[arxivid])[0]['title']\n",
    "            if dics['img_urls']:\n",
    "                imgurl = dics['img_urls'][0]\n",
    "    # linkはないけど画像はある時\n",
    "    elif dics['img_urls']:\n",
    "        imgurl = dics['img_urls'][0]\n",
    "        title = \"image only\"\n",
    "    \n",
    "    ddic['id'] = dics['tweet_id']\n",
    "    ddic['title'] = title\n",
    "    ddic['imgurl'] = imgurl\n",
    "    ddic['text'] = dics['text']\n",
    "    docdics.append(ddic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_media': True,\n",
       " 'hashtags': [],\n",
       " 'img_urls': ['https://pbs.twimg.com/media/EW0oDztVAAIjhWL.jpg'],\n",
       " 'is_replied': True,\n",
       " 'is_reply_to': False,\n",
       " 'likes': 50,\n",
       " 'links': ['https://arxiv.org/pdf/2003.10983.pdf'],\n",
       " 'parent_tweet_id': '',\n",
       " 'replies': 1,\n",
       " 'reply_to_users': [],\n",
       " 'retweets': 14,\n",
       " 'screen_name': 'slam_hub',\n",
       " 'text': 'Kinect Fusionに利用されているTSDFを学習器に置き換えたDeepSDFを局所適用し，詳細な形状表現を可能にした．DeepSDFは全体を関数近似するのに対し，提案手法はVoxel単位で関数近似．DeepSDFが8日かかった形状復元が，提案手法では1分と大幅に短縮．\\nhttps://arxiv.org/pdf/2003.10983.pdf\\xa0…pic.twitter.com/Y89OJ9FDXh',\n",
       " 'text_html': '<p class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\" data-aria-label-part=\"0\" lang=\"ja\">Kinect Fusionに利用されているTSDFを学習器に置き換えたDeepSDFを局所適用し，詳細な形状表現を可能にした．DeepSDFは全体を関数近似するのに対し，提案手法はVoxel単位で関数近似．DeepSDFが8日かかった形状復元が，提案手法では1分と大幅に短縮．\\n<a class=\"twitter-timeline-link\" data-expanded-url=\"https://arxiv.org/pdf/2003.10983.pdf\" dir=\"ltr\" href=\"https://t.co/m2NeCBySES\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://arxiv.org/pdf/2003.10983.pdf\"><span class=\"tco-ellipsis\"></span><span class=\"invisible\">https://</span><span class=\"js-display-url\">arxiv.org/pdf/2003.10983</span><span class=\"invisible\">.pdf</span><span class=\"tco-ellipsis\"><span class=\"invisible\">\\xa0</span>…</span></a><a class=\"twitter-timeline-link u-hidden\" data-pre-embedded=\"true\" dir=\"ltr\" href=\"https://t.co/Y89OJ9FDXh\">pic.twitter.com/Y89OJ9FDXh</a></p>',\n",
       " 'timestamp': '2020-04-30T03:43:06',\n",
       " 'timestamp_epochs': 1588218186,\n",
       " 'tweet_id': '1255704202178752513',\n",
       " 'tweet_url': '/slam_hub/status/1255704202178752513',\n",
       " 'user_id': '1244129482132209664',\n",
       " 'username': 'SLAM-Hub',\n",
       " 'video_url': ''}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'https://i.ytimg.com/vi_webp/tnPfbJaPrSQ/maxresdefault.webp\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need escape? not allow \"\"?\n",
    "subprocess.check_output(\"youtube-dl --get-thumbnail \\\"\"+dics['links'][0] +\"\\\"\", shell=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.systemでは'でエスケープしてもOK，しかしSubprocessではだめだった。\n",
    "os.system(\"youtube-dl --get-thumbnail \\'\"+dics['links'][0] +'\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docdics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 後処理工程\n",
    "- byte文字列がある場合がある\n",
    "- 広報ツイートなどをフィルタリングする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R&Dコミュニティ「SLAM-Hub」を立ち上げました！\n",
      "一緒に研究開発してくれるメンバーを募集しています．https://slamhub.xslam.org/?0 \n",
      "ありがとうございます。 FutureMapping 2 もご覧ください！https://arxiv.org/abs/1910.14139 \n",
      "SLAM-Hub members got one paper accepted by #ECCV2020 !\n",
      "コンピュータビジョン分野の国際会議ECCV2020にSLAM-Hubのメンバーから1本の論文が採択されました！\n",
      "SLAM-Hub members got 4 papers accepted by IROS2020!\n",
      "ロボティクス分野の国際会議IROS2020にSLAM-Hubのメンバーから4本の論文が採択されました！\n",
      "OUR Shurijo reports.\n",
      "2020/06/08 までの合計: 3134 名、44435枚\n",
      "Data collected by 2020/06/08: 44435 images from 3134 people.\n",
      "\n",
      "https://our-shurijo.org \n",
      "#首里城\n",
      "#OUR_Shurijo\n",
      "【お願い】OpenVSLAMに関する使い方や仕様等の質問のみに関しては，他の方々と情報を共有できるGitHubのissueやSlackなどへお願いいたします．（問い合わせが多く個別に対応するのが難しい状況です） #openvslam\n",
      "本日のRSJセミナーの講演資料です． Visual SLAMと深層学習を用いた３Dモデリングについて，私たち（@sumicco_cv ，@mikiya85 ）の開発するOpenVSLAMを交えて紹介しています．（OpenVSLAMは先日ROS2に対応し，皆様のおかげでスター数は2,300を超えました！）https://www.slideshare.net/KenSakurada/126-rsj2020522 …\n",
      "I am excited to release pySLAM v2. Now you can play with #SLAM techniques, #VisualOdometry, #Keyframes, #BundleAdjustment, #FeatureMatching, and many modern #LocalFeatures (based on DL).  Everything is accessible from a single #python environment.  https://github.com/luigifreda/pyslam …pic.twitter.com/6K5yi5Z2G1\n"
     ]
    }
   ],
   "source": [
    "# 広報ツイートが紛れ込んでいる。URLか本文のキーワードで落とすか。\"可能\"，\"提案\"，\"実現\"のどれかがはいっていないなら弾く。\n",
    "filterwords = [\"可能\",\"提案\",\"実現\",\"推定\",\"定式\",\"最適化\",\"生成\",\"評価\"]\n",
    "for docs in docdics:\n",
    "    if not any(wd in docs[\"text\"] for wd in filterwords):\n",
    "        print(docs[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/AYjgeaQR8uQ/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/b62iDkLgGSI/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/ifL8yTbRFDk/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/tnPfbJaPrSQ/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/apmmduXTnaE/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/9-ixexpjN-8/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/Jvl42VJmYxg/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/dI2FZG_txN0/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/DYBmD88vpiA/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/glZyJ66ktog/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi/B4YBWFuYBdE/hqdefault.jpg?sqp=-oaymwEZCNACELwBSFXyq4qpAwsIARUAAIhCGAFwAQ==&rs=AOn4CLA2YAttIJoqzxaW92JVThwPYIN-HA\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/2ck5_sToayc/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/co7y6LQ7Kqc/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/D0JObXCfxv0/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/sq2hhkHgtb0/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/lE5gjzRKWuA/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/6OoRZrqfSJ4/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/YTfliBco6aw/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/Bb92aMBJR44/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/GuLzjnFGDKs/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/Zttl3eDjNyc/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/JHz_ImeI8HE/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/a5JWe6mOAEs/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/xIHCyyaB5gU/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/7hxGmMk4MZ0/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/h0bqURQlZGA/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi/N9p1_Fkxxro/maxresdefault.jpg\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/RFhH4j0gzsI/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/H80Bnxm8IPE/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/HMetye3gmAs/maxresdefault.webp\\n'\n",
      "b'Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\\n'\n",
      "b'https://i.ytimg.com/vi_webp/5Tia2oblJAg/maxresdefault.webp\\n'\n"
     ]
    }
   ],
   "source": [
    "# Encode to utf8\n",
    "# string byteの混じった文字をstringに統一する\n",
    "# type(b'abcd') == bytes\n",
    "\n",
    "for docs in docdics:\n",
    "    for dd in docs:\n",
    "        if type(docs[dd]) == bytes:\n",
    "            print(docs[dd])\n",
    "            \n",
    "# .decode()でデコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering\n",
    "filterwords = [\"可能\",\"提案\",\"実現\",\"推定\",\"定式\",\"最適化\",\"生成\",\"評価\"]\n",
    "filtereddics=[]\n",
    "for docs in docdics:\n",
    "    if any(wd in docs[\"text\"] for wd in filterwords):\n",
    "        fdocs = docs\n",
    "        # decode bytes\n",
    "        for dd in fdocs:\n",
    "            if type(fdocs[dd]) == bytes:\n",
    "                fdocs[dd]=fdocs[dd].decode()\n",
    "        \n",
    "        filtereddics.append(fdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtereddics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown，Marp形式に落とすか。\n",
    "\n",
    "- 個々のdicsを開いて\n",
    "- `##`以下にタイトルを書く\n",
    "- 画像は右40%くらいにcontainにする？\n",
    "- テキストを貼る\n",
    "- 区切り`---`を入れる\n",
    "\n",
    "メモ\n",
    "- エンコードエラーが出る。https://qiita.com/butada/items/33db39ced989c2ebf644\n",
    "- まとめてStringにしてから，byteに変換しようかな。\n",
    "- とりまencoding='utf-8'で開くことを試す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdocfile = open(\"survey.md\",\"w+\",encoding='utf-8')\n",
    "\n",
    "#for i in range(3):\n",
    "#    mkdics = filtereddics[i]\n",
    "for mkdics in filtereddics:\n",
    "    onepagestring = \"\\n---\\n\"+\"## \" + mkdics[\"title\"]\\\n",
    "    + \"\\n\" + mkdics[\"text\"]\\\n",
    "    + \"\\n ![bg right:40% cover](\"+mkdics[\"imgurl\"]+\")\\n\"\n",
    "    mkdocfile.write(onepagestring)\n",
    "    \n",
    "mkdocfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1286862906143854593',\n",
       " 'imgurl': 'https://pbs.twimg.com/media/Edva-6wU8AUplEB.png',\n",
       " 'text': '多視点物体認識のための特徴量抽出を自己教師あり学習する手法を提案．代理タスクとして，オブジェクトクラス分類を通し距離学習を行う．これにより視点に因らず同一オブジェクトならば埋め込み表現上でクラスターを形成．ダウンストリームタスクで他手法より高い精度を達成．\\nhttps://arxiv.org/abs/2003.12735\\xa0pic.twitter.com/YyvMehYHKd',\n",
       " 'title': 'Exploit Clues from Views: Self-Supervised and Regularized Learning for\\n  Multiview Object Recognition'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtereddics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdoc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
